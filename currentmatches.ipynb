{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import warnings\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "np.set_printoptions(precision=3, suppress=True)  # Print as 0.001 instead of 9.876e-4\n",
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding match data from file\n",
    "data = np.loadtxt('newmatchdata.csv',skiprows=1,delimiter=',')\n",
    "X_data = np.array(data[:,1:]).astype(np.int32)\n",
    "y_data = np.array(data[:,:1]).astype(np.int32)\n",
    "\n",
    "#turn all data into feature vector\n",
    "#feature vector creation\n",
    "X_val_trn = torch.zeros((40000,138*2),dtype=torch.float32)\n",
    "j = 0\n",
    "for d in X_data:\n",
    "    for i in range(len(d)):\n",
    "        if(i < len(d)/2):\n",
    "            h = d[i]\n",
    "            X_val_trn[j][h] += 1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            h = d[i]\n",
    "            X_val_trn[j][h+137] += 1\n",
    "      \n",
    "    j += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w2/kkshvcxn49l4c5_1vv9cd2nr0000gn/T/ipykernel_22362/1988071220.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train,dtype=torch.float32)\n",
      "/var/folders/w2/kkshvcxn49l4c5_1vv9cd2nr0000gn/T/ipykernel_22362/1988071220.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val,dtype=torch.float32)\n",
      "/var/folders/w2/kkshvcxn49l4c5_1vv9cd2nr0000gn/T/ipykernel_22362/1988071220.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#sklearn train test split\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_val_trn,y_data, test_size = 0.2, shuffle= True)\n",
    "\n",
    "#split train into train/validation set\n",
    "\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_train,y_train, test_size = 0.15, shuffle= True)\n",
    "\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(X_val,dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val,dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "#sklearn train test split\n",
    "#use test set to calculate error, CCE, accuracy etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model creation\n",
    "#addition of multiple hidden layers and drop rate to help with regularization\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,100),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100,50),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50,1)\n",
    ")\n",
    "\n",
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: loss on final training batch: 0.6924\n",
      "Epoch  1: accuracy on validation set: 0.4910\n",
      "Epoch  1: loss on validation set: 0.6931\n",
      "Epoch  2: loss on final training batch: 0.6917\n",
      "Epoch  2: accuracy on validation set: 0.4910\n",
      "Epoch  2: loss on validation set: 0.6929\n",
      "Epoch  3: loss on final training batch: 0.6920\n",
      "Epoch  3: accuracy on validation set: 0.4908\n",
      "Epoch  3: loss on validation set: 0.6928\n",
      "Epoch  4: loss on final training batch: 0.6919\n",
      "Epoch  4: accuracy on validation set: 0.4906\n",
      "Epoch  4: loss on validation set: 0.6926\n",
      "Epoch  5: loss on final training batch: 0.6916\n",
      "Epoch  5: accuracy on validation set: 0.4925\n",
      "Epoch  5: loss on validation set: 0.6924\n",
      "Epoch  6: loss on final training batch: 0.6911\n",
      "Epoch  6: accuracy on validation set: 0.4950\n",
      "Epoch  6: loss on validation set: 0.6925\n",
      "Epoch  7: loss on final training batch: 0.6913\n",
      "Epoch  7: accuracy on validation set: 0.4938\n",
      "Epoch  7: loss on validation set: 0.6922\n",
      "Epoch  8: loss on final training batch: 0.6909\n",
      "Epoch  8: accuracy on validation set: 0.4963\n",
      "Epoch  8: loss on validation set: 0.6919\n",
      "Epoch  9: loss on final training batch: 0.6908\n",
      "Epoch  9: accuracy on validation set: 0.5077\n",
      "Epoch  9: loss on validation set: 0.6918\n",
      "Epoch 10: loss on final training batch: 0.6903\n",
      "Epoch 10: accuracy on validation set: 0.5010\n",
      "Epoch 10: loss on validation set: 0.6915\n",
      "Epoch 11: loss on final training batch: 0.6908\n",
      "Epoch 11: accuracy on validation set: 0.5052\n",
      "Epoch 11: loss on validation set: 0.6913\n",
      "Epoch 12: loss on final training batch: 0.6894\n",
      "Epoch 12: accuracy on validation set: 0.4992\n",
      "Epoch 12: loss on validation set: 0.6909\n",
      "Epoch 13: loss on final training batch: 0.6895\n",
      "Epoch 13: accuracy on validation set: 0.5073\n",
      "Epoch 13: loss on validation set: 0.6908\n",
      "Epoch 14: loss on final training batch: 0.6878\n",
      "Epoch 14: accuracy on validation set: 0.5150\n",
      "Epoch 14: loss on validation set: 0.6903\n",
      "Epoch 15: loss on final training batch: 0.6872\n",
      "Epoch 15: accuracy on validation set: 0.5177\n",
      "Epoch 15: loss on validation set: 0.6897\n",
      "Epoch 16: loss on final training batch: 0.6861\n",
      "Epoch 16: accuracy on validation set: 0.5198\n",
      "Epoch 16: loss on validation set: 0.6893\n",
      "Epoch 17: loss on final training batch: 0.6854\n",
      "Epoch 17: accuracy on validation set: 0.5206\n",
      "Epoch 17: loss on validation set: 0.6886\n",
      "Epoch 18: loss on final training batch: 0.6851\n",
      "Epoch 18: accuracy on validation set: 0.5242\n",
      "Epoch 18: loss on validation set: 0.6881\n",
      "Epoch 19: loss on final training batch: 0.6822\n",
      "Epoch 19: accuracy on validation set: 0.5273\n",
      "Epoch 19: loss on validation set: 0.6874\n",
      "Epoch 20: loss on final training batch: 0.6813\n",
      "Epoch 20: accuracy on validation set: 0.5233\n",
      "Epoch 20: loss on validation set: 0.6867\n",
      "Epoch 21: loss on final training batch: 0.6806\n",
      "Epoch 21: accuracy on validation set: 0.5260\n",
      "Epoch 21: loss on validation set: 0.6861\n",
      "Epoch 22: loss on final training batch: 0.6806\n",
      "Epoch 22: accuracy on validation set: 0.5290\n",
      "Epoch 22: loss on validation set: 0.6855\n",
      "Epoch 23: loss on final training batch: 0.6770\n",
      "Epoch 23: accuracy on validation set: 0.5323\n",
      "Epoch 23: loss on validation set: 0.6845\n",
      "Epoch 24: loss on final training batch: 0.6763\n",
      "Epoch 24: accuracy on validation set: 0.5344\n",
      "Epoch 24: loss on validation set: 0.6842\n",
      "Epoch 25: loss on final training batch: 0.6762\n",
      "Epoch 25: accuracy on validation set: 0.5329\n",
      "Epoch 25: loss on validation set: 0.6834\n",
      "Epoch 26: loss on final training batch: 0.6735\n",
      "Epoch 26: accuracy on validation set: 0.5325\n",
      "Epoch 26: loss on validation set: 0.6829\n",
      "Epoch 27: loss on final training batch: 0.6722\n",
      "Epoch 27: accuracy on validation set: 0.5319\n",
      "Epoch 27: loss on validation set: 0.6824\n",
      "Epoch 28: loss on final training batch: 0.6711\n",
      "Epoch 28: accuracy on validation set: 0.5321\n",
      "Epoch 28: loss on validation set: 0.6822\n",
      "Epoch 29: loss on final training batch: 0.6714\n",
      "Epoch 29: accuracy on validation set: 0.5327\n",
      "Epoch 29: loss on validation set: 0.6813\n",
      "Epoch 30: loss on final training batch: 0.6649\n",
      "Epoch 30: accuracy on validation set: 0.5335\n",
      "Epoch 30: loss on validation set: 0.6814\n",
      "Epoch 31: loss on final training batch: 0.6680\n",
      "Epoch 31: accuracy on validation set: 0.5371\n",
      "Epoch 31: loss on validation set: 0.6806\n",
      "Epoch 32: loss on final training batch: 0.6658\n",
      "Epoch 32: accuracy on validation set: 0.5350\n",
      "Epoch 32: loss on validation set: 0.6801\n",
      "Epoch 33: loss on final training batch: 0.6653\n",
      "Epoch 33: accuracy on validation set: 0.5375\n",
      "Epoch 33: loss on validation set: 0.6799\n",
      "Epoch 34: loss on final training batch: 0.6650\n",
      "Epoch 34: accuracy on validation set: 0.5323\n",
      "Epoch 34: loss on validation set: 0.6802\n",
      "Epoch 35: loss on final training batch: 0.6642\n",
      "Epoch 35: accuracy on validation set: 0.5367\n",
      "Epoch 35: loss on validation set: 0.6799\n",
      "Epoch 36: loss on final training batch: 0.6626\n",
      "Epoch 36: accuracy on validation set: 0.5367\n",
      "Epoch 36: loss on validation set: 0.6801\n",
      "Epoch 37: loss on final training batch: 0.6610\n",
      "Epoch 37: accuracy on validation set: 0.5385\n",
      "Epoch 37: loss on validation set: 0.6795\n",
      "Epoch 38: loss on final training batch: 0.6612\n",
      "Epoch 38: accuracy on validation set: 0.5400\n",
      "Epoch 38: loss on validation set: 0.6797\n",
      "Epoch 39: loss on final training batch: 0.6604\n",
      "Epoch 39: accuracy on validation set: 0.5396\n",
      "Epoch 39: loss on validation set: 0.6793\n",
      "Epoch 40: loss on final training batch: 0.6626\n",
      "Epoch 40: accuracy on validation set: 0.5360\n",
      "Epoch 40: loss on validation set: 0.6792\n",
      "Epoch 41: loss on final training batch: 0.6598\n",
      "Epoch 41: accuracy on validation set: 0.5354\n",
      "Epoch 41: loss on validation set: 0.6796\n",
      "Epoch 42: loss on final training batch: 0.6593\n",
      "Epoch 42: accuracy on validation set: 0.5358\n",
      "Epoch 42: loss on validation set: 0.6796\n",
      "Epoch 43: loss on final training batch: 0.6599\n",
      "Epoch 43: accuracy on validation set: 0.5412\n",
      "Epoch 43: loss on validation set: 0.6800\n",
      "Epoch 44: loss on final training batch: 0.6579\n",
      "Epoch 44: accuracy on validation set: 0.5404\n",
      "Epoch 44: loss on validation set: 0.6799\n",
      "Epoch 45: loss on final training batch: 0.6576\n",
      "Epoch 45: accuracy on validation set: 0.5390\n",
      "Epoch 45: loss on validation set: 0.6794\n",
      "Epoch 46: loss on final training batch: 0.6553\n",
      "Epoch 46: accuracy on validation set: 0.5350\n",
      "Epoch 46: loss on validation set: 0.6790\n",
      "Epoch 47: loss on final training batch: 0.6578\n",
      "Epoch 47: accuracy on validation set: 0.5369\n",
      "Epoch 47: loss on validation set: 0.6796\n",
      "Epoch 48: loss on final training batch: 0.6581\n",
      "Epoch 48: accuracy on validation set: 0.5392\n",
      "Epoch 48: loss on validation set: 0.6797\n",
      "Epoch 49: loss on final training batch: 0.6554\n",
      "Epoch 49: accuracy on validation set: 0.5412\n",
      "Epoch 49: loss on validation set: 0.6799\n",
      "Epoch 50: loss on final training batch: 0.6561\n",
      "Epoch 50: accuracy on validation set: 0.5398\n",
      "Epoch 50: loss on validation set: 0.6797\n",
      "Epoch 51: loss on final training batch: 0.6570\n",
      "Epoch 51: accuracy on validation set: 0.5369\n",
      "Epoch 51: loss on validation set: 0.6792\n",
      "Epoch 52: loss on final training batch: 0.6541\n",
      "Epoch 52: accuracy on validation set: 0.5369\n",
      "Epoch 52: loss on validation set: 0.6800\n",
      "Epoch 53: loss on final training batch: 0.6534\n",
      "Epoch 53: accuracy on validation set: 0.5402\n",
      "Epoch 53: loss on validation set: 0.6801\n",
      "Epoch 54: loss on final training batch: 0.6549\n",
      "Epoch 54: accuracy on validation set: 0.5385\n",
      "Epoch 54: loss on validation set: 0.6800\n",
      "Epoch 55: loss on final training batch: 0.6525\n",
      "Epoch 55: accuracy on validation set: 0.5365\n",
      "Epoch 55: loss on validation set: 0.6802\n",
      "Epoch 56: loss on final training batch: 0.6533\n",
      "Epoch 56: accuracy on validation set: 0.5373\n",
      "Epoch 56: loss on validation set: 0.6802\n",
      "Epoch 57: loss on final training batch: 0.6556\n",
      "Epoch 57: accuracy on validation set: 0.5356\n",
      "Epoch 57: loss on validation set: 0.6798\n",
      "Epoch 58: loss on final training batch: 0.6520\n",
      "Epoch 58: accuracy on validation set: 0.5377\n",
      "Epoch 58: loss on validation set: 0.6804\n",
      "Epoch 59: loss on final training batch: 0.6548\n",
      "Epoch 59: accuracy on validation set: 0.5385\n",
      "Epoch 59: loss on validation set: 0.6800\n",
      "Epoch 60: loss on final training batch: 0.6551\n",
      "Epoch 60: accuracy on validation set: 0.5404\n",
      "Epoch 60: loss on validation set: 0.6805\n",
      "Epoch 61: loss on final training batch: 0.6561\n",
      "Epoch 61: accuracy on validation set: 0.5375\n",
      "Epoch 61: loss on validation set: 0.6806\n",
      "Epoch 62: loss on final training batch: 0.6511\n",
      "Epoch 62: accuracy on validation set: 0.5354\n",
      "Epoch 62: loss on validation set: 0.6802\n",
      "Epoch 63: loss on final training batch: 0.6505\n",
      "Epoch 63: accuracy on validation set: 0.5365\n",
      "Epoch 63: loss on validation set: 0.6807\n",
      "Epoch 64: loss on final training batch: 0.6535\n",
      "Epoch 64: accuracy on validation set: 0.5356\n",
      "Epoch 64: loss on validation set: 0.6806\n",
      "Epoch 65: loss on final training batch: 0.6515\n",
      "Epoch 65: accuracy on validation set: 0.5375\n",
      "Epoch 65: loss on validation set: 0.6806\n",
      "Epoch 66: loss on final training batch: 0.6495\n",
      "Epoch 66: accuracy on validation set: 0.5329\n",
      "Epoch 66: loss on validation set: 0.6804\n",
      "Epoch 67: loss on final training batch: 0.6515\n",
      "Epoch 67: accuracy on validation set: 0.5346\n",
      "Epoch 67: loss on validation set: 0.6808\n",
      "Epoch 68: loss on final training batch: 0.6508\n",
      "Epoch 68: accuracy on validation set: 0.5362\n",
      "Epoch 68: loss on validation set: 0.6812\n",
      "Epoch 69: loss on final training batch: 0.6496\n",
      "Epoch 69: accuracy on validation set: 0.5358\n",
      "Epoch 69: loss on validation set: 0.6809\n",
      "Epoch 70: loss on final training batch: 0.6497\n",
      "Epoch 70: accuracy on validation set: 0.5365\n",
      "Epoch 70: loss on validation set: 0.6817\n",
      "Epoch 71: loss on final training batch: 0.6529\n",
      "Epoch 71: accuracy on validation set: 0.5383\n",
      "Epoch 71: loss on validation set: 0.6813\n",
      "Epoch 72: loss on final training batch: 0.6526\n",
      "Epoch 72: accuracy on validation set: 0.5371\n",
      "Epoch 72: loss on validation set: 0.6815\n",
      "Epoch 73: loss on final training batch: 0.6512\n",
      "Epoch 73: accuracy on validation set: 0.5319\n",
      "Epoch 73: loss on validation set: 0.6817\n",
      "Epoch 74: loss on final training batch: 0.6487\n",
      "Epoch 74: accuracy on validation set: 0.5333\n",
      "Epoch 74: loss on validation set: 0.6810\n",
      "Epoch 75: loss on final training batch: 0.6454\n",
      "Epoch 75: accuracy on validation set: 0.5315\n",
      "Epoch 75: loss on validation set: 0.6815\n",
      "Epoch 76: loss on final training batch: 0.6469\n",
      "Epoch 76: accuracy on validation set: 0.5354\n",
      "Epoch 76: loss on validation set: 0.6815\n",
      "Epoch 77: loss on final training batch: 0.6518\n",
      "Epoch 77: accuracy on validation set: 0.5342\n",
      "Epoch 77: loss on validation set: 0.6821\n",
      "Epoch 78: loss on final training batch: 0.6502\n",
      "Epoch 78: accuracy on validation set: 0.5308\n",
      "Epoch 78: loss on validation set: 0.6814\n",
      "Epoch 79: loss on final training batch: 0.6478\n",
      "Epoch 79: accuracy on validation set: 0.5317\n",
      "Epoch 79: loss on validation set: 0.6817\n",
      "Epoch 80: loss on final training batch: 0.6478\n",
      "Epoch 80: accuracy on validation set: 0.5367\n",
      "Epoch 80: loss on validation set: 0.6818\n",
      "Epoch 81: loss on final training batch: 0.6466\n",
      "Epoch 81: accuracy on validation set: 0.5352\n",
      "Epoch 81: loss on validation set: 0.6823\n",
      "Epoch 82: loss on final training batch: 0.6462\n",
      "Epoch 82: accuracy on validation set: 0.5346\n",
      "Epoch 82: loss on validation set: 0.6820\n",
      "Epoch 83: loss on final training batch: 0.6473\n",
      "Epoch 83: accuracy on validation set: 0.5354\n",
      "Epoch 83: loss on validation set: 0.6821\n",
      "Epoch 84: loss on final training batch: 0.6474\n",
      "Epoch 84: accuracy on validation set: 0.5310\n",
      "Epoch 84: loss on validation set: 0.6824\n",
      "Epoch 85: loss on final training batch: 0.6453\n",
      "Epoch 85: accuracy on validation set: 0.5335\n",
      "Epoch 85: loss on validation set: 0.6819\n",
      "Epoch 86: loss on final training batch: 0.6469\n",
      "Epoch 86: accuracy on validation set: 0.5333\n",
      "Epoch 86: loss on validation set: 0.6828\n",
      "Epoch 87: loss on final training batch: 0.6483\n",
      "Epoch 87: accuracy on validation set: 0.5375\n",
      "Epoch 87: loss on validation set: 0.6826\n",
      "Epoch 88: loss on final training batch: 0.6431\n",
      "Epoch 88: accuracy on validation set: 0.5354\n",
      "Epoch 88: loss on validation set: 0.6831\n",
      "Epoch 89: loss on final training batch: 0.6473\n",
      "Epoch 89: accuracy on validation set: 0.5312\n",
      "Epoch 89: loss on validation set: 0.6829\n",
      "Epoch 90: loss on final training batch: 0.6445\n",
      "Epoch 90: accuracy on validation set: 0.5340\n",
      "Epoch 90: loss on validation set: 0.6829\n",
      "Epoch 91: loss on final training batch: 0.6423\n",
      "Epoch 91: accuracy on validation set: 0.5288\n",
      "Epoch 91: loss on validation set: 0.6830\n",
      "Epoch 92: loss on final training batch: 0.6470\n",
      "Epoch 92: accuracy on validation set: 0.5348\n",
      "Epoch 92: loss on validation set: 0.6827\n",
      "Epoch 93: loss on final training batch: 0.6468\n",
      "Epoch 93: accuracy on validation set: 0.5369\n",
      "Epoch 93: loss on validation set: 0.6834\n",
      "Epoch 94: loss on final training batch: 0.6432\n",
      "Epoch 94: accuracy on validation set: 0.5315\n",
      "Epoch 94: loss on validation set: 0.6828\n",
      "Epoch 95: loss on final training batch: 0.6409\n",
      "Epoch 95: accuracy on validation set: 0.5344\n",
      "Epoch 95: loss on validation set: 0.6830\n",
      "Epoch 96: loss on final training batch: 0.6443\n",
      "Epoch 96: accuracy on validation set: 0.5323\n",
      "Epoch 96: loss on validation set: 0.6843\n",
      "Epoch 97: loss on final training batch: 0.6415\n",
      "Epoch 97: accuracy on validation set: 0.5335\n",
      "Epoch 97: loss on validation set: 0.6845\n",
      "Epoch 98: loss on final training batch: 0.6424\n",
      "Epoch 98: accuracy on validation set: 0.5348\n",
      "Epoch 98: loss on validation set: 0.6839\n",
      "Epoch 99: loss on final training batch: 0.6434\n",
      "Epoch 99: accuracy on validation set: 0.5310\n",
      "Epoch 99: loss on validation set: 0.6838\n",
      "Epoch 100: loss on final training batch: 0.6419\n",
      "Epoch 100: accuracy on validation set: 0.5325\n",
      "Epoch 100: loss on validation set: 0.6839\n",
      "Epoch 101: loss on final training batch: 0.6450\n",
      "Epoch 101: accuracy on validation set: 0.5346\n",
      "Epoch 101: loss on validation set: 0.6845\n",
      "Epoch 102: loss on final training batch: 0.6415\n",
      "Epoch 102: accuracy on validation set: 0.5342\n",
      "Epoch 102: loss on validation set: 0.6845\n",
      "Epoch 103: loss on final training batch: 0.6447\n",
      "Epoch 103: accuracy on validation set: 0.5367\n",
      "Epoch 103: loss on validation set: 0.6849\n",
      "Epoch 104: loss on final training batch: 0.6395\n",
      "Epoch 104: accuracy on validation set: 0.5308\n",
      "Epoch 104: loss on validation set: 0.6849\n",
      "Epoch 105: loss on final training batch: 0.6390\n",
      "Epoch 105: accuracy on validation set: 0.5323\n",
      "Epoch 105: loss on validation set: 0.6850\n",
      "Epoch 106: loss on final training batch: 0.6377\n",
      "Epoch 106: accuracy on validation set: 0.5344\n",
      "Epoch 106: loss on validation set: 0.6845\n",
      "Epoch 107: loss on final training batch: 0.6378\n",
      "Epoch 107: accuracy on validation set: 0.5337\n",
      "Epoch 107: loss on validation set: 0.6845\n",
      "Epoch 108: loss on final training batch: 0.6402\n",
      "Epoch 108: accuracy on validation set: 0.5346\n",
      "Epoch 108: loss on validation set: 0.6849\n",
      "Epoch 109: loss on final training batch: 0.6394\n",
      "Epoch 109: accuracy on validation set: 0.5390\n",
      "Epoch 109: loss on validation set: 0.6855\n",
      "Epoch 110: loss on final training batch: 0.6402\n",
      "Epoch 110: accuracy on validation set: 0.5354\n",
      "Epoch 110: loss on validation set: 0.6855\n",
      "Epoch 111: loss on final training batch: 0.6370\n",
      "Epoch 111: accuracy on validation set: 0.5352\n",
      "Epoch 111: loss on validation set: 0.6856\n",
      "Epoch 112: loss on final training batch: 0.6350\n",
      "Epoch 112: accuracy on validation set: 0.5300\n",
      "Epoch 112: loss on validation set: 0.6852\n",
      "Epoch 113: loss on final training batch: 0.6376\n",
      "Epoch 113: accuracy on validation set: 0.5279\n",
      "Epoch 113: loss on validation set: 0.6860\n",
      "Epoch 114: loss on final training batch: 0.6367\n",
      "Epoch 114: accuracy on validation set: 0.5352\n",
      "Epoch 114: loss on validation set: 0.6859\n",
      "Epoch 115: loss on final training batch: 0.6367\n",
      "Epoch 115: accuracy on validation set: 0.5346\n",
      "Epoch 115: loss on validation set: 0.6861\n",
      "Epoch 116: loss on final training batch: 0.6350\n",
      "Epoch 116: accuracy on validation set: 0.5312\n",
      "Epoch 116: loss on validation set: 0.6857\n",
      "Epoch 117: loss on final training batch: 0.6334\n",
      "Epoch 117: accuracy on validation set: 0.5373\n",
      "Epoch 117: loss on validation set: 0.6865\n",
      "Epoch 118: loss on final training batch: 0.6295\n",
      "Epoch 118: accuracy on validation set: 0.5288\n",
      "Epoch 118: loss on validation set: 0.6871\n",
      "Epoch 119: loss on final training batch: 0.6357\n",
      "Epoch 119: accuracy on validation set: 0.5358\n",
      "Epoch 119: loss on validation set: 0.6868\n",
      "Epoch 120: loss on final training batch: 0.6302\n",
      "Epoch 120: accuracy on validation set: 0.5333\n",
      "Epoch 120: loss on validation set: 0.6871\n"
     ]
    }
   ],
   "source": [
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,138),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(138,69),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.1),\n",
    "    torch.nn.Linear(69,1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#loss function and optimizer\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.Adam(model2.parameters(),lr=1.5e-5, weight_decay=2.2e-5)\n",
    "\n",
    "#epoch number\n",
    "num_epoch = 120\n",
    "next_epoch = 1\n",
    "batch_size = 160\n",
    "\n",
    "#training loop\n",
    "for epoch in range(next_epoch, next_epoch+num_epoch):\n",
    "    \n",
    "    \n",
    "    # Make an entire pass (an 'epoch') over the training data in batch_size chunks\n",
    "    for i in range(0, len(X_train), batch_size):        \n",
    "        X = X_train[i:i+batch_size]     # Slice out a mini-batch of features\n",
    "        y = y_train[i:i+batch_size]     # Slice out a mini-batch of targets\n",
    "        \n",
    "        y_pred = model2(X)                   # Make predictions (final-layer activations)\n",
    "        \n",
    "        l = loss(y_pred, y)                 # Compute loss with respect to predictions\n",
    "        \n",
    "        model2.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\n",
    "        l.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\n",
    "        optim.step()                    # Use the gradients to take a step with SGD.\n",
    "        \n",
    "    print(\"Epoch %2d: loss on final training batch: %.4f\" % (epoch, l.item()))\n",
    "    # pred = torch.sign(model(X_val))\n",
    "    # acc = torch.mean((pred == y_val).float())\n",
    "    \n",
    "    #create dire query\n",
    "    # dire_X =  torch.index_select(X, 1, torch.LongTensor([*range(138,276)]))\n",
    "    # dire_X = torch.cat((dire_X,torch.index_select(X, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "\n",
    "    dire_X =  torch.index_select(X_val, 1, torch.LongTensor([*range(138,276)]))\n",
    "    dire_X = torch.cat((dire_X,torch.index_select(X_val, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "\n",
    "    dire_pred = (model2(dire_X) >= 0).float()\n",
    "    \n",
    "    rad_pred = (model2(X_val) >= 0).float()\n",
    "    \n",
    "\n",
    "    overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "    \n",
    "    acc = torch.mean((overall_prob == y_val).float())\n",
    "    \n",
    "    print(\"Epoch %2d: accuracy on validation set: %.4f\" % (epoch, acc))\n",
    "    \n",
    "    print(\"Epoch %2d: loss on validation set: %.4f\" % (epoch, loss(model2(X_val), y_val)))\n",
    "    \n",
    "   \n",
    "\n",
    "next_epoch = epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.522)\n"
     ]
    }
   ],
   "source": [
    "#test accuracy on test set \n",
    "dire_X =  torch.index_select(X_test, 1, torch.LongTensor([*range(138,276)]))\n",
    "dire_X = torch.cat((dire_X,torch.index_select(X_test, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "dire_pred = (model2(dire_X) >= 0).float()\n",
    "rad_pred = (model2(X_test) >= 0).float()\n",
    "\n",
    "\n",
    "overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "acc = torch.mean((overall_prob == y_test).float())\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
