{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import warnings\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "np.set_printoptions(precision=3, suppress=True)  # Print as 0.001 instead of 9.876e-4\n",
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding match data from file\n",
    "data = np.loadtxt('newmatchdata.csv',skiprows=1,delimiter=',')\n",
    "X_data = np.array(data[:,1:]).astype(np.int32)\n",
    "y_data = np.array(data[:,:1]).astype(np.int32)\n",
    "\n",
    "#turn all data into feature vector\n",
    "#feature vector creation\n",
    "X_val_trn = torch.zeros((40000,138*2),dtype=torch.float32)\n",
    "j = 0\n",
    "for d in X_data:\n",
    "    for i in range(len(d)):\n",
    "        if(i < len(d)/2):\n",
    "            h = d[i]\n",
    "            X_val_trn[j][h] += 1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            h = d[i]\n",
    "            X_val_trn[j][h+137] += 1\n",
    "      \n",
    "    j += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w2/kkshvcxn49l4c5_1vv9cd2nr0000gn/T/ipykernel_67867/2977093579.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train,dtype=torch.float32)\n",
      "/var/folders/w2/kkshvcxn49l4c5_1vv9cd2nr0000gn/T/ipykernel_67867/2977093579.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val,dtype=torch.float32)\n",
      "/var/folders/w2/kkshvcxn49l4c5_1vv9cd2nr0000gn/T/ipykernel_67867/2977093579.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#sklearn train test split\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_val_trn,y_data, test_size = 0.2, shuffle= True)\n",
    "\n",
    "#split train into train/validation set\n",
    "\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_train,y_train, test_size = 0.15, shuffle= True)\n",
    "\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(X_val,dtype=torch.float32)\n",
    "\n",
    "y_val = torch.tensor(y_val,dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "#sklearn train test split\n",
    "#use test set to calculate error, CCE, accuracy etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model creation\n",
    "#addition of multiple hidden layers and drop rate to help with regularization\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,100),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100,50),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50,1)\n",
    ")\n",
    "\n",
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: loss on final training batch: 0.6910\n",
      "Epoch  1: accuracy on validation set: 0.4963\n",
      "Epoch  1: accuracy on training set: 0.4919\n",
      "Epoch  1: loss on validation set: 0.6930\n",
      "Epoch  2: loss on final training batch: 0.6926\n",
      "Epoch  2: accuracy on validation set: 0.4958\n",
      "Epoch  2: accuracy on training set: 0.4927\n",
      "Epoch  2: loss on validation set: 0.6923\n",
      "Epoch  3: loss on final training batch: 0.6908\n",
      "Epoch  3: accuracy on validation set: 0.5038\n",
      "Epoch  3: accuracy on training set: 0.4992\n",
      "Epoch  3: loss on validation set: 0.6916\n",
      "Epoch  4: loss on final training batch: 0.6890\n",
      "Epoch  4: accuracy on validation set: 0.5154\n",
      "Epoch  4: accuracy on training set: 0.5135\n",
      "Epoch  4: loss on validation set: 0.6899\n",
      "Epoch  5: loss on final training batch: 0.6862\n",
      "Epoch  5: accuracy on validation set: 0.5183\n",
      "Epoch  5: accuracy on training set: 0.5247\n",
      "Epoch  5: loss on validation set: 0.6871\n",
      "Epoch  6: loss on final training batch: 0.6862\n",
      "Epoch  6: accuracy on validation set: 0.5240\n",
      "Epoch  6: accuracy on training set: 0.5297\n",
      "Epoch  6: loss on validation set: 0.6844\n",
      "Epoch  7: loss on final training batch: 0.6780\n",
      "Epoch  7: accuracy on validation set: 0.5304\n",
      "Epoch  7: accuracy on training set: 0.5390\n",
      "Epoch  7: loss on validation set: 0.6821\n",
      "Epoch  8: loss on final training batch: 0.6758\n",
      "Epoch  8: accuracy on validation set: 0.5331\n",
      "Epoch  8: accuracy on training set: 0.5426\n",
      "Epoch  8: loss on validation set: 0.6801\n",
      "Epoch  9: loss on final training batch: 0.6797\n",
      "Epoch  9: accuracy on validation set: 0.5369\n",
      "Epoch  9: accuracy on training set: 0.5439\n",
      "Epoch  9: loss on validation set: 0.6792\n",
      "Epoch 10: loss on final training batch: 0.6693\n",
      "Epoch 10: accuracy on validation set: 0.5327\n",
      "Epoch 10: accuracy on training set: 0.5463\n",
      "Epoch 10: loss on validation set: 0.6785\n",
      "Epoch 11: loss on final training batch: 0.6721\n",
      "Epoch 11: accuracy on validation set: 0.5346\n",
      "Epoch 11: accuracy on training set: 0.5471\n",
      "Epoch 11: loss on validation set: 0.6779\n",
      "Epoch 12: loss on final training batch: 0.6752\n",
      "Epoch 12: accuracy on validation set: 0.5342\n",
      "Epoch 12: accuracy on training set: 0.5504\n",
      "Epoch 12: loss on validation set: 0.6783\n",
      "Epoch 13: loss on final training batch: 0.6722\n",
      "Epoch 13: accuracy on validation set: 0.5342\n",
      "Epoch 13: accuracy on training set: 0.5505\n",
      "Epoch 13: loss on validation set: 0.6789\n",
      "Epoch 14: loss on final training batch: 0.6654\n",
      "Epoch 14: accuracy on validation set: 0.5377\n",
      "Epoch 14: accuracy on training set: 0.5538\n",
      "Epoch 14: loss on validation set: 0.6791\n",
      "Epoch 15: loss on final training batch: 0.6592\n",
      "Epoch 15: accuracy on validation set: 0.5390\n",
      "Epoch 15: accuracy on training set: 0.5563\n",
      "Epoch 15: loss on validation set: 0.6777\n",
      "Epoch 16: loss on final training batch: 0.6506\n",
      "Epoch 16: accuracy on validation set: 0.5379\n",
      "Epoch 16: accuracy on training set: 0.5597\n",
      "Epoch 16: loss on validation set: 0.6789\n",
      "Epoch 17: loss on final training batch: 0.6569\n",
      "Epoch 17: accuracy on validation set: 0.5367\n",
      "Epoch 17: accuracy on training set: 0.5603\n",
      "Epoch 17: loss on validation set: 0.6795\n",
      "Epoch 18: loss on final training batch: 0.6575\n",
      "Epoch 18: accuracy on validation set: 0.5333\n",
      "Epoch 18: accuracy on training set: 0.5620\n",
      "Epoch 18: loss on validation set: 0.6795\n",
      "Epoch 19: loss on final training batch: 0.6636\n",
      "Epoch 19: accuracy on validation set: 0.5348\n",
      "Epoch 19: accuracy on training set: 0.5633\n",
      "Epoch 19: loss on validation set: 0.6792\n",
      "Epoch 20: loss on final training batch: 0.6413\n",
      "Epoch 20: accuracy on validation set: 0.5362\n",
      "Epoch 20: accuracy on training set: 0.5653\n",
      "Epoch 20: loss on validation set: 0.6785\n",
      "Epoch 21: loss on final training batch: 0.6491\n",
      "Epoch 21: accuracy on validation set: 0.5379\n",
      "Epoch 21: accuracy on training set: 0.5673\n",
      "Epoch 21: loss on validation set: 0.6809\n",
      "Epoch 22: loss on final training batch: 0.6523\n",
      "Epoch 22: accuracy on validation set: 0.5362\n",
      "Epoch 22: accuracy on training set: 0.5693\n",
      "Epoch 22: loss on validation set: 0.6797\n",
      "Epoch 23: loss on final training batch: 0.6337\n",
      "Epoch 23: accuracy on validation set: 0.5440\n",
      "Epoch 23: accuracy on training set: 0.5685\n",
      "Epoch 23: loss on validation set: 0.6797\n",
      "Epoch 24: loss on final training batch: 0.6420\n",
      "Epoch 24: accuracy on validation set: 0.5417\n",
      "Epoch 24: accuracy on training set: 0.5726\n",
      "Epoch 24: loss on validation set: 0.6803\n",
      "Epoch 25: loss on final training batch: 0.6199\n",
      "Epoch 25: accuracy on validation set: 0.5410\n",
      "Epoch 25: accuracy on training set: 0.5751\n",
      "Epoch 25: loss on validation set: 0.6804\n",
      "Epoch 26: loss on final training batch: 0.6332\n",
      "Epoch 26: accuracy on validation set: 0.5387\n",
      "Epoch 26: accuracy on training set: 0.5754\n",
      "Epoch 26: loss on validation set: 0.6806\n",
      "Epoch 27: loss on final training batch: 0.6186\n",
      "Epoch 27: accuracy on validation set: 0.5406\n",
      "Epoch 27: accuracy on training set: 0.5811\n",
      "Epoch 27: loss on validation set: 0.6829\n",
      "Epoch 28: loss on final training batch: 0.6211\n",
      "Epoch 28: accuracy on validation set: 0.5412\n",
      "Epoch 28: accuracy on training set: 0.5824\n",
      "Epoch 28: loss on validation set: 0.6832\n",
      "Epoch 29: loss on final training batch: 0.6241\n",
      "Epoch 29: accuracy on validation set: 0.5400\n",
      "Epoch 29: accuracy on training set: 0.5861\n",
      "Epoch 29: loss on validation set: 0.6852\n",
      "Epoch 30: loss on final training batch: 0.6040\n",
      "Epoch 30: accuracy on validation set: 0.5377\n",
      "Epoch 30: accuracy on training set: 0.5864\n",
      "Epoch 30: loss on validation set: 0.6841\n",
      "Epoch 31: loss on final training batch: 0.5950\n",
      "Epoch 31: accuracy on validation set: 0.5408\n",
      "Epoch 31: accuracy on training set: 0.5869\n",
      "Epoch 31: loss on validation set: 0.6866\n",
      "Epoch 32: loss on final training batch: 0.6093\n",
      "Epoch 32: accuracy on validation set: 0.5365\n",
      "Epoch 32: accuracy on training set: 0.5921\n",
      "Epoch 32: loss on validation set: 0.6866\n",
      "Epoch 33: loss on final training batch: 0.5942\n",
      "Epoch 33: accuracy on validation set: 0.5408\n",
      "Epoch 33: accuracy on training set: 0.5926\n",
      "Epoch 33: loss on validation set: 0.6861\n",
      "Epoch 34: loss on final training batch: 0.5885\n",
      "Epoch 34: accuracy on validation set: 0.5369\n",
      "Epoch 34: accuracy on training set: 0.5967\n",
      "Epoch 34: loss on validation set: 0.6912\n",
      "Epoch 35: loss on final training batch: 0.5809\n",
      "Epoch 35: accuracy on validation set: 0.5369\n",
      "Epoch 35: accuracy on training set: 0.5955\n",
      "Epoch 35: loss on validation set: 0.6927\n",
      "Epoch 36: loss on final training batch: 0.5829\n",
      "Epoch 36: accuracy on validation set: 0.5352\n",
      "Epoch 36: accuracy on training set: 0.6031\n",
      "Epoch 36: loss on validation set: 0.6971\n",
      "Epoch 37: loss on final training batch: 0.5758\n",
      "Epoch 37: accuracy on validation set: 0.5365\n",
      "Epoch 37: accuracy on training set: 0.6033\n",
      "Epoch 37: loss on validation set: 0.6950\n",
      "Epoch 38: loss on final training batch: 0.5634\n",
      "Epoch 38: accuracy on validation set: 0.5352\n",
      "Epoch 38: accuracy on training set: 0.6116\n",
      "Epoch 38: loss on validation set: 0.7000\n",
      "Epoch 39: loss on final training batch: 0.5609\n",
      "Epoch 39: accuracy on validation set: 0.5263\n",
      "Epoch 39: accuracy on training set: 0.6100\n",
      "Epoch 39: loss on validation set: 0.7011\n",
      "Epoch 40: loss on final training batch: 0.5538\n",
      "Epoch 40: accuracy on validation set: 0.5265\n",
      "Epoch 40: accuracy on training set: 0.6141\n",
      "Epoch 40: loss on validation set: 0.7032\n",
      "Epoch 41: loss on final training batch: 0.5480\n",
      "Epoch 41: accuracy on validation set: 0.5221\n",
      "Epoch 41: accuracy on training set: 0.6179\n",
      "Epoch 41: loss on validation set: 0.7071\n",
      "Epoch 42: loss on final training batch: 0.5559\n",
      "Epoch 42: accuracy on validation set: 0.5292\n",
      "Epoch 42: accuracy on training set: 0.6156\n",
      "Epoch 42: loss on validation set: 0.7095\n",
      "Epoch 43: loss on final training batch: 0.5591\n",
      "Epoch 43: accuracy on validation set: 0.5323\n",
      "Epoch 43: accuracy on training set: 0.6189\n",
      "Epoch 43: loss on validation set: 0.7168\n",
      "Epoch 44: loss on final training batch: 0.5645\n",
      "Epoch 44: accuracy on validation set: 0.5310\n",
      "Epoch 44: accuracy on training set: 0.6206\n",
      "Epoch 44: loss on validation set: 0.7191\n",
      "Epoch 45: loss on final training batch: 0.4999\n",
      "Epoch 45: accuracy on validation set: 0.5348\n",
      "Epoch 45: accuracy on training set: 0.6215\n",
      "Epoch 45: loss on validation set: 0.7230\n",
      "Epoch 46: loss on final training batch: 0.5216\n",
      "Epoch 46: accuracy on validation set: 0.5298\n",
      "Epoch 46: accuracy on training set: 0.6198\n",
      "Epoch 46: loss on validation set: 0.7329\n",
      "Epoch 47: loss on final training batch: 0.5384\n",
      "Epoch 47: accuracy on validation set: 0.5233\n",
      "Epoch 47: accuracy on training set: 0.6233\n",
      "Epoch 47: loss on validation set: 0.7324\n",
      "Epoch 48: loss on final training batch: 0.5461\n",
      "Epoch 48: accuracy on validation set: 0.5288\n",
      "Epoch 48: accuracy on training set: 0.6264\n",
      "Epoch 48: loss on validation set: 0.7322\n",
      "Epoch 49: loss on final training batch: 0.4964\n",
      "Epoch 49: accuracy on validation set: 0.5156\n",
      "Epoch 49: accuracy on training set: 0.6237\n",
      "Epoch 49: loss on validation set: 0.7438\n",
      "Epoch 50: loss on final training batch: 0.5057\n",
      "Epoch 50: accuracy on validation set: 0.5215\n",
      "Epoch 50: accuracy on training set: 0.6258\n",
      "Epoch 50: loss on validation set: 0.7470\n",
      "Epoch 51: loss on final training batch: 0.4908\n",
      "Epoch 51: accuracy on validation set: 0.5250\n",
      "Epoch 51: accuracy on training set: 0.6269\n",
      "Epoch 51: loss on validation set: 0.7572\n",
      "Epoch 52: loss on final training batch: 0.5075\n",
      "Epoch 52: accuracy on validation set: 0.5142\n",
      "Epoch 52: accuracy on training set: 0.6272\n",
      "Epoch 52: loss on validation set: 0.7531\n",
      "Epoch 53: loss on final training batch: 0.4880\n",
      "Epoch 53: accuracy on validation set: 0.5146\n",
      "Epoch 53: accuracy on training set: 0.6299\n",
      "Epoch 53: loss on validation set: 0.7654\n",
      "Epoch 54: loss on final training batch: 0.4932\n",
      "Epoch 54: accuracy on validation set: 0.5181\n",
      "Epoch 54: accuracy on training set: 0.6297\n",
      "Epoch 54: loss on validation set: 0.7699\n",
      "Epoch 55: loss on final training batch: 0.4698\n",
      "Epoch 55: accuracy on validation set: 0.5288\n",
      "Epoch 55: accuracy on training set: 0.6282\n",
      "Epoch 55: loss on validation set: 0.7768\n",
      "Epoch 56: loss on final training batch: 0.4987\n",
      "Epoch 56: accuracy on validation set: 0.5229\n",
      "Epoch 56: accuracy on training set: 0.6328\n",
      "Epoch 56: loss on validation set: 0.7769\n",
      "Epoch 57: loss on final training batch: 0.4830\n",
      "Epoch 57: accuracy on validation set: 0.5196\n",
      "Epoch 57: accuracy on training set: 0.6325\n",
      "Epoch 57: loss on validation set: 0.7783\n",
      "Epoch 58: loss on final training batch: 0.4992\n",
      "Epoch 58: accuracy on validation set: 0.5206\n",
      "Epoch 58: accuracy on training set: 0.6327\n",
      "Epoch 58: loss on validation set: 0.7872\n",
      "Epoch 59: loss on final training batch: 0.4697\n",
      "Epoch 59: accuracy on validation set: 0.5146\n",
      "Epoch 59: accuracy on training set: 0.6399\n",
      "Epoch 59: loss on validation set: 0.7965\n",
      "Epoch 60: loss on final training batch: 0.5051\n",
      "Epoch 60: accuracy on validation set: 0.5163\n",
      "Epoch 60: accuracy on training set: 0.6368\n",
      "Epoch 60: loss on validation set: 0.7951\n",
      "Epoch 61: loss on final training batch: 0.4746\n",
      "Epoch 61: accuracy on validation set: 0.5240\n",
      "Epoch 61: accuracy on training set: 0.6392\n",
      "Epoch 61: loss on validation set: 0.8038\n",
      "Epoch 62: loss on final training batch: 0.4461\n",
      "Epoch 62: accuracy on validation set: 0.5196\n",
      "Epoch 62: accuracy on training set: 0.6374\n",
      "Epoch 62: loss on validation set: 0.8024\n",
      "Epoch 63: loss on final training batch: 0.4536\n",
      "Epoch 63: accuracy on validation set: 0.5146\n",
      "Epoch 63: accuracy on training set: 0.6349\n",
      "Epoch 63: loss on validation set: 0.8161\n",
      "Epoch 64: loss on final training batch: 0.4578\n",
      "Epoch 64: accuracy on validation set: 0.5150\n",
      "Epoch 64: accuracy on training set: 0.6403\n",
      "Epoch 64: loss on validation set: 0.8192\n",
      "Epoch 65: loss on final training batch: 0.4377\n",
      "Epoch 65: accuracy on validation set: 0.5127\n",
      "Epoch 65: accuracy on training set: 0.6451\n",
      "Epoch 65: loss on validation set: 0.8290\n",
      "Epoch 66: loss on final training batch: 0.4447\n",
      "Epoch 66: accuracy on validation set: 0.5158\n",
      "Epoch 66: accuracy on training set: 0.6431\n",
      "Epoch 66: loss on validation set: 0.8268\n",
      "Epoch 67: loss on final training batch: 0.4607\n",
      "Epoch 67: accuracy on validation set: 0.5235\n",
      "Epoch 67: accuracy on training set: 0.6478\n",
      "Epoch 67: loss on validation set: 0.8339\n",
      "Epoch 68: loss on final training batch: 0.4573\n",
      "Epoch 68: accuracy on validation set: 0.5081\n",
      "Epoch 68: accuracy on training set: 0.6466\n",
      "Epoch 68: loss on validation set: 0.8366\n",
      "Epoch 69: loss on final training batch: 0.4491\n",
      "Epoch 69: accuracy on validation set: 0.5125\n",
      "Epoch 69: accuracy on training set: 0.6415\n",
      "Epoch 69: loss on validation set: 0.8482\n",
      "Epoch 70: loss on final training batch: 0.5008\n",
      "Epoch 70: accuracy on validation set: 0.5115\n",
      "Epoch 70: accuracy on training set: 0.6467\n",
      "Epoch 70: loss on validation set: 0.8525\n",
      "Epoch 71: loss on final training batch: 0.4090\n",
      "Epoch 71: accuracy on validation set: 0.5231\n",
      "Epoch 71: accuracy on training set: 0.6469\n",
      "Epoch 71: loss on validation set: 0.8594\n",
      "Epoch 72: loss on final training batch: 0.4503\n",
      "Epoch 72: accuracy on validation set: 0.5183\n",
      "Epoch 72: accuracy on training set: 0.6481\n",
      "Epoch 72: loss on validation set: 0.8605\n",
      "Epoch 73: loss on final training batch: 0.4032\n",
      "Epoch 73: accuracy on validation set: 0.5110\n",
      "Epoch 73: accuracy on training set: 0.6501\n",
      "Epoch 73: loss on validation set: 0.8635\n",
      "Epoch 74: loss on final training batch: 0.4331\n",
      "Epoch 74: accuracy on validation set: 0.5135\n",
      "Epoch 74: accuracy on training set: 0.6486\n",
      "Epoch 74: loss on validation set: 0.8713\n",
      "Epoch 75: loss on final training batch: 0.3665\n",
      "Epoch 75: accuracy on validation set: 0.5125\n",
      "Epoch 75: accuracy on training set: 0.6502\n",
      "Epoch 75: loss on validation set: 0.8680\n",
      "Epoch 76: loss on final training batch: 0.3962\n",
      "Epoch 76: accuracy on validation set: 0.5108\n",
      "Epoch 76: accuracy on training set: 0.6531\n",
      "Epoch 76: loss on validation set: 0.8832\n",
      "Epoch 77: loss on final training batch: 0.4553\n",
      "Epoch 77: accuracy on validation set: 0.5135\n",
      "Epoch 77: accuracy on training set: 0.6499\n",
      "Epoch 77: loss on validation set: 0.8930\n",
      "Epoch 78: loss on final training batch: 0.4035\n",
      "Epoch 78: accuracy on validation set: 0.5133\n",
      "Epoch 78: accuracy on training set: 0.6541\n",
      "Epoch 78: loss on validation set: 0.8920\n",
      "Epoch 79: loss on final training batch: 0.4559\n",
      "Epoch 79: accuracy on validation set: 0.5113\n",
      "Epoch 79: accuracy on training set: 0.6547\n",
      "Epoch 79: loss on validation set: 0.8885\n",
      "Epoch 80: loss on final training batch: 0.4991\n",
      "Epoch 80: accuracy on validation set: 0.5181\n",
      "Epoch 80: accuracy on training set: 0.6560\n",
      "Epoch 80: loss on validation set: 0.9014\n",
      "Epoch 81: loss on final training batch: 0.4703\n",
      "Epoch 81: accuracy on validation set: 0.5096\n",
      "Epoch 81: accuracy on training set: 0.6522\n",
      "Epoch 81: loss on validation set: 0.8873\n",
      "Epoch 82: loss on final training batch: 0.3560\n",
      "Epoch 82: accuracy on validation set: 0.5133\n",
      "Epoch 82: accuracy on training set: 0.6539\n",
      "Epoch 82: loss on validation set: 0.9108\n",
      "Epoch 83: loss on final training batch: 0.3336\n",
      "Epoch 83: accuracy on validation set: 0.5150\n",
      "Epoch 83: accuracy on training set: 0.6518\n",
      "Epoch 83: loss on validation set: 0.9094\n",
      "Epoch 84: loss on final training batch: 0.4067\n",
      "Epoch 84: accuracy on validation set: 0.5038\n",
      "Epoch 84: accuracy on training set: 0.6559\n",
      "Epoch 84: loss on validation set: 0.9230\n",
      "Epoch 85: loss on final training batch: 0.3643\n",
      "Epoch 85: accuracy on validation set: 0.5165\n",
      "Epoch 85: accuracy on training set: 0.6611\n",
      "Epoch 85: loss on validation set: 0.9163\n",
      "Epoch 86: loss on final training batch: 0.3575\n",
      "Epoch 86: accuracy on validation set: 0.5167\n",
      "Epoch 86: accuracy on training set: 0.6596\n",
      "Epoch 86: loss on validation set: 0.9273\n",
      "Epoch 87: loss on final training batch: 0.3844\n",
      "Epoch 87: accuracy on validation set: 0.5092\n",
      "Epoch 87: accuracy on training set: 0.6563\n",
      "Epoch 87: loss on validation set: 0.9388\n",
      "Epoch 88: loss on final training batch: 0.3756\n",
      "Epoch 88: accuracy on validation set: 0.5021\n",
      "Epoch 88: accuracy on training set: 0.6616\n",
      "Epoch 88: loss on validation set: 0.9385\n",
      "Epoch 89: loss on final training batch: 0.3347\n",
      "Epoch 89: accuracy on validation set: 0.5192\n",
      "Epoch 89: accuracy on training set: 0.6604\n",
      "Epoch 89: loss on validation set: 0.9356\n",
      "Epoch 90: loss on final training batch: 0.3755\n",
      "Epoch 90: accuracy on validation set: 0.5173\n",
      "Epoch 90: accuracy on training set: 0.6621\n",
      "Epoch 90: loss on validation set: 0.9400\n",
      "Epoch 91: loss on final training batch: 0.4063\n",
      "Epoch 91: accuracy on validation set: 0.5067\n",
      "Epoch 91: accuracy on training set: 0.6643\n",
      "Epoch 91: loss on validation set: 0.9446\n",
      "Epoch 92: loss on final training batch: 0.3825\n",
      "Epoch 92: accuracy on validation set: 0.5098\n",
      "Epoch 92: accuracy on training set: 0.6631\n",
      "Epoch 92: loss on validation set: 0.9545\n",
      "Epoch 93: loss on final training batch: 0.4269\n",
      "Epoch 93: accuracy on validation set: 0.5142\n",
      "Epoch 93: accuracy on training set: 0.6608\n",
      "Epoch 93: loss on validation set: 0.9509\n",
      "Epoch 94: loss on final training batch: 0.3895\n",
      "Epoch 94: accuracy on validation set: 0.5077\n",
      "Epoch 94: accuracy on training set: 0.6681\n",
      "Epoch 94: loss on validation set: 0.9570\n",
      "Epoch 95: loss on final training batch: 0.3744\n",
      "Epoch 95: accuracy on validation set: 0.5069\n",
      "Epoch 95: accuracy on training set: 0.6619\n",
      "Epoch 95: loss on validation set: 0.9682\n",
      "Epoch 96: loss on final training batch: 0.3607\n",
      "Epoch 96: accuracy on validation set: 0.5208\n",
      "Epoch 96: accuracy on training set: 0.6636\n",
      "Epoch 96: loss on validation set: 0.9780\n",
      "Epoch 97: loss on final training batch: 0.3483\n",
      "Epoch 97: accuracy on validation set: 0.5171\n",
      "Epoch 97: accuracy on training set: 0.6625\n",
      "Epoch 97: loss on validation set: 0.9778\n",
      "Epoch 98: loss on final training batch: 0.3452\n",
      "Epoch 98: accuracy on validation set: 0.5179\n",
      "Epoch 98: accuracy on training set: 0.6647\n",
      "Epoch 98: loss on validation set: 0.9739\n",
      "Epoch 99: loss on final training batch: 0.3257\n",
      "Epoch 99: accuracy on validation set: 0.5100\n",
      "Epoch 99: accuracy on training set: 0.6664\n",
      "Epoch 99: loss on validation set: 0.9816\n",
      "Epoch 100: loss on final training batch: 0.4335\n",
      "Epoch 100: accuracy on validation set: 0.5085\n",
      "Epoch 100: accuracy on training set: 0.6658\n",
      "Epoch 100: loss on validation set: 0.9908\n",
      "Epoch 101: loss on final training batch: 0.3125\n",
      "Epoch 101: accuracy on validation set: 0.5100\n",
      "Epoch 101: accuracy on training set: 0.6630\n",
      "Epoch 101: loss on validation set: 0.9813\n",
      "Epoch 102: loss on final training batch: 0.3780\n",
      "Epoch 102: accuracy on validation set: 0.5190\n",
      "Epoch 102: accuracy on training set: 0.6656\n",
      "Epoch 102: loss on validation set: 0.9947\n",
      "Epoch 103: loss on final training batch: 0.4035\n",
      "Epoch 103: accuracy on validation set: 0.5173\n",
      "Epoch 103: accuracy on training set: 0.6662\n",
      "Epoch 103: loss on validation set: 0.9946\n",
      "Epoch 104: loss on final training batch: 0.3990\n",
      "Epoch 104: accuracy on validation set: 0.5054\n",
      "Epoch 104: accuracy on training set: 0.6631\n",
      "Epoch 104: loss on validation set: 1.0049\n",
      "Epoch 105: loss on final training batch: 0.2866\n",
      "Epoch 105: accuracy on validation set: 0.5077\n",
      "Epoch 105: accuracy on training set: 0.6657\n",
      "Epoch 105: loss on validation set: 1.0071\n",
      "Epoch 106: loss on final training batch: 0.3463\n",
      "Epoch 106: accuracy on validation set: 0.5056\n",
      "Epoch 106: accuracy on training set: 0.6713\n",
      "Epoch 106: loss on validation set: 1.0228\n",
      "Epoch 107: loss on final training batch: 0.3718\n",
      "Epoch 107: accuracy on validation set: 0.5121\n",
      "Epoch 107: accuracy on training set: 0.6737\n",
      "Epoch 107: loss on validation set: 1.0136\n",
      "Epoch 108: loss on final training batch: 0.3475\n",
      "Epoch 108: accuracy on validation set: 0.5142\n",
      "Epoch 108: accuracy on training set: 0.6680\n",
      "Epoch 108: loss on validation set: 1.0126\n",
      "Epoch 109: loss on final training batch: 0.3764\n",
      "Epoch 109: accuracy on validation set: 0.5108\n",
      "Epoch 109: accuracy on training set: 0.6692\n",
      "Epoch 109: loss on validation set: 1.0317\n",
      "Epoch 110: loss on final training batch: 0.3318\n",
      "Epoch 110: accuracy on validation set: 0.5106\n",
      "Epoch 110: accuracy on training set: 0.6652\n",
      "Epoch 110: loss on validation set: 1.0372\n",
      "Epoch 111: loss on final training batch: 0.3498\n",
      "Epoch 111: accuracy on validation set: 0.5106\n",
      "Epoch 111: accuracy on training set: 0.6711\n",
      "Epoch 111: loss on validation set: 1.0449\n",
      "Epoch 112: loss on final training batch: 0.4217\n",
      "Epoch 112: accuracy on validation set: 0.5100\n",
      "Epoch 112: accuracy on training set: 0.6718\n",
      "Epoch 112: loss on validation set: 1.0443\n",
      "Epoch 113: loss on final training batch: 0.3872\n",
      "Epoch 113: accuracy on validation set: 0.5100\n",
      "Epoch 113: accuracy on training set: 0.6688\n",
      "Epoch 113: loss on validation set: 1.0427\n",
      "Epoch 114: loss on final training batch: 0.3170\n",
      "Epoch 114: accuracy on validation set: 0.5150\n",
      "Epoch 114: accuracy on training set: 0.6707\n",
      "Epoch 114: loss on validation set: 1.0311\n",
      "Epoch 115: loss on final training batch: 0.4577\n",
      "Epoch 115: accuracy on validation set: 0.5169\n",
      "Epoch 115: accuracy on training set: 0.6735\n",
      "Epoch 115: loss on validation set: 1.0493\n",
      "Epoch 116: loss on final training batch: 0.3175\n",
      "Epoch 116: accuracy on validation set: 0.5142\n",
      "Epoch 116: accuracy on training set: 0.6733\n",
      "Epoch 116: loss on validation set: 1.0583\n",
      "Epoch 117: loss on final training batch: 0.3607\n",
      "Epoch 117: accuracy on validation set: 0.5167\n",
      "Epoch 117: accuracy on training set: 0.6737\n",
      "Epoch 117: loss on validation set: 1.0393\n",
      "Epoch 118: loss on final training batch: 0.4364\n",
      "Epoch 118: accuracy on validation set: 0.5113\n",
      "Epoch 118: accuracy on training set: 0.6708\n",
      "Epoch 118: loss on validation set: 1.0639\n",
      "Epoch 119: loss on final training batch: 0.3075\n",
      "Epoch 119: accuracy on validation set: 0.5200\n",
      "Epoch 119: accuracy on training set: 0.6778\n",
      "Epoch 119: loss on validation set: 1.0845\n",
      "Epoch 120: loss on final training batch: 0.3098\n",
      "Epoch 120: accuracy on validation set: 0.5146\n",
      "Epoch 120: accuracy on training set: 0.6714\n",
      "Epoch 120: loss on validation set: 1.0718\n"
     ]
    }
   ],
   "source": [
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,138),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.1),\n",
    "    torch.nn.Linear(138,69),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.2),\n",
    "    torch.nn.Linear(69,1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#loss function and optimizer\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.Adam(model2.parameters(),lr=5e-5, weight_decay=2.5e-5)\n",
    "\n",
    "#epoch number\n",
    "num_epoch = 120\n",
    "next_epoch = 1\n",
    "batch_size = 80\n",
    "\n",
    "#training loop\n",
    "for epoch in range(next_epoch, next_epoch+num_epoch):\n",
    "    \n",
    "    \n",
    "    # Make an entire pass (an 'epoch') over the training data in batch_size chunks\n",
    "    for i in range(0, len(X_train), batch_size):        \n",
    "        X = X_train[i:i+batch_size]     # Slice out a mini-batch of features\n",
    "        y = y_train[i:i+batch_size]     # Slice out a mini-batch of targets\n",
    "        \n",
    "        y_pred = model2(X)                   # Make predictions (final-layer activations)\n",
    "        \n",
    "        l = loss(y_pred, y)                 # Compute loss with respect to predictions\n",
    "        \n",
    "        model2.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\n",
    "        l.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\n",
    "        optim.step()                    # Use the gradients to take a step with SGD.\n",
    "        \n",
    "    print(\"Epoch %2d: loss on final training batch: %.4f\" % (epoch, l.item()))\n",
    "    # pred = torch.sign(model(X_val))\n",
    "    # acc = torch.mean((pred == y_val).float())\n",
    "    \n",
    "    #create dire query\n",
    "\n",
    "    #validation set calculations\n",
    "    dire_X =  torch.index_select(X_val, 1, torch.LongTensor([*range(138,276)]))\n",
    "    dire_X = torch.cat((dire_X,torch.index_select(X_val, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "\n",
    "    dire_pred = (model2(dire_X) >= 0).float()\n",
    "    \n",
    "    rad_pred = (model2(X_val) >= 0).float()\n",
    "    \n",
    "\n",
    "    overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "    \n",
    "    val_acc = torch.mean((overall_prob == y_val).float())\n",
    "\n",
    "    #training set calculations\n",
    "    dire_X_train =  torch.index_select(X_train, 1, torch.LongTensor([*range(138,276)]))\n",
    "    dire_X_train = torch.cat((dire_X_train,torch.index_select(X_train, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "\n",
    "    dire_pred = (model2(dire_X_train) >= 0).float()\n",
    "    \n",
    "    rad_pred = (model2(X_train) >= 0).float()\n",
    "    \n",
    "\n",
    "    overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "    \n",
    "    train_acc = torch.mean((overall_prob == y_train).float())\n",
    "    \n",
    "    print(\"Epoch %2d: accuracy on validation set: %.4f\" % (epoch, val_acc))\n",
    "    print(\"Epoch %2d: accuracy on training set: %.4f\" % (epoch, train_acc))\n",
    "    \n",
    "    print(\"Epoch %2d: loss on validation set: %.4f\" % (epoch, loss(model2(X_val), y_val)))\n",
    "    \n",
    "   \n",
    "\n",
    "next_epoch = epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  49.471875 %\n",
      "held-out accuracy (testing):  48.5625 %\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (226846857.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_val_trn,y_data, test_size = 0.2, shuffle= True)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = 4,activation='relu',solver='adam',batch_size=16,learning_rate_init=0.05,max_iter=3000,random_state=0)# Your code here\n",
    "log = LogisticRegression()\n",
    "mlp.fit(X_train,y_train.ravel())\n",
    "log.fit(X_train,y_train.ravel())\n",
    "\n",
    "print('training accuracy: ',mlp.score(X_train,y_train)*100,'%')\n",
    "print('held-out accuracy (testing): ',mlp.score(X_test,y_test)*100,'%')\n",
    "\n",
    "dire_X =  torch.index_select(X_test, 1, torch.LongTensor([*range(138,276)]))\n",
    "dire_X = torch.cat((dire_X, torch.index_select(X_test, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "\n",
    "\n",
    "\n",
    "dire_prob = log.score(dire_X,y_test)\n",
    "rad_prob = log.score(X_test,y_test)\n",
    "\n",
    "overall_prob = ((rad_prob + (1 - dire_prob))/2)\n",
    "\n",
    "print('log training accuracy: ',log.score(X_train,y_train)*100,'%')\n",
    "print('log held-out accuracy (testing): ',log.score(X_test,y_test)*100,'%')\n",
    "\n",
    "print('log overall accuracy: ',overall_prob*100,'%')\n",
    "\n",
    "for k in range(2,20):\n",
    "    scores = sklearn.model_selection.cross_val_score(log,X_test,y_test.ravel(),cv=k) \n",
    "    print(\"held-out accuracy (%d-fold):   %.1f%%\" % (k, scores.mean()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.511)\n"
     ]
    }
   ],
   "source": [
    "#test accuracy on test set \n",
    "dire_X =  torch.index_select(X_test, 1, torch.LongTensor([*range(138,276)]))\n",
    "dire_X = torch.cat((dire_X,torch.index_select(X_test, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "dire_pred = (model2(dire_X) >= 0).float()\n",
    "rad_pred = (model2(X_test) >= 0).float()\n",
    "\n",
    "\n",
    "overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "acc = torch.mean((overall_prob == y_test).float())\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
