{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import warnings\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "np.set_printoptions(precision=3, suppress=True)  # Print as 0.001 instead of 9.876e-4\n",
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding match data from file\n",
    "data = np.loadtxt('newmatchdata.csv',skiprows=1,delimiter=',')\n",
    "X_data = np.array(data[:,1:]).astype(np.int32)\n",
    "y_data = np.array(data[:,:1]).astype(np.int32)\n",
    "\n",
    "#turn all data into feature vector\n",
    "#feature vector creation\n",
    "X_val_trn = torch.zeros((40000,138*2),dtype=torch.float32)\n",
    "j = 0\n",
    "for d in X_data:\n",
    "    for i in range(len(d)):\n",
    "        if(i < len(d)/2):\n",
    "            h = d[i]\n",
    "            X_val_trn[j][h] += 1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            h = d[i]\n",
    "            X_val_trn[j][h+137] += 1\n",
    "      \n",
    "    j += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w2/kkshvcxn49l4c5_1vv9cd2nr0000gn/T/ipykernel_67867/2977093579.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train,dtype=torch.float32)\n",
      "/var/folders/w2/kkshvcxn49l4c5_1vv9cd2nr0000gn/T/ipykernel_67867/2977093579.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val,dtype=torch.float32)\n",
      "/var/folders/w2/kkshvcxn49l4c5_1vv9cd2nr0000gn/T/ipykernel_67867/2977093579.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#sklearn train test split\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_val_trn,y_data, test_size = 0.2, shuffle= True)\n",
    "\n",
    "#split train into train/validation set\n",
    "\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_train,y_train, test_size = 0.15, shuffle= True)\n",
    "\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(X_val,dtype=torch.float32)\n",
    "\n",
    "y_val = torch.tensor(y_val,dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "#sklearn train test split\n",
    "#use test set to calculate error, CCE, accuracy etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model creation\n",
    "#addition of multiple hidden layers and drop rate to help with regularization\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,100),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100,50),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50,1)\n",
    ")\n",
    "\n",
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: loss on final training batch: 0.6960\n",
      "Epoch  1: accuracy on validation set: 0.4915\n",
      "Epoch  1: accuracy on training set: 0.4936\n",
      "Epoch  1: loss on validation set: 0.6929\n",
      "Epoch  2: loss on final training batch: 0.6969\n",
      "Epoch  2: accuracy on validation set: 0.4921\n",
      "Epoch  2: accuracy on training set: 0.4946\n",
      "Epoch  2: loss on validation set: 0.6927\n",
      "Epoch  3: loss on final training batch: 0.6952\n",
      "Epoch  3: accuracy on validation set: 0.4960\n",
      "Epoch  3: accuracy on training set: 0.4956\n",
      "Epoch  3: loss on validation set: 0.6924\n",
      "Epoch  4: loss on final training batch: 0.6935\n",
      "Epoch  4: accuracy on validation set: 0.4971\n",
      "Epoch  4: accuracy on training set: 0.5055\n",
      "Epoch  4: loss on validation set: 0.6915\n",
      "Epoch  5: loss on final training batch: 0.6913\n",
      "Epoch  5: accuracy on validation set: 0.5017\n",
      "Epoch  5: accuracy on training set: 0.5108\n",
      "Epoch  5: loss on validation set: 0.6908\n",
      "Epoch  6: loss on final training batch: 0.6923\n",
      "Epoch  6: accuracy on validation set: 0.5098\n",
      "Epoch  6: accuracy on training set: 0.5197\n",
      "Epoch  6: loss on validation set: 0.6903\n",
      "Epoch  7: loss on final training batch: 0.6907\n",
      "Epoch  7: accuracy on validation set: 0.5075\n",
      "Epoch  7: accuracy on training set: 0.5292\n",
      "Epoch  7: loss on validation set: 0.6892\n",
      "Epoch  8: loss on final training batch: 0.6865\n",
      "Epoch  8: accuracy on validation set: 0.5210\n",
      "Epoch  8: accuracy on training set: 0.5339\n",
      "Epoch  8: loss on validation set: 0.6877\n",
      "Epoch  9: loss on final training batch: 0.6866\n",
      "Epoch  9: accuracy on validation set: 0.5188\n",
      "Epoch  9: accuracy on training set: 0.5400\n",
      "Epoch  9: loss on validation set: 0.6865\n",
      "Epoch 10: loss on final training batch: 0.6875\n",
      "Epoch 10: accuracy on validation set: 0.5194\n",
      "Epoch 10: accuracy on training set: 0.5419\n",
      "Epoch 10: loss on validation set: 0.6848\n",
      "Epoch 11: loss on final training batch: 0.6814\n",
      "Epoch 11: accuracy on validation set: 0.5223\n",
      "Epoch 11: accuracy on training set: 0.5465\n",
      "Epoch 11: loss on validation set: 0.6832\n",
      "Epoch 12: loss on final training batch: 0.6731\n",
      "Epoch 12: accuracy on validation set: 0.5223\n",
      "Epoch 12: accuracy on training set: 0.5474\n",
      "Epoch 12: loss on validation set: 0.6830\n",
      "Epoch 13: loss on final training batch: 0.6719\n",
      "Epoch 13: accuracy on validation set: 0.5271\n",
      "Epoch 13: accuracy on training set: 0.5451\n",
      "Epoch 13: loss on validation set: 0.6812\n",
      "Epoch 14: loss on final training batch: 0.6744\n",
      "Epoch 14: accuracy on validation set: 0.5198\n",
      "Epoch 14: accuracy on training set: 0.5500\n",
      "Epoch 14: loss on validation set: 0.6813\n",
      "Epoch 15: loss on final training batch: 0.6690\n",
      "Epoch 15: accuracy on validation set: 0.5281\n",
      "Epoch 15: accuracy on training set: 0.5507\n",
      "Epoch 15: loss on validation set: 0.6810\n",
      "Epoch 16: loss on final training batch: 0.6679\n",
      "Epoch 16: accuracy on validation set: 0.5308\n",
      "Epoch 16: accuracy on training set: 0.5513\n",
      "Epoch 16: loss on validation set: 0.6820\n",
      "Epoch 17: loss on final training batch: 0.6679\n",
      "Epoch 17: accuracy on validation set: 0.5227\n",
      "Epoch 17: accuracy on training set: 0.5520\n",
      "Epoch 17: loss on validation set: 0.6818\n",
      "Epoch 18: loss on final training batch: 0.6730\n",
      "Epoch 18: accuracy on validation set: 0.5252\n",
      "Epoch 18: accuracy on training set: 0.5521\n",
      "Epoch 18: loss on validation set: 0.6820\n",
      "Epoch 19: loss on final training batch: 0.6718\n",
      "Epoch 19: accuracy on validation set: 0.5340\n",
      "Epoch 19: accuracy on training set: 0.5540\n",
      "Epoch 19: loss on validation set: 0.6818\n",
      "Epoch 20: loss on final training batch: 0.6647\n",
      "Epoch 20: accuracy on validation set: 0.5312\n",
      "Epoch 20: accuracy on training set: 0.5524\n",
      "Epoch 20: loss on validation set: 0.6807\n",
      "Epoch 21: loss on final training batch: 0.6713\n",
      "Epoch 21: accuracy on validation set: 0.5296\n",
      "Epoch 21: accuracy on training set: 0.5550\n",
      "Epoch 21: loss on validation set: 0.6830\n",
      "Epoch 22: loss on final training batch: 0.6705\n",
      "Epoch 22: accuracy on validation set: 0.5350\n",
      "Epoch 22: accuracy on training set: 0.5550\n",
      "Epoch 22: loss on validation set: 0.6835\n",
      "Epoch 23: loss on final training batch: 0.6649\n",
      "Epoch 23: accuracy on validation set: 0.5329\n",
      "Epoch 23: accuracy on training set: 0.5565\n",
      "Epoch 23: loss on validation set: 0.6826\n",
      "Epoch 24: loss on final training batch: 0.6676\n",
      "Epoch 24: accuracy on validation set: 0.5348\n",
      "Epoch 24: accuracy on training set: 0.5590\n",
      "Epoch 24: loss on validation set: 0.6828\n",
      "Epoch 25: loss on final training batch: 0.6582\n",
      "Epoch 25: accuracy on validation set: 0.5304\n",
      "Epoch 25: accuracy on training set: 0.5571\n",
      "Epoch 25: loss on validation set: 0.6832\n",
      "Epoch 26: loss on final training batch: 0.6585\n",
      "Epoch 26: accuracy on validation set: 0.5246\n",
      "Epoch 26: accuracy on training set: 0.5567\n",
      "Epoch 26: loss on validation set: 0.6835\n",
      "Epoch 27: loss on final training batch: 0.6635\n",
      "Epoch 27: accuracy on validation set: 0.5317\n",
      "Epoch 27: accuracy on training set: 0.5596\n",
      "Epoch 27: loss on validation set: 0.6831\n",
      "Epoch 28: loss on final training batch: 0.6560\n",
      "Epoch 28: accuracy on validation set: 0.5296\n",
      "Epoch 28: accuracy on training set: 0.5594\n",
      "Epoch 28: loss on validation set: 0.6833\n",
      "Epoch 29: loss on final training batch: 0.6578\n",
      "Epoch 29: accuracy on validation set: 0.5352\n",
      "Epoch 29: accuracy on training set: 0.5612\n",
      "Epoch 29: loss on validation set: 0.6838\n",
      "Epoch 30: loss on final training batch: 0.6550\n",
      "Epoch 30: accuracy on validation set: 0.5298\n",
      "Epoch 30: accuracy on training set: 0.5629\n",
      "Epoch 30: loss on validation set: 0.6838\n",
      "Epoch 31: loss on final training batch: 0.6575\n",
      "Epoch 31: accuracy on validation set: 0.5250\n",
      "Epoch 31: accuracy on training set: 0.5644\n",
      "Epoch 31: loss on validation set: 0.6835\n",
      "Epoch 32: loss on final training batch: 0.6496\n",
      "Epoch 32: accuracy on validation set: 0.5337\n",
      "Epoch 32: accuracy on training set: 0.5629\n",
      "Epoch 32: loss on validation set: 0.6847\n",
      "Epoch 33: loss on final training batch: 0.6508\n",
      "Epoch 33: accuracy on validation set: 0.5271\n",
      "Epoch 33: accuracy on training set: 0.5671\n",
      "Epoch 33: loss on validation set: 0.6853\n",
      "Epoch 34: loss on final training batch: 0.6450\n",
      "Epoch 34: accuracy on validation set: 0.5298\n",
      "Epoch 34: accuracy on training set: 0.5637\n",
      "Epoch 34: loss on validation set: 0.6848\n",
      "Epoch 35: loss on final training batch: 0.6463\n",
      "Epoch 35: accuracy on validation set: 0.5323\n",
      "Epoch 35: accuracy on training set: 0.5649\n",
      "Epoch 35: loss on validation set: 0.6854\n",
      "Epoch 36: loss on final training batch: 0.6486\n",
      "Epoch 36: accuracy on validation set: 0.5327\n",
      "Epoch 36: accuracy on training set: 0.5688\n",
      "Epoch 36: loss on validation set: 0.6841\n",
      "Epoch 37: loss on final training batch: 0.6488\n",
      "Epoch 37: accuracy on validation set: 0.5285\n",
      "Epoch 37: accuracy on training set: 0.5687\n",
      "Epoch 37: loss on validation set: 0.6864\n",
      "Epoch 38: loss on final training batch: 0.6531\n",
      "Epoch 38: accuracy on validation set: 0.5340\n",
      "Epoch 38: accuracy on training set: 0.5698\n",
      "Epoch 38: loss on validation set: 0.6861\n",
      "Epoch 39: loss on final training batch: 0.6546\n",
      "Epoch 39: accuracy on validation set: 0.5331\n",
      "Epoch 39: accuracy on training set: 0.5695\n",
      "Epoch 39: loss on validation set: 0.6854\n",
      "Epoch 40: loss on final training batch: 0.6502\n",
      "Epoch 40: accuracy on validation set: 0.5254\n",
      "Epoch 40: accuracy on training set: 0.5683\n",
      "Epoch 40: loss on validation set: 0.6854\n",
      "Epoch 41: loss on final training batch: 0.6516\n",
      "Epoch 41: accuracy on validation set: 0.5290\n",
      "Epoch 41: accuracy on training set: 0.5704\n",
      "Epoch 41: loss on validation set: 0.6865\n",
      "Epoch 42: loss on final training batch: 0.6392\n",
      "Epoch 42: accuracy on validation set: 0.5288\n",
      "Epoch 42: accuracy on training set: 0.5760\n",
      "Epoch 42: loss on validation set: 0.6868\n",
      "Epoch 43: loss on final training batch: 0.6343\n",
      "Epoch 43: accuracy on validation set: 0.5254\n",
      "Epoch 43: accuracy on training set: 0.5753\n",
      "Epoch 43: loss on validation set: 0.6892\n",
      "Epoch 44: loss on final training batch: 0.6240\n",
      "Epoch 44: accuracy on validation set: 0.5292\n",
      "Epoch 44: accuracy on training set: 0.5750\n",
      "Epoch 44: loss on validation set: 0.6877\n",
      "Epoch 45: loss on final training batch: 0.6371\n",
      "Epoch 45: accuracy on validation set: 0.5294\n",
      "Epoch 45: accuracy on training set: 0.5760\n",
      "Epoch 45: loss on validation set: 0.6889\n",
      "Epoch 46: loss on final training batch: 0.6280\n",
      "Epoch 46: accuracy on validation set: 0.5279\n",
      "Epoch 46: accuracy on training set: 0.5785\n",
      "Epoch 46: loss on validation set: 0.6885\n",
      "Epoch 47: loss on final training batch: 0.6206\n",
      "Epoch 47: accuracy on validation set: 0.5260\n",
      "Epoch 47: accuracy on training set: 0.5788\n",
      "Epoch 47: loss on validation set: 0.6902\n",
      "Epoch 48: loss on final training batch: 0.6297\n",
      "Epoch 48: accuracy on validation set: 0.5208\n",
      "Epoch 48: accuracy on training set: 0.5807\n",
      "Epoch 48: loss on validation set: 0.6910\n",
      "Epoch 49: loss on final training batch: 0.6109\n",
      "Epoch 49: accuracy on validation set: 0.5273\n",
      "Epoch 49: accuracy on training set: 0.5801\n",
      "Epoch 49: loss on validation set: 0.6902\n",
      "Epoch 50: loss on final training batch: 0.6450\n",
      "Epoch 50: accuracy on validation set: 0.5160\n",
      "Epoch 50: accuracy on training set: 0.5843\n",
      "Epoch 50: loss on validation set: 0.6928\n",
      "Epoch 51: loss on final training batch: 0.6147\n",
      "Epoch 51: accuracy on validation set: 0.5246\n",
      "Epoch 51: accuracy on training set: 0.5844\n",
      "Epoch 51: loss on validation set: 0.6930\n",
      "Epoch 52: loss on final training batch: 0.6290\n",
      "Epoch 52: accuracy on validation set: 0.5196\n",
      "Epoch 52: accuracy on training set: 0.5851\n",
      "Epoch 52: loss on validation set: 0.6933\n",
      "Epoch 53: loss on final training batch: 0.6104\n",
      "Epoch 53: accuracy on validation set: 0.5258\n",
      "Epoch 53: accuracy on training set: 0.5878\n",
      "Epoch 53: loss on validation set: 0.6914\n",
      "Epoch 54: loss on final training batch: 0.6012\n",
      "Epoch 54: accuracy on validation set: 0.5225\n",
      "Epoch 54: accuracy on training set: 0.5856\n",
      "Epoch 54: loss on validation set: 0.6944\n",
      "Epoch 55: loss on final training batch: 0.6088\n",
      "Epoch 55: accuracy on validation set: 0.5242\n",
      "Epoch 55: accuracy on training set: 0.5882\n",
      "Epoch 55: loss on validation set: 0.6957\n",
      "Epoch 56: loss on final training batch: 0.6220\n",
      "Epoch 56: accuracy on validation set: 0.5219\n",
      "Epoch 56: accuracy on training set: 0.5944\n",
      "Epoch 56: loss on validation set: 0.6964\n",
      "Epoch 57: loss on final training batch: 0.6013\n",
      "Epoch 57: accuracy on validation set: 0.5246\n",
      "Epoch 57: accuracy on training set: 0.5945\n",
      "Epoch 57: loss on validation set: 0.6974\n",
      "Epoch 58: loss on final training batch: 0.6051\n",
      "Epoch 58: accuracy on validation set: 0.5271\n",
      "Epoch 58: accuracy on training set: 0.5968\n",
      "Epoch 58: loss on validation set: 0.6988\n",
      "Epoch 59: loss on final training batch: 0.6044\n",
      "Epoch 59: accuracy on validation set: 0.5225\n",
      "Epoch 59: accuracy on training set: 0.5955\n",
      "Epoch 59: loss on validation set: 0.6958\n",
      "Epoch 60: loss on final training batch: 0.5768\n",
      "Epoch 60: accuracy on validation set: 0.5235\n",
      "Epoch 60: accuracy on training set: 0.5986\n",
      "Epoch 60: loss on validation set: 0.6994\n",
      "Epoch 61: loss on final training batch: 0.5700\n",
      "Epoch 61: accuracy on validation set: 0.5179\n",
      "Epoch 61: accuracy on training set: 0.5976\n",
      "Epoch 61: loss on validation set: 0.7003\n",
      "Epoch 62: loss on final training batch: 0.5642\n",
      "Epoch 62: accuracy on validation set: 0.5244\n",
      "Epoch 62: accuracy on training set: 0.5983\n",
      "Epoch 62: loss on validation set: 0.7013\n",
      "Epoch 63: loss on final training batch: 0.5826\n",
      "Epoch 63: accuracy on validation set: 0.5221\n",
      "Epoch 63: accuracy on training set: 0.6042\n",
      "Epoch 63: loss on validation set: 0.7037\n",
      "Epoch 64: loss on final training batch: 0.5845\n",
      "Epoch 64: accuracy on validation set: 0.5175\n",
      "Epoch 64: accuracy on training set: 0.6038\n",
      "Epoch 64: loss on validation set: 0.7063\n",
      "Epoch 65: loss on final training batch: 0.5950\n",
      "Epoch 65: accuracy on validation set: 0.5202\n",
      "Epoch 65: accuracy on training set: 0.6032\n",
      "Epoch 65: loss on validation set: 0.7081\n",
      "Epoch 66: loss on final training batch: 0.5649\n",
      "Epoch 66: accuracy on validation set: 0.5142\n",
      "Epoch 66: accuracy on training set: 0.6066\n",
      "Epoch 66: loss on validation set: 0.7096\n",
      "Epoch 67: loss on final training batch: 0.5903\n",
      "Epoch 67: accuracy on validation set: 0.5169\n",
      "Epoch 67: accuracy on training set: 0.6094\n",
      "Epoch 67: loss on validation set: 0.7104\n",
      "Epoch 68: loss on final training batch: 0.5674\n",
      "Epoch 68: accuracy on validation set: 0.5179\n",
      "Epoch 68: accuracy on training set: 0.6091\n",
      "Epoch 68: loss on validation set: 0.7131\n",
      "Epoch 69: loss on final training batch: 0.5707\n",
      "Epoch 69: accuracy on validation set: 0.5094\n",
      "Epoch 69: accuracy on training set: 0.6093\n",
      "Epoch 69: loss on validation set: 0.7162\n",
      "Epoch 70: loss on final training batch: 0.5451\n",
      "Epoch 70: accuracy on validation set: 0.5181\n",
      "Epoch 70: accuracy on training set: 0.6114\n",
      "Epoch 70: loss on validation set: 0.7180\n",
      "Epoch 71: loss on final training batch: 0.5434\n",
      "Epoch 71: accuracy on validation set: 0.5173\n",
      "Epoch 71: accuracy on training set: 0.6107\n",
      "Epoch 71: loss on validation set: 0.7171\n",
      "Epoch 72: loss on final training batch: 0.5830\n",
      "Epoch 72: accuracy on validation set: 0.5163\n",
      "Epoch 72: accuracy on training set: 0.6112\n",
      "Epoch 72: loss on validation set: 0.7217\n",
      "Epoch 73: loss on final training batch: 0.5882\n",
      "Epoch 73: accuracy on validation set: 0.5142\n",
      "Epoch 73: accuracy on training set: 0.6123\n",
      "Epoch 73: loss on validation set: 0.7211\n",
      "Epoch 74: loss on final training batch: 0.5481\n",
      "Epoch 74: accuracy on validation set: 0.5073\n",
      "Epoch 74: accuracy on training set: 0.6163\n",
      "Epoch 74: loss on validation set: 0.7200\n",
      "Epoch 75: loss on final training batch: 0.5564\n",
      "Epoch 75: accuracy on validation set: 0.5075\n",
      "Epoch 75: accuracy on training set: 0.6143\n",
      "Epoch 75: loss on validation set: 0.7253\n",
      "Epoch 76: loss on final training batch: 0.5404\n",
      "Epoch 76: accuracy on validation set: 0.5052\n",
      "Epoch 76: accuracy on training set: 0.6139\n",
      "Epoch 76: loss on validation set: 0.7318\n",
      "Epoch 77: loss on final training batch: 0.5563\n",
      "Epoch 77: accuracy on validation set: 0.5096\n",
      "Epoch 77: accuracy on training set: 0.6189\n",
      "Epoch 77: loss on validation set: 0.7367\n",
      "Epoch 78: loss on final training batch: 0.5551\n",
      "Epoch 78: accuracy on validation set: 0.5173\n",
      "Epoch 78: accuracy on training set: 0.6159\n",
      "Epoch 78: loss on validation set: 0.7382\n",
      "Epoch 79: loss on final training batch: 0.5035\n",
      "Epoch 79: accuracy on validation set: 0.5156\n",
      "Epoch 79: accuracy on training set: 0.6206\n",
      "Epoch 79: loss on validation set: 0.7370\n",
      "Epoch 80: loss on final training batch: 0.5180\n",
      "Epoch 80: accuracy on validation set: 0.5029\n",
      "Epoch 80: accuracy on training set: 0.6199\n",
      "Epoch 80: loss on validation set: 0.7395\n",
      "Epoch 81: loss on final training batch: 0.5575\n",
      "Epoch 81: accuracy on validation set: 0.5052\n",
      "Epoch 81: accuracy on training set: 0.6211\n",
      "Epoch 81: loss on validation set: 0.7507\n",
      "Epoch 82: loss on final training batch: 0.5630\n",
      "Epoch 82: accuracy on validation set: 0.5135\n",
      "Epoch 82: accuracy on training set: 0.6237\n",
      "Epoch 82: loss on validation set: 0.7539\n",
      "Epoch 83: loss on final training batch: 0.5131\n",
      "Epoch 83: accuracy on validation set: 0.5069\n",
      "Epoch 83: accuracy on training set: 0.6233\n",
      "Epoch 83: loss on validation set: 0.7447\n",
      "Epoch 84: loss on final training batch: 0.4781\n",
      "Epoch 84: accuracy on validation set: 0.5077\n",
      "Epoch 84: accuracy on training set: 0.6253\n",
      "Epoch 84: loss on validation set: 0.7549\n",
      "Epoch 85: loss on final training batch: 0.5360\n",
      "Epoch 85: accuracy on validation set: 0.5158\n",
      "Epoch 85: accuracy on training set: 0.6242\n",
      "Epoch 85: loss on validation set: 0.7571\n",
      "Epoch 86: loss on final training batch: 0.5245\n",
      "Epoch 86: accuracy on validation set: 0.5040\n",
      "Epoch 86: accuracy on training set: 0.6268\n",
      "Epoch 86: loss on validation set: 0.7561\n",
      "Epoch 87: loss on final training batch: 0.5264\n",
      "Epoch 87: accuracy on validation set: 0.5129\n",
      "Epoch 87: accuracy on training set: 0.6276\n",
      "Epoch 87: loss on validation set: 0.7589\n",
      "Epoch 88: loss on final training batch: 0.5177\n",
      "Epoch 88: accuracy on validation set: 0.5033\n",
      "Epoch 88: accuracy on training set: 0.6257\n",
      "Epoch 88: loss on validation set: 0.7601\n",
      "Epoch 89: loss on final training batch: 0.4608\n",
      "Epoch 89: accuracy on validation set: 0.5075\n",
      "Epoch 89: accuracy on training set: 0.6279\n",
      "Epoch 89: loss on validation set: 0.7637\n",
      "Epoch 90: loss on final training batch: 0.4988\n",
      "Epoch 90: accuracy on validation set: 0.5065\n",
      "Epoch 90: accuracy on training set: 0.6236\n",
      "Epoch 90: loss on validation set: 0.7725\n",
      "Epoch 91: loss on final training batch: 0.5001\n",
      "Epoch 91: accuracy on validation set: 0.5113\n",
      "Epoch 91: accuracy on training set: 0.6279\n",
      "Epoch 91: loss on validation set: 0.7728\n",
      "Epoch 92: loss on final training batch: 0.5228\n",
      "Epoch 92: accuracy on validation set: 0.5050\n",
      "Epoch 92: accuracy on training set: 0.6294\n",
      "Epoch 92: loss on validation set: 0.7748\n",
      "Epoch 93: loss on final training batch: 0.4708\n",
      "Epoch 93: accuracy on validation set: 0.5102\n",
      "Epoch 93: accuracy on training set: 0.6307\n",
      "Epoch 93: loss on validation set: 0.7759\n",
      "Epoch 94: loss on final training batch: 0.4734\n",
      "Epoch 94: accuracy on validation set: 0.5121\n",
      "Epoch 94: accuracy on training set: 0.6314\n",
      "Epoch 94: loss on validation set: 0.7775\n",
      "Epoch 95: loss on final training batch: 0.4661\n",
      "Epoch 95: accuracy on validation set: 0.5119\n",
      "Epoch 95: accuracy on training set: 0.6304\n",
      "Epoch 95: loss on validation set: 0.7864\n",
      "Epoch 96: loss on final training batch: 0.4521\n",
      "Epoch 96: accuracy on validation set: 0.5008\n",
      "Epoch 96: accuracy on training set: 0.6278\n",
      "Epoch 96: loss on validation set: 0.7886\n",
      "Epoch 97: loss on final training batch: 0.4456\n",
      "Epoch 97: accuracy on validation set: 0.4981\n",
      "Epoch 97: accuracy on training set: 0.6289\n",
      "Epoch 97: loss on validation set: 0.7976\n",
      "Epoch 98: loss on final training batch: 0.4371\n",
      "Epoch 98: accuracy on validation set: 0.5025\n",
      "Epoch 98: accuracy on training set: 0.6347\n",
      "Epoch 98: loss on validation set: 0.7967\n",
      "Epoch 99: loss on final training batch: 0.4710\n",
      "Epoch 99: accuracy on validation set: 0.5092\n",
      "Epoch 99: accuracy on training set: 0.6319\n",
      "Epoch 99: loss on validation set: 0.7986\n",
      "Epoch 100: loss on final training batch: 0.4682\n",
      "Epoch 100: accuracy on validation set: 0.4969\n",
      "Epoch 100: accuracy on training set: 0.6342\n",
      "Epoch 100: loss on validation set: 0.8078\n",
      "Epoch 101: loss on final training batch: 0.4940\n",
      "Epoch 101: accuracy on validation set: 0.5148\n",
      "Epoch 101: accuracy on training set: 0.6339\n",
      "Epoch 101: loss on validation set: 0.8070\n",
      "Epoch 102: loss on final training batch: 0.4702\n",
      "Epoch 102: accuracy on validation set: 0.5098\n",
      "Epoch 102: accuracy on training set: 0.6312\n",
      "Epoch 102: loss on validation set: 0.8061\n",
      "Epoch 103: loss on final training batch: 0.4687\n",
      "Epoch 103: accuracy on validation set: 0.5092\n",
      "Epoch 103: accuracy on training set: 0.6332\n",
      "Epoch 103: loss on validation set: 0.8203\n",
      "Epoch 104: loss on final training batch: 0.4981\n",
      "Epoch 104: accuracy on validation set: 0.5004\n",
      "Epoch 104: accuracy on training set: 0.6350\n",
      "Epoch 104: loss on validation set: 0.8122\n",
      "Epoch 105: loss on final training batch: 0.4707\n",
      "Epoch 105: accuracy on validation set: 0.5000\n",
      "Epoch 105: accuracy on training set: 0.6377\n",
      "Epoch 105: loss on validation set: 0.8149\n",
      "Epoch 106: loss on final training batch: 0.4677\n",
      "Epoch 106: accuracy on validation set: 0.5054\n",
      "Epoch 106: accuracy on training set: 0.6408\n",
      "Epoch 106: loss on validation set: 0.8192\n",
      "Epoch 107: loss on final training batch: 0.4760\n",
      "Epoch 107: accuracy on validation set: 0.5033\n",
      "Epoch 107: accuracy on training set: 0.6385\n",
      "Epoch 107: loss on validation set: 0.8250\n",
      "Epoch 108: loss on final training batch: 0.4941\n",
      "Epoch 108: accuracy on validation set: 0.4981\n",
      "Epoch 108: accuracy on training set: 0.6394\n",
      "Epoch 108: loss on validation set: 0.8249\n",
      "Epoch 109: loss on final training batch: 0.4430\n",
      "Epoch 109: accuracy on validation set: 0.4985\n",
      "Epoch 109: accuracy on training set: 0.6388\n",
      "Epoch 109: loss on validation set: 0.8361\n",
      "Epoch 110: loss on final training batch: 0.4803\n",
      "Epoch 110: accuracy on validation set: 0.5092\n",
      "Epoch 110: accuracy on training set: 0.6394\n",
      "Epoch 110: loss on validation set: 0.8368\n",
      "Epoch 111: loss on final training batch: 0.4417\n",
      "Epoch 111: accuracy on validation set: 0.4967\n",
      "Epoch 111: accuracy on training set: 0.6446\n",
      "Epoch 111: loss on validation set: 0.8323\n",
      "Epoch 112: loss on final training batch: 0.4494\n",
      "Epoch 112: accuracy on validation set: 0.5096\n",
      "Epoch 112: accuracy on training set: 0.6413\n",
      "Epoch 112: loss on validation set: 0.8374\n",
      "Epoch 113: loss on final training batch: 0.4601\n",
      "Epoch 113: accuracy on validation set: 0.5113\n",
      "Epoch 113: accuracy on training set: 0.6401\n",
      "Epoch 113: loss on validation set: 0.8430\n",
      "Epoch 114: loss on final training batch: 0.3823\n",
      "Epoch 114: accuracy on validation set: 0.4904\n",
      "Epoch 114: accuracy on training set: 0.6392\n",
      "Epoch 114: loss on validation set: 0.8538\n",
      "Epoch 115: loss on final training batch: 0.3922\n",
      "Epoch 115: accuracy on validation set: 0.4994\n",
      "Epoch 115: accuracy on training set: 0.6426\n",
      "Epoch 115: loss on validation set: 0.8594\n",
      "Epoch 116: loss on final training batch: 0.4480\n",
      "Epoch 116: accuracy on validation set: 0.5044\n",
      "Epoch 116: accuracy on training set: 0.6411\n",
      "Epoch 116: loss on validation set: 0.8534\n",
      "Epoch 117: loss on final training batch: 0.4643\n",
      "Epoch 117: accuracy on validation set: 0.5140\n",
      "Epoch 117: accuracy on training set: 0.6453\n",
      "Epoch 117: loss on validation set: 0.8516\n",
      "Epoch 118: loss on final training batch: 0.4363\n",
      "Epoch 118: accuracy on validation set: 0.4996\n",
      "Epoch 118: accuracy on training set: 0.6445\n",
      "Epoch 118: loss on validation set: 0.8673\n",
      "Epoch 119: loss on final training batch: 0.4635\n",
      "Epoch 119: accuracy on validation set: 0.5096\n",
      "Epoch 119: accuracy on training set: 0.6442\n",
      "Epoch 119: loss on validation set: 0.8509\n",
      "Epoch 120: loss on final training batch: 0.4056\n",
      "Epoch 120: accuracy on validation set: 0.4983\n",
      "Epoch 120: accuracy on training set: 0.6437\n",
      "Epoch 120: loss on validation set: 0.8630\n"
     ]
    }
   ],
   "source": [
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,138),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.1),\n",
    "    torch.nn.Linear(138,69),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.2),\n",
    "    torch.nn.Linear(69,1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#loss function and optimizer\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.Adam(model2.parameters(),lr=3e-5, weight_decay=1e-5)\n",
    "\n",
    "#epoch number\n",
    "num_epoch = 120\n",
    "next_epoch = 1\n",
    "batch_size = 80\n",
    "\n",
    "#training loop\n",
    "for epoch in range(next_epoch, next_epoch+num_epoch):\n",
    "    \n",
    "    \n",
    "    # Make an entire pass (an 'epoch') over the training data in batch_size chunks\n",
    "    for i in range(0, len(X_train), batch_size):        \n",
    "        X = X_train[i:i+batch_size]     # Slice out a mini-batch of features\n",
    "        y = y_train[i:i+batch_size]     # Slice out a mini-batch of targets\n",
    "        \n",
    "        y_pred = model2(X)                   # Make predictions (final-layer activations)\n",
    "        \n",
    "        l = loss(y_pred, y)                 # Compute loss with respect to predictions\n",
    "        \n",
    "        model2.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\n",
    "        l.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\n",
    "        optim.step()                    # Use the gradients to take a step with SGD.\n",
    "        \n",
    "    print(\"Epoch %2d: loss on final training batch: %.4f\" % (epoch, l.item()))\n",
    "    # pred = torch.sign(model(X_val))\n",
    "    # acc = torch.mean((pred == y_val).float())\n",
    "    \n",
    "    #create dire query\n",
    "\n",
    "    #validation set calculations\n",
    "    dire_X =  torch.index_select(X_val, 1, torch.LongTensor([*range(138,276)]))\n",
    "    dire_X = torch.cat((dire_X,torch.index_select(X_val, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "\n",
    "    dire_pred = (model2(dire_X) >= 0).float()\n",
    "    \n",
    "    rad_pred = (model2(X_val) >= 0).float()\n",
    "    \n",
    "\n",
    "    overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "    \n",
    "    val_acc = torch.mean((overall_prob == y_val).float())\n",
    "\n",
    "    #training set calculations\n",
    "    dire_X_train =  torch.index_select(X_train, 1, torch.LongTensor([*range(138,276)]))\n",
    "    dire_X_train = torch.cat((dire_X_train,torch.index_select(X_train, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "\n",
    "    dire_pred = (model2(dire_X_train) >= 0).float()\n",
    "    \n",
    "    rad_pred = (model2(X_train) >= 0).float()\n",
    "    \n",
    "\n",
    "    overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "    \n",
    "    train_acc = torch.mean((overall_prob == y_train).float())\n",
    "    \n",
    "    print(\"Epoch %2d: accuracy on validation set: %.4f\" % (epoch, val_acc))\n",
    "    print(\"Epoch %2d: accuracy on training set: %.4f\" % (epoch, train_acc))\n",
    "    \n",
    "    print(\"Epoch %2d: loss on validation set: %.4f\" % (epoch, loss(model2(X_val), y_val)))\n",
    "    \n",
    "   \n",
    "\n",
    "next_epoch = epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  49.471875 %\n",
      "held-out accuracy (testing):  48.5625 %\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (226846857.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_val_trn,y_data, test_size = 0.2, shuffle= True)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = 4,activation='relu',solver='adam',batch_size=16,learning_rate_init=0.05,max_iter=3000,random_state=0)# Your code here\n",
    "log = LogisticRegression()\n",
    "mlp.fit(X_train,y_train.ravel())\n",
    "log.fit(X_train,y_train.ravel())\n",
    "\n",
    "print('training accuracy: ',mlp.score(X_train,y_train)*100,'%')\n",
    "print('held-out accuracy (testing): ',mlp.score(X_test,y_test)*100,'%')\n",
    "\n",
    "dire_X =  torch.index_select(X_test, 1, torch.LongTensor([*range(138,276)]))\n",
    "dire_X = torch.cat((dire_X, torch.index_select(X_test, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "\n",
    "\n",
    "\n",
    "dire_prob = log.score(dire_X,y_test)\n",
    "rad_prob = log.score(X_test,y_test)\n",
    "\n",
    "overall_prob = ((rad_prob + (1 - dire_prob))/2)\n",
    "\n",
    "print('log training accuracy: ',log.score(X_train,y_train)*100,'%')\n",
    "print('log held-out accuracy (testing): ',log.score(X_test,y_test)*100,'%')\n",
    "\n",
    "print('log overall accuracy: ',overall_prob*100,'%')\n",
    "\n",
    "for k in range(2,20):\n",
    "    scores = sklearn.model_selection.cross_val_score(log,X_test,y_test.ravel(),cv=k) \n",
    "    print(\"held-out accuracy (%d-fold):   %.1f%%\" % (k, scores.mean()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.516)\n"
     ]
    }
   ],
   "source": [
    "#test accuracy on test set \n",
    "dire_X =  torch.index_select(X_test, 1, torch.LongTensor([*range(138,276)]))\n",
    "dire_X = torch.cat((dire_X,torch.index_select(X_test, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "dire_pred = (model2(dire_X) >= 0).float()\n",
    "rad_pred = (model2(X_test) >= 0).float()\n",
    "\n",
    "\n",
    "overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "acc = torch.mean((overall_prob == y_test).float())\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
