{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import warnings\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "np.set_printoptions(precision=3, suppress=True)  # Print as 0.001 instead of 9.876e-4\n",
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding match data from file\n",
    "\n",
    "data = np.loadtxt('newmatchdata.csv',skiprows=1,delimiter=',')\n",
    "X_data = np.array(data[:,1:]).astype(np.int32)\n",
    "y_data = np.array(data[:,:1]).astype(np.int32)\n",
    "\n",
    "#turn all data into feature vector\n",
    "#feature vector creation\n",
    "X_val_trn = torch.zeros((40000,138*2),dtype=torch.float32)\n",
    "j = 0\n",
    "for d in X_data:\n",
    "    for i in range(len(d)):\n",
    "        if(i < len(d)/2):\n",
    "            h = d[i]\n",
    "            X_val_trn[j][h] += 1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            h = d[i]\n",
    "            X_val_trn[j][h+137] += 1\n",
    "      \n",
    "    j += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_16504\\337929334.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train,dtype=torch.float32)\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_16504\\337929334.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val,dtype=torch.float32)\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_16504\\337929334.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sklearn train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_val_trn,y_data, test_size = 0.2, shuffle= True)\n",
    "\n",
    "#split train into train/validation set\n",
    "\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_train,y_train, test_size = 0.15, shuffle= True)\n",
    "\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(X_val,dtype=torch.float32)\n",
    "\n",
    "y_val = torch.tensor(y_val,dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "#sklearn train test split\n",
    "#use test set to calculate error, CCE, accuracy etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model creation\n",
    "#addition of multiple hidden layers and drop rate to help with regularization\n",
    "print(torch.cuda.is_available())\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,100),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100,50),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50,1)\n",
    ")\n",
    "\n",
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: loss on final training batch: 0.6955\n",
      "Epoch  1: accuracy on validation set: 0.4977\n",
      "Epoch  1: accuracy on training set: 0.4920\n",
      "Epoch  1: loss on validation set: 0.6953\n",
      "Epoch  2: loss on final training batch: 0.6917\n",
      "Epoch  2: accuracy on validation set: 0.4977\n",
      "Epoch  2: accuracy on training set: 0.4920\n",
      "Epoch  2: loss on validation set: 0.6949\n",
      "Epoch  3: loss on final training batch: 0.6905\n",
      "Epoch  3: accuracy on validation set: 0.4977\n",
      "Epoch  3: accuracy on training set: 0.4920\n",
      "Epoch  3: loss on validation set: 0.6946\n",
      "Epoch  4: loss on final training batch: 0.6907\n",
      "Epoch  4: accuracy on validation set: 0.4977\n",
      "Epoch  4: accuracy on training set: 0.4920\n",
      "Epoch  4: loss on validation set: 0.6944\n",
      "Epoch  5: loss on final training batch: 0.6913\n",
      "Epoch  5: accuracy on validation set: 0.4977\n",
      "Epoch  5: accuracy on training set: 0.4920\n",
      "Epoch  5: loss on validation set: 0.6942\n",
      "Epoch  6: loss on final training batch: 0.6896\n",
      "Epoch  6: accuracy on validation set: 0.4977\n",
      "Epoch  6: accuracy on training set: 0.4920\n",
      "Epoch  6: loss on validation set: 0.6941\n",
      "Epoch  7: loss on final training batch: 0.6912\n",
      "Epoch  7: accuracy on validation set: 0.4977\n",
      "Epoch  7: accuracy on training set: 0.4920\n",
      "Epoch  7: loss on validation set: 0.6940\n",
      "Epoch  8: loss on final training batch: 0.6931\n",
      "Epoch  8: accuracy on validation set: 0.4977\n",
      "Epoch  8: accuracy on training set: 0.4920\n",
      "Epoch  8: loss on validation set: 0.6939\n",
      "Epoch  9: loss on final training batch: 0.6921\n",
      "Epoch  9: accuracy on validation set: 0.4977\n",
      "Epoch  9: accuracy on training set: 0.4920\n",
      "Epoch  9: loss on validation set: 0.6938\n",
      "Epoch 10: loss on final training batch: 0.6915\n",
      "Epoch 10: accuracy on validation set: 0.4977\n",
      "Epoch 10: accuracy on training set: 0.4920\n",
      "Epoch 10: loss on validation set: 0.6937\n",
      "Epoch 11: loss on final training batch: 0.6909\n",
      "Epoch 11: accuracy on validation set: 0.4977\n",
      "Epoch 11: accuracy on training set: 0.4920\n",
      "Epoch 11: loss on validation set: 0.6937\n",
      "Epoch 12: loss on final training batch: 0.6910\n",
      "Epoch 12: accuracy on validation set: 0.4977\n",
      "Epoch 12: accuracy on training set: 0.4920\n",
      "Epoch 12: loss on validation set: 0.6936\n",
      "Epoch 13: loss on final training batch: 0.6904\n",
      "Epoch 13: accuracy on validation set: 0.4977\n",
      "Epoch 13: accuracy on training set: 0.4920\n",
      "Epoch 13: loss on validation set: 0.6936\n",
      "Epoch 14: loss on final training batch: 0.6954\n",
      "Epoch 14: accuracy on validation set: 0.4977\n",
      "Epoch 14: accuracy on training set: 0.4920\n",
      "Epoch 14: loss on validation set: 0.6935\n",
      "Epoch 15: loss on final training batch: 0.6940\n",
      "Epoch 15: accuracy on validation set: 0.4977\n",
      "Epoch 15: accuracy on training set: 0.4920\n",
      "Epoch 15: loss on validation set: 0.6935\n",
      "Epoch 16: loss on final training batch: 0.6952\n",
      "Epoch 16: accuracy on validation set: 0.4977\n",
      "Epoch 16: accuracy on training set: 0.4920\n",
      "Epoch 16: loss on validation set: 0.6935\n",
      "Epoch 17: loss on final training batch: 0.6936\n",
      "Epoch 17: accuracy on validation set: 0.4977\n",
      "Epoch 17: accuracy on training set: 0.4920\n",
      "Epoch 17: loss on validation set: 0.6935\n",
      "Epoch 18: loss on final training batch: 0.6950\n",
      "Epoch 18: accuracy on validation set: 0.4977\n",
      "Epoch 18: accuracy on training set: 0.4920\n",
      "Epoch 18: loss on validation set: 0.6934\n",
      "Epoch 19: loss on final training batch: 0.6962\n",
      "Epoch 19: accuracy on validation set: 0.4977\n",
      "Epoch 19: accuracy on training set: 0.4920\n",
      "Epoch 19: loss on validation set: 0.6934\n",
      "Epoch 20: loss on final training batch: 0.6957\n",
      "Epoch 20: accuracy on validation set: 0.4977\n",
      "Epoch 20: accuracy on training set: 0.4920\n",
      "Epoch 20: loss on validation set: 0.6934\n",
      "Epoch 21: loss on final training batch: 0.6906\n",
      "Epoch 21: accuracy on validation set: 0.4977\n",
      "Epoch 21: accuracy on training set: 0.4920\n",
      "Epoch 21: loss on validation set: 0.6934\n",
      "Epoch 22: loss on final training batch: 0.6920\n",
      "Epoch 22: accuracy on validation set: 0.4977\n",
      "Epoch 22: accuracy on training set: 0.4920\n",
      "Epoch 22: loss on validation set: 0.6934\n",
      "Epoch 23: loss on final training batch: 0.6937\n",
      "Epoch 23: accuracy on validation set: 0.4977\n",
      "Epoch 23: accuracy on training set: 0.4920\n",
      "Epoch 23: loss on validation set: 0.6933\n",
      "Epoch 24: loss on final training batch: 0.6925\n",
      "Epoch 24: accuracy on validation set: 0.4977\n",
      "Epoch 24: accuracy on training set: 0.4920\n",
      "Epoch 24: loss on validation set: 0.6933\n",
      "Epoch 25: loss on final training batch: 0.6919\n",
      "Epoch 25: accuracy on validation set: 0.4977\n",
      "Epoch 25: accuracy on training set: 0.4920\n",
      "Epoch 25: loss on validation set: 0.6933\n",
      "Epoch 26: loss on final training batch: 0.6921\n",
      "Epoch 26: accuracy on validation set: 0.4977\n",
      "Epoch 26: accuracy on training set: 0.4920\n",
      "Epoch 26: loss on validation set: 0.6933\n",
      "Epoch 27: loss on final training batch: 0.6925\n",
      "Epoch 27: accuracy on validation set: 0.4977\n",
      "Epoch 27: accuracy on training set: 0.4920\n",
      "Epoch 27: loss on validation set: 0.6933\n",
      "Epoch 28: loss on final training batch: 0.6922\n",
      "Epoch 28: accuracy on validation set: 0.4977\n",
      "Epoch 28: accuracy on training set: 0.4920\n",
      "Epoch 28: loss on validation set: 0.6933\n",
      "Epoch 29: loss on final training batch: 0.6934\n",
      "Epoch 29: accuracy on validation set: 0.4977\n",
      "Epoch 29: accuracy on training set: 0.4920\n",
      "Epoch 29: loss on validation set: 0.6933\n",
      "Epoch 30: loss on final training batch: 0.6940\n",
      "Epoch 30: accuracy on validation set: 0.4977\n",
      "Epoch 30: accuracy on training set: 0.4920\n",
      "Epoch 30: loss on validation set: 0.6933\n",
      "Epoch 31: loss on final training batch: 0.6931\n",
      "Epoch 31: accuracy on validation set: 0.4977\n",
      "Epoch 31: accuracy on training set: 0.4920\n",
      "Epoch 31: loss on validation set: 0.6933\n",
      "Epoch 32: loss on final training batch: 0.6953\n",
      "Epoch 32: accuracy on validation set: 0.4977\n",
      "Epoch 32: accuracy on training set: 0.4920\n",
      "Epoch 32: loss on validation set: 0.6933\n",
      "Epoch 33: loss on final training batch: 0.6899\n",
      "Epoch 33: accuracy on validation set: 0.4977\n",
      "Epoch 33: accuracy on training set: 0.4920\n",
      "Epoch 33: loss on validation set: 0.6933\n",
      "Epoch 34: loss on final training batch: 0.6916\n",
      "Epoch 34: accuracy on validation set: 0.4977\n",
      "Epoch 34: accuracy on training set: 0.4920\n",
      "Epoch 34: loss on validation set: 0.6933\n",
      "Epoch 35: loss on final training batch: 0.6915\n",
      "Epoch 35: accuracy on validation set: 0.4977\n",
      "Epoch 35: accuracy on training set: 0.4920\n",
      "Epoch 35: loss on validation set: 0.6933\n",
      "Epoch 36: loss on final training batch: 0.6918\n",
      "Epoch 36: accuracy on validation set: 0.4977\n",
      "Epoch 36: accuracy on training set: 0.4920\n",
      "Epoch 36: loss on validation set: 0.6933\n",
      "Epoch 37: loss on final training batch: 0.6900\n",
      "Epoch 37: accuracy on validation set: 0.4977\n",
      "Epoch 37: accuracy on training set: 0.4920\n",
      "Epoch 37: loss on validation set: 0.6933\n",
      "Epoch 38: loss on final training batch: 0.6929\n",
      "Epoch 38: accuracy on validation set: 0.4977\n",
      "Epoch 38: accuracy on training set: 0.4920\n",
      "Epoch 38: loss on validation set: 0.6932\n",
      "Epoch 39: loss on final training batch: 0.6893\n",
      "Epoch 39: accuracy on validation set: 0.4977\n",
      "Epoch 39: accuracy on training set: 0.4920\n",
      "Epoch 39: loss on validation set: 0.6932\n",
      "Epoch 40: loss on final training batch: 0.6917\n",
      "Epoch 40: accuracy on validation set: 0.4977\n",
      "Epoch 40: accuracy on training set: 0.4920\n",
      "Epoch 40: loss on validation set: 0.6932\n",
      "Epoch 41: loss on final training batch: 0.6930\n",
      "Epoch 41: accuracy on validation set: 0.4977\n",
      "Epoch 41: accuracy on training set: 0.4920\n",
      "Epoch 41: loss on validation set: 0.6932\n",
      "Epoch 42: loss on final training batch: 0.6945\n",
      "Epoch 42: accuracy on validation set: 0.4977\n",
      "Epoch 42: accuracy on training set: 0.4920\n",
      "Epoch 42: loss on validation set: 0.6932\n",
      "Epoch 43: loss on final training batch: 0.6945\n",
      "Epoch 43: accuracy on validation set: 0.4977\n",
      "Epoch 43: accuracy on training set: 0.4920\n",
      "Epoch 43: loss on validation set: 0.6932\n",
      "Epoch 44: loss on final training batch: 0.6918\n",
      "Epoch 44: accuracy on validation set: 0.4977\n",
      "Epoch 44: accuracy on training set: 0.4920\n",
      "Epoch 44: loss on validation set: 0.6932\n",
      "Epoch 45: loss on final training batch: 0.6925\n",
      "Epoch 45: accuracy on validation set: 0.4977\n",
      "Epoch 45: accuracy on training set: 0.4920\n",
      "Epoch 45: loss on validation set: 0.6932\n",
      "Epoch 46: loss on final training batch: 0.6940\n",
      "Epoch 46: accuracy on validation set: 0.4977\n",
      "Epoch 46: accuracy on training set: 0.4920\n",
      "Epoch 46: loss on validation set: 0.6932\n",
      "Epoch 47: loss on final training batch: 0.6913\n",
      "Epoch 47: accuracy on validation set: 0.4977\n",
      "Epoch 47: accuracy on training set: 0.4920\n",
      "Epoch 47: loss on validation set: 0.6932\n",
      "Epoch 48: loss on final training batch: 0.6938\n",
      "Epoch 48: accuracy on validation set: 0.4977\n",
      "Epoch 48: accuracy on training set: 0.4920\n",
      "Epoch 48: loss on validation set: 0.6932\n",
      "Epoch 49: loss on final training batch: 0.6904\n",
      "Epoch 49: accuracy on validation set: 0.4977\n",
      "Epoch 49: accuracy on training set: 0.4920\n",
      "Epoch 49: loss on validation set: 0.6932\n",
      "Epoch 50: loss on final training batch: 0.6932\n",
      "Epoch 50: accuracy on validation set: 0.4977\n",
      "Epoch 50: accuracy on training set: 0.4920\n",
      "Epoch 50: loss on validation set: 0.6932\n",
      "Epoch 51: loss on final training batch: 0.6922\n",
      "Epoch 51: accuracy on validation set: 0.4977\n",
      "Epoch 51: accuracy on training set: 0.4920\n",
      "Epoch 51: loss on validation set: 0.6932\n",
      "Epoch 52: loss on final training batch: 0.6937\n",
      "Epoch 52: accuracy on validation set: 0.4977\n",
      "Epoch 52: accuracy on training set: 0.4920\n",
      "Epoch 52: loss on validation set: 0.6932\n",
      "Epoch 53: loss on final training batch: 0.6936\n",
      "Epoch 53: accuracy on validation set: 0.4977\n",
      "Epoch 53: accuracy on training set: 0.4920\n",
      "Epoch 53: loss on validation set: 0.6932\n",
      "Epoch 54: loss on final training batch: 0.6931\n",
      "Epoch 54: accuracy on validation set: 0.4977\n",
      "Epoch 54: accuracy on training set: 0.4920\n",
      "Epoch 54: loss on validation set: 0.6932\n",
      "Epoch 55: loss on final training batch: 0.6902\n",
      "Epoch 55: accuracy on validation set: 0.4977\n",
      "Epoch 55: accuracy on training set: 0.4920\n",
      "Epoch 55: loss on validation set: 0.6932\n",
      "Epoch 56: loss on final training batch: 0.6901\n",
      "Epoch 56: accuracy on validation set: 0.4977\n",
      "Epoch 56: accuracy on training set: 0.4920\n",
      "Epoch 56: loss on validation set: 0.6931\n",
      "Epoch 57: loss on final training batch: 0.6929\n",
      "Epoch 57: accuracy on validation set: 0.4977\n",
      "Epoch 57: accuracy on training set: 0.4920\n",
      "Epoch 57: loss on validation set: 0.6931\n",
      "Epoch 58: loss on final training batch: 0.6925\n",
      "Epoch 58: accuracy on validation set: 0.4977\n",
      "Epoch 58: accuracy on training set: 0.4920\n",
      "Epoch 58: loss on validation set: 0.6931\n",
      "Epoch 59: loss on final training batch: 0.6930\n",
      "Epoch 59: accuracy on validation set: 0.4977\n",
      "Epoch 59: accuracy on training set: 0.4920\n",
      "Epoch 59: loss on validation set: 0.6931\n",
      "Epoch 60: loss on final training batch: 0.6935\n",
      "Epoch 60: accuracy on validation set: 0.4977\n",
      "Epoch 60: accuracy on training set: 0.4920\n",
      "Epoch 60: loss on validation set: 0.6931\n",
      "Epoch 61: loss on final training batch: 0.6942\n",
      "Epoch 61: accuracy on validation set: 0.4977\n",
      "Epoch 61: accuracy on training set: 0.4920\n",
      "Epoch 61: loss on validation set: 0.6931\n",
      "Epoch 62: loss on final training batch: 0.6909\n",
      "Epoch 62: accuracy on validation set: 0.4977\n",
      "Epoch 62: accuracy on training set: 0.4920\n",
      "Epoch 62: loss on validation set: 0.6931\n",
      "Epoch 63: loss on final training batch: 0.6904\n",
      "Epoch 63: accuracy on validation set: 0.4977\n",
      "Epoch 63: accuracy on training set: 0.4920\n",
      "Epoch 63: loss on validation set: 0.6931\n",
      "Epoch 64: loss on final training batch: 0.6920\n",
      "Epoch 64: accuracy on validation set: 0.4977\n",
      "Epoch 64: accuracy on training set: 0.4920\n",
      "Epoch 64: loss on validation set: 0.6930\n",
      "Epoch 65: loss on final training batch: 0.6907\n",
      "Epoch 65: accuracy on validation set: 0.4977\n",
      "Epoch 65: accuracy on training set: 0.4920\n",
      "Epoch 65: loss on validation set: 0.6930\n",
      "Epoch 66: loss on final training batch: 0.6908\n",
      "Epoch 66: accuracy on validation set: 0.4977\n",
      "Epoch 66: accuracy on training set: 0.4920\n",
      "Epoch 66: loss on validation set: 0.6929\n",
      "Epoch 67: loss on final training batch: 0.6939\n",
      "Epoch 67: accuracy on validation set: 0.4977\n",
      "Epoch 67: accuracy on training set: 0.4920\n",
      "Epoch 67: loss on validation set: 0.6929\n",
      "Epoch 68: loss on final training batch: 0.6916\n",
      "Epoch 68: accuracy on validation set: 0.4977\n",
      "Epoch 68: accuracy on training set: 0.4920\n",
      "Epoch 68: loss on validation set: 0.6929\n",
      "Epoch 69: loss on final training batch: 0.6932\n",
      "Epoch 69: accuracy on validation set: 0.4977\n",
      "Epoch 69: accuracy on training set: 0.4920\n",
      "Epoch 69: loss on validation set: 0.6928\n",
      "Epoch 70: loss on final training batch: 0.6892\n",
      "Epoch 70: accuracy on validation set: 0.4977\n",
      "Epoch 70: accuracy on training set: 0.4920\n",
      "Epoch 70: loss on validation set: 0.6927\n",
      "Epoch 71: loss on final training batch: 0.6901\n",
      "Epoch 71: accuracy on validation set: 0.4977\n",
      "Epoch 71: accuracy on training set: 0.4920\n",
      "Epoch 71: loss on validation set: 0.6926\n",
      "Epoch 72: loss on final training batch: 0.6924\n",
      "Epoch 72: accuracy on validation set: 0.4977\n",
      "Epoch 72: accuracy on training set: 0.4920\n",
      "Epoch 72: loss on validation set: 0.6925\n",
      "Epoch 73: loss on final training batch: 0.6923\n",
      "Epoch 73: accuracy on validation set: 0.4977\n",
      "Epoch 73: accuracy on training set: 0.4920\n",
      "Epoch 73: loss on validation set: 0.6924\n",
      "Epoch 74: loss on final training batch: 0.6914\n",
      "Epoch 74: accuracy on validation set: 0.4977\n",
      "Epoch 74: accuracy on training set: 0.4920\n",
      "Epoch 74: loss on validation set: 0.6922\n",
      "Epoch 75: loss on final training batch: 0.6923\n",
      "Epoch 75: accuracy on validation set: 0.4977\n",
      "Epoch 75: accuracy on training set: 0.4920\n",
      "Epoch 75: loss on validation set: 0.6919\n",
      "Epoch 76: loss on final training batch: 0.6907\n",
      "Epoch 76: accuracy on validation set: 0.4981\n",
      "Epoch 76: accuracy on training set: 0.4916\n",
      "Epoch 76: loss on validation set: 0.6916\n",
      "Epoch 77: loss on final training batch: 0.6884\n",
      "Epoch 77: accuracy on validation set: 0.5040\n",
      "Epoch 77: accuracy on training set: 0.4955\n",
      "Epoch 77: loss on validation set: 0.6912\n",
      "Epoch 78: loss on final training batch: 0.6877\n",
      "Epoch 78: accuracy on validation set: 0.5027\n",
      "Epoch 78: accuracy on training set: 0.5059\n",
      "Epoch 78: loss on validation set: 0.6906\n",
      "Epoch 79: loss on final training batch: 0.6876\n",
      "Epoch 79: accuracy on validation set: 0.5210\n",
      "Epoch 79: accuracy on training set: 0.5149\n",
      "Epoch 79: loss on validation set: 0.6899\n",
      "Epoch 80: loss on final training batch: 0.6841\n",
      "Epoch 80: accuracy on validation set: 0.5233\n",
      "Epoch 80: accuracy on training set: 0.5247\n",
      "Epoch 80: loss on validation set: 0.6890\n",
      "Epoch 81: loss on final training batch: 0.6849\n",
      "Epoch 81: accuracy on validation set: 0.5271\n",
      "Epoch 81: accuracy on training set: 0.5310\n",
      "Epoch 81: loss on validation set: 0.6880\n",
      "Epoch 82: loss on final training batch: 0.6854\n",
      "Epoch 82: accuracy on validation set: 0.5292\n",
      "Epoch 82: accuracy on training set: 0.5364\n",
      "Epoch 82: loss on validation set: 0.6870\n",
      "Epoch 83: loss on final training batch: 0.6832\n",
      "Epoch 83: accuracy on validation set: 0.5288\n",
      "Epoch 83: accuracy on training set: 0.5348\n",
      "Epoch 83: loss on validation set: 0.6862\n",
      "Epoch 84: loss on final training batch: 0.6873\n",
      "Epoch 84: accuracy on validation set: 0.5312\n",
      "Epoch 84: accuracy on training set: 0.5368\n",
      "Epoch 84: loss on validation set: 0.6855\n",
      "Epoch 85: loss on final training batch: 0.6718\n",
      "Epoch 85: accuracy on validation set: 0.5331\n",
      "Epoch 85: accuracy on training set: 0.5383\n",
      "Epoch 85: loss on validation set: 0.6848\n",
      "Epoch 86: loss on final training batch: 0.6822\n",
      "Epoch 86: accuracy on validation set: 0.5360\n",
      "Epoch 86: accuracy on training set: 0.5410\n",
      "Epoch 86: loss on validation set: 0.6842\n",
      "Epoch 87: loss on final training batch: 0.6873\n",
      "Epoch 87: accuracy on validation set: 0.5315\n",
      "Epoch 87: accuracy on training set: 0.5369\n",
      "Epoch 87: loss on validation set: 0.6841\n",
      "Epoch 88: loss on final training batch: 0.6839\n",
      "Epoch 88: accuracy on validation set: 0.5342\n",
      "Epoch 88: accuracy on training set: 0.5378\n",
      "Epoch 88: loss on validation set: 0.6839\n",
      "Epoch 89: loss on final training batch: 0.6794\n",
      "Epoch 89: accuracy on validation set: 0.5335\n",
      "Epoch 89: accuracy on training set: 0.5407\n",
      "Epoch 89: loss on validation set: 0.6838\n",
      "Epoch 90: loss on final training batch: 0.6897\n",
      "Epoch 90: accuracy on validation set: 0.5327\n",
      "Epoch 90: accuracy on training set: 0.5404\n",
      "Epoch 90: loss on validation set: 0.6838\n",
      "Epoch 91: loss on final training batch: 0.6797\n",
      "Epoch 91: accuracy on validation set: 0.5308\n",
      "Epoch 91: accuracy on training set: 0.5401\n",
      "Epoch 91: loss on validation set: 0.6838\n",
      "Epoch 92: loss on final training batch: 0.6759\n",
      "Epoch 92: accuracy on validation set: 0.5333\n",
      "Epoch 92: accuracy on training set: 0.5415\n",
      "Epoch 92: loss on validation set: 0.6837\n",
      "Epoch 93: loss on final training batch: 0.6718\n",
      "Epoch 93: accuracy on validation set: 0.5304\n",
      "Epoch 93: accuracy on training set: 0.5410\n",
      "Epoch 93: loss on validation set: 0.6838\n",
      "Epoch 94: loss on final training batch: 0.6705\n",
      "Epoch 94: accuracy on validation set: 0.5317\n",
      "Epoch 94: accuracy on training set: 0.5397\n",
      "Epoch 94: loss on validation set: 0.6838\n",
      "Epoch 95: loss on final training batch: 0.6736\n",
      "Epoch 95: accuracy on validation set: 0.5292\n",
      "Epoch 95: accuracy on training set: 0.5395\n",
      "Epoch 95: loss on validation set: 0.6839\n",
      "Epoch 96: loss on final training batch: 0.6820\n",
      "Epoch 96: accuracy on validation set: 0.5298\n",
      "Epoch 96: accuracy on training set: 0.5403\n",
      "Epoch 96: loss on validation set: 0.6838\n",
      "Epoch 97: loss on final training batch: 0.6731\n",
      "Epoch 97: accuracy on validation set: 0.5296\n",
      "Epoch 97: accuracy on training set: 0.5396\n",
      "Epoch 97: loss on validation set: 0.6839\n",
      "Epoch 98: loss on final training batch: 0.6772\n",
      "Epoch 98: accuracy on validation set: 0.5317\n",
      "Epoch 98: accuracy on training set: 0.5414\n",
      "Epoch 98: loss on validation set: 0.6839\n",
      "Epoch 99: loss on final training batch: 0.6682\n",
      "Epoch 99: accuracy on validation set: 0.5315\n",
      "Epoch 99: accuracy on training set: 0.5420\n",
      "Epoch 99: loss on validation set: 0.6838\n",
      "Epoch 100: loss on final training batch: 0.6719\n",
      "Epoch 100: accuracy on validation set: 0.5285\n",
      "Epoch 100: accuracy on training set: 0.5413\n",
      "Epoch 100: loss on validation set: 0.6840\n",
      "Epoch 101: loss on final training batch: 0.6638\n",
      "Epoch 101: accuracy on validation set: 0.5271\n",
      "Epoch 101: accuracy on training set: 0.5415\n",
      "Epoch 101: loss on validation set: 0.6839\n",
      "Epoch 102: loss on final training batch: 0.6718\n",
      "Epoch 102: accuracy on validation set: 0.5300\n",
      "Epoch 102: accuracy on training set: 0.5433\n",
      "Epoch 102: loss on validation set: 0.6837\n",
      "Epoch 103: loss on final training batch: 0.6797\n",
      "Epoch 103: accuracy on validation set: 0.5296\n",
      "Epoch 103: accuracy on training set: 0.5423\n",
      "Epoch 103: loss on validation set: 0.6837\n",
      "Epoch 104: loss on final training batch: 0.6794\n",
      "Epoch 104: accuracy on validation set: 0.5304\n",
      "Epoch 104: accuracy on training set: 0.5394\n",
      "Epoch 104: loss on validation set: 0.6840\n",
      "Epoch 105: loss on final training batch: 0.6732\n",
      "Epoch 105: accuracy on validation set: 0.5269\n",
      "Epoch 105: accuracy on training set: 0.5424\n",
      "Epoch 105: loss on validation set: 0.6839\n",
      "Epoch 106: loss on final training batch: 0.6624\n",
      "Epoch 106: accuracy on validation set: 0.5265\n",
      "Epoch 106: accuracy on training set: 0.5411\n",
      "Epoch 106: loss on validation set: 0.6840\n",
      "Epoch 107: loss on final training batch: 0.6742\n",
      "Epoch 107: accuracy on validation set: 0.5275\n",
      "Epoch 107: accuracy on training set: 0.5394\n",
      "Epoch 107: loss on validation set: 0.6842\n",
      "Epoch 108: loss on final training batch: 0.6703\n",
      "Epoch 108: accuracy on validation set: 0.5267\n",
      "Epoch 108: accuracy on training set: 0.5414\n",
      "Epoch 108: loss on validation set: 0.6840\n",
      "Epoch 109: loss on final training batch: 0.6700\n",
      "Epoch 109: accuracy on validation set: 0.5292\n",
      "Epoch 109: accuracy on training set: 0.5407\n",
      "Epoch 109: loss on validation set: 0.6841\n",
      "Epoch 110: loss on final training batch: 0.6742\n",
      "Epoch 110: accuracy on validation set: 0.5288\n",
      "Epoch 110: accuracy on training set: 0.5424\n",
      "Epoch 110: loss on validation set: 0.6839\n",
      "Epoch 111: loss on final training batch: 0.6689\n",
      "Epoch 111: accuracy on validation set: 0.5283\n",
      "Epoch 111: accuracy on training set: 0.5410\n",
      "Epoch 111: loss on validation set: 0.6840\n",
      "Epoch 112: loss on final training batch: 0.6603\n",
      "Epoch 112: accuracy on validation set: 0.5283\n",
      "Epoch 112: accuracy on training set: 0.5414\n",
      "Epoch 112: loss on validation set: 0.6840\n",
      "Epoch 113: loss on final training batch: 0.6730\n",
      "Epoch 113: accuracy on validation set: 0.5288\n",
      "Epoch 113: accuracy on training set: 0.5404\n",
      "Epoch 113: loss on validation set: 0.6842\n",
      "Epoch 114: loss on final training batch: 0.6755\n",
      "Epoch 114: accuracy on validation set: 0.5271\n",
      "Epoch 114: accuracy on training set: 0.5416\n",
      "Epoch 114: loss on validation set: 0.6841\n",
      "Epoch 115: loss on final training batch: 0.6707\n",
      "Epoch 115: accuracy on validation set: 0.5277\n",
      "Epoch 115: accuracy on training set: 0.5426\n",
      "Epoch 115: loss on validation set: 0.6841\n",
      "Epoch 116: loss on final training batch: 0.6808\n",
      "Epoch 116: accuracy on validation set: 0.5294\n",
      "Epoch 116: accuracy on training set: 0.5423\n",
      "Epoch 116: loss on validation set: 0.6842\n",
      "Epoch 117: loss on final training batch: 0.6716\n",
      "Epoch 117: accuracy on validation set: 0.5310\n",
      "Epoch 117: accuracy on training set: 0.5420\n",
      "Epoch 117: loss on validation set: 0.6842\n",
      "Epoch 118: loss on final training batch: 0.6713\n",
      "Epoch 118: accuracy on validation set: 0.5304\n",
      "Epoch 118: accuracy on training set: 0.5436\n",
      "Epoch 118: loss on validation set: 0.6841\n",
      "Epoch 119: loss on final training batch: 0.6683\n",
      "Epoch 119: accuracy on validation set: 0.5329\n",
      "Epoch 119: accuracy on training set: 0.5431\n",
      "Epoch 119: loss on validation set: 0.6841\n",
      "Epoch 120: loss on final training batch: 0.6740\n",
      "Epoch 120: accuracy on validation set: 0.5296\n",
      "Epoch 120: accuracy on training set: 0.5426\n",
      "Epoch 120: loss on validation set: 0.6843\n",
      "Epoch 121: loss on final training batch: 0.6646\n",
      "Epoch 121: accuracy on validation set: 0.5300\n",
      "Epoch 121: accuracy on training set: 0.5429\n",
      "Epoch 121: loss on validation set: 0.6842\n",
      "Epoch 122: loss on final training batch: 0.6686\n",
      "Epoch 122: accuracy on validation set: 0.5308\n",
      "Epoch 122: accuracy on training set: 0.5435\n",
      "Epoch 122: loss on validation set: 0.6842\n",
      "Epoch 123: loss on final training batch: 0.6672\n",
      "Epoch 123: accuracy on validation set: 0.5317\n",
      "Epoch 123: accuracy on training set: 0.5437\n",
      "Epoch 123: loss on validation set: 0.6842\n",
      "Epoch 124: loss on final training batch: 0.6616\n",
      "Epoch 124: accuracy on validation set: 0.5319\n",
      "Epoch 124: accuracy on training set: 0.5439\n",
      "Epoch 124: loss on validation set: 0.6843\n",
      "Epoch 125: loss on final training batch: 0.6670\n",
      "Epoch 125: accuracy on validation set: 0.5321\n",
      "Epoch 125: accuracy on training set: 0.5449\n",
      "Epoch 125: loss on validation set: 0.6842\n",
      "Epoch 126: loss on final training batch: 0.6580\n",
      "Epoch 126: accuracy on validation set: 0.5319\n",
      "Epoch 126: accuracy on training set: 0.5449\n",
      "Epoch 126: loss on validation set: 0.6843\n",
      "Epoch 127: loss on final training batch: 0.6684\n",
      "Epoch 127: accuracy on validation set: 0.5319\n",
      "Epoch 127: accuracy on training set: 0.5457\n",
      "Epoch 127: loss on validation set: 0.6844\n",
      "Epoch 128: loss on final training batch: 0.6595\n",
      "Epoch 128: accuracy on validation set: 0.5317\n",
      "Epoch 128: accuracy on training set: 0.5461\n",
      "Epoch 128: loss on validation set: 0.6843\n",
      "Epoch 129: loss on final training batch: 0.6653\n",
      "Epoch 129: accuracy on validation set: 0.5285\n",
      "Epoch 129: accuracy on training set: 0.5436\n",
      "Epoch 129: loss on validation set: 0.6845\n",
      "Epoch 130: loss on final training batch: 0.6611\n",
      "Epoch 130: accuracy on validation set: 0.5304\n",
      "Epoch 130: accuracy on training set: 0.5453\n",
      "Epoch 130: loss on validation set: 0.6843\n",
      "Epoch 131: loss on final training batch: 0.6594\n",
      "Epoch 131: accuracy on validation set: 0.5308\n",
      "Epoch 131: accuracy on training set: 0.5448\n",
      "Epoch 131: loss on validation set: 0.6844\n",
      "Epoch 132: loss on final training batch: 0.6482\n",
      "Epoch 132: accuracy on validation set: 0.5298\n",
      "Epoch 132: accuracy on training set: 0.5450\n",
      "Epoch 132: loss on validation set: 0.6845\n",
      "Epoch 133: loss on final training batch: 0.6701\n",
      "Epoch 133: accuracy on validation set: 0.5300\n",
      "Epoch 133: accuracy on training set: 0.5448\n",
      "Epoch 133: loss on validation set: 0.6844\n",
      "Epoch 134: loss on final training batch: 0.6552\n",
      "Epoch 134: accuracy on validation set: 0.5283\n",
      "Epoch 134: accuracy on training set: 0.5452\n",
      "Epoch 134: loss on validation set: 0.6845\n",
      "Epoch 135: loss on final training batch: 0.6578\n",
      "Epoch 135: accuracy on validation set: 0.5294\n",
      "Epoch 135: accuracy on training set: 0.5456\n",
      "Epoch 135: loss on validation set: 0.6845\n",
      "Epoch 136: loss on final training batch: 0.6654\n",
      "Epoch 136: accuracy on validation set: 0.5265\n",
      "Epoch 136: accuracy on training set: 0.5464\n",
      "Epoch 136: loss on validation set: 0.6846\n",
      "Epoch 137: loss on final training batch: 0.6617\n",
      "Epoch 137: accuracy on validation set: 0.5277\n",
      "Epoch 137: accuracy on training set: 0.5464\n",
      "Epoch 137: loss on validation set: 0.6848\n",
      "Epoch 138: loss on final training batch: 0.6517\n",
      "Epoch 138: accuracy on validation set: 0.5308\n",
      "Epoch 138: accuracy on training set: 0.5465\n",
      "Epoch 138: loss on validation set: 0.6847\n",
      "Epoch 139: loss on final training batch: 0.6563\n",
      "Epoch 139: accuracy on validation set: 0.5308\n",
      "Epoch 139: accuracy on training set: 0.5466\n",
      "Epoch 139: loss on validation set: 0.6847\n",
      "Epoch 140: loss on final training batch: 0.6495\n",
      "Epoch 140: accuracy on validation set: 0.5292\n",
      "Epoch 140: accuracy on training set: 0.5486\n",
      "Epoch 140: loss on validation set: 0.6846\n",
      "Epoch 141: loss on final training batch: 0.6567\n",
      "Epoch 141: accuracy on validation set: 0.5288\n",
      "Epoch 141: accuracy on training set: 0.5485\n",
      "Epoch 141: loss on validation set: 0.6847\n",
      "Epoch 142: loss on final training batch: 0.6589\n",
      "Epoch 142: accuracy on validation set: 0.5288\n",
      "Epoch 142: accuracy on training set: 0.5475\n",
      "Epoch 142: loss on validation set: 0.6847\n",
      "Epoch 143: loss on final training batch: 0.6499\n",
      "Epoch 143: accuracy on validation set: 0.5290\n",
      "Epoch 143: accuracy on training set: 0.5479\n",
      "Epoch 143: loss on validation set: 0.6848\n",
      "Epoch 144: loss on final training batch: 0.6503\n",
      "Epoch 144: accuracy on validation set: 0.5290\n",
      "Epoch 144: accuracy on training set: 0.5492\n",
      "Epoch 144: loss on validation set: 0.6847\n",
      "Epoch 145: loss on final training batch: 0.6510\n",
      "Epoch 145: accuracy on validation set: 0.5296\n",
      "Epoch 145: accuracy on training set: 0.5494\n",
      "Epoch 145: loss on validation set: 0.6847\n",
      "Epoch 146: loss on final training batch: 0.6568\n",
      "Epoch 146: accuracy on validation set: 0.5302\n",
      "Epoch 146: accuracy on training set: 0.5487\n",
      "Epoch 146: loss on validation set: 0.6848\n",
      "Epoch 147: loss on final training batch: 0.6506\n",
      "Epoch 147: accuracy on validation set: 0.5288\n",
      "Epoch 147: accuracy on training set: 0.5500\n",
      "Epoch 147: loss on validation set: 0.6848\n",
      "Epoch 148: loss on final training batch: 0.6586\n",
      "Epoch 148: accuracy on validation set: 0.5271\n",
      "Epoch 148: accuracy on training set: 0.5486\n",
      "Epoch 148: loss on validation set: 0.6848\n",
      "Epoch 149: loss on final training batch: 0.6523\n",
      "Epoch 149: accuracy on validation set: 0.5279\n",
      "Epoch 149: accuracy on training set: 0.5493\n",
      "Epoch 149: loss on validation set: 0.6848\n",
      "Epoch 150: loss on final training batch: 0.6435\n",
      "Epoch 150: accuracy on validation set: 0.5281\n",
      "Epoch 150: accuracy on training set: 0.5517\n",
      "Epoch 150: loss on validation set: 0.6847\n",
      "Epoch 151: loss on final training batch: 0.6495\n",
      "Epoch 151: accuracy on validation set: 0.5281\n",
      "Epoch 151: accuracy on training set: 0.5501\n",
      "Epoch 151: loss on validation set: 0.6849\n",
      "Epoch 152: loss on final training batch: 0.6590\n",
      "Epoch 152: accuracy on validation set: 0.5277\n",
      "Epoch 152: accuracy on training set: 0.5512\n",
      "Epoch 152: loss on validation set: 0.6849\n",
      "Epoch 153: loss on final training batch: 0.6466\n",
      "Epoch 153: accuracy on validation set: 0.5277\n",
      "Epoch 153: accuracy on training set: 0.5516\n",
      "Epoch 153: loss on validation set: 0.6849\n",
      "Epoch 154: loss on final training batch: 0.6473\n",
      "Epoch 154: accuracy on validation set: 0.5288\n",
      "Epoch 154: accuracy on training set: 0.5515\n",
      "Epoch 154: loss on validation set: 0.6850\n",
      "Epoch 155: loss on final training batch: 0.6437\n",
      "Epoch 155: accuracy on validation set: 0.5285\n",
      "Epoch 155: accuracy on training set: 0.5526\n",
      "Epoch 155: loss on validation set: 0.6850\n",
      "Epoch 156: loss on final training batch: 0.6533\n",
      "Epoch 156: accuracy on validation set: 0.5271\n",
      "Epoch 156: accuracy on training set: 0.5528\n",
      "Epoch 156: loss on validation set: 0.6850\n",
      "Epoch 157: loss on final training batch: 0.6451\n",
      "Epoch 157: accuracy on validation set: 0.5265\n",
      "Epoch 157: accuracy on training set: 0.5524\n",
      "Epoch 157: loss on validation set: 0.6850\n",
      "Epoch 158: loss on final training batch: 0.6495\n",
      "Epoch 158: accuracy on validation set: 0.5263\n",
      "Epoch 158: accuracy on training set: 0.5535\n",
      "Epoch 158: loss on validation set: 0.6851\n",
      "Epoch 159: loss on final training batch: 0.6521\n",
      "Epoch 159: accuracy on validation set: 0.5281\n",
      "Epoch 159: accuracy on training set: 0.5543\n",
      "Epoch 159: loss on validation set: 0.6851\n",
      "Epoch 160: loss on final training batch: 0.6362\n",
      "Epoch 160: accuracy on validation set: 0.5281\n",
      "Epoch 160: accuracy on training set: 0.5543\n",
      "Epoch 160: loss on validation set: 0.6852\n",
      "Epoch 161: loss on final training batch: 0.6449\n",
      "Epoch 161: accuracy on validation set: 0.5269\n",
      "Epoch 161: accuracy on training set: 0.5539\n",
      "Epoch 161: loss on validation set: 0.6853\n",
      "Epoch 162: loss on final training batch: 0.6393\n",
      "Epoch 162: accuracy on validation set: 0.5269\n",
      "Epoch 162: accuracy on training set: 0.5545\n",
      "Epoch 162: loss on validation set: 0.6852\n",
      "Epoch 163: loss on final training batch: 0.6335\n",
      "Epoch 163: accuracy on validation set: 0.5290\n",
      "Epoch 163: accuracy on training set: 0.5557\n",
      "Epoch 163: loss on validation set: 0.6851\n",
      "Epoch 164: loss on final training batch: 0.6363\n",
      "Epoch 164: accuracy on validation set: 0.5292\n",
      "Epoch 164: accuracy on training set: 0.5554\n",
      "Epoch 164: loss on validation set: 0.6852\n",
      "Epoch 165: loss on final training batch: 0.6422\n",
      "Epoch 165: accuracy on validation set: 0.5292\n",
      "Epoch 165: accuracy on training set: 0.5572\n",
      "Epoch 165: loss on validation set: 0.6851\n",
      "Epoch 166: loss on final training batch: 0.6296\n",
      "Epoch 166: accuracy on validation set: 0.5292\n",
      "Epoch 166: accuracy on training set: 0.5572\n",
      "Epoch 166: loss on validation set: 0.6853\n",
      "Epoch 167: loss on final training batch: 0.6354\n",
      "Epoch 167: accuracy on validation set: 0.5292\n",
      "Epoch 167: accuracy on training set: 0.5574\n",
      "Epoch 167: loss on validation set: 0.6854\n",
      "Epoch 168: loss on final training batch: 0.6281\n",
      "Epoch 168: accuracy on validation set: 0.5285\n",
      "Epoch 168: accuracy on training set: 0.5592\n",
      "Epoch 168: loss on validation set: 0.6853\n",
      "Epoch 169: loss on final training batch: 0.6217\n",
      "Epoch 169: accuracy on validation set: 0.5292\n",
      "Epoch 169: accuracy on training set: 0.5597\n",
      "Epoch 169: loss on validation set: 0.6854\n",
      "Epoch 170: loss on final training batch: 0.6239\n",
      "Epoch 170: accuracy on validation set: 0.5304\n",
      "Epoch 170: accuracy on training set: 0.5589\n",
      "Epoch 170: loss on validation set: 0.6855\n",
      "Epoch 171: loss on final training batch: 0.6352\n",
      "Epoch 171: accuracy on validation set: 0.5292\n",
      "Epoch 171: accuracy on training set: 0.5611\n",
      "Epoch 171: loss on validation set: 0.6855\n",
      "Epoch 172: loss on final training batch: 0.6180\n",
      "Epoch 172: accuracy on validation set: 0.5285\n",
      "Epoch 172: accuracy on training set: 0.5618\n",
      "Epoch 172: loss on validation set: 0.6856\n",
      "Epoch 173: loss on final training batch: 0.6278\n",
      "Epoch 173: accuracy on validation set: 0.5279\n",
      "Epoch 173: accuracy on training set: 0.5619\n",
      "Epoch 173: loss on validation set: 0.6857\n",
      "Epoch 174: loss on final training batch: 0.6306\n",
      "Epoch 174: accuracy on validation set: 0.5294\n",
      "Epoch 174: accuracy on training set: 0.5633\n",
      "Epoch 174: loss on validation set: 0.6856\n",
      "Epoch 175: loss on final training batch: 0.6252\n",
      "Epoch 175: accuracy on validation set: 0.5283\n",
      "Epoch 175: accuracy on training set: 0.5622\n",
      "Epoch 175: loss on validation set: 0.6858\n",
      "Epoch 176: loss on final training batch: 0.6276\n",
      "Epoch 176: accuracy on validation set: 0.5304\n",
      "Epoch 176: accuracy on training set: 0.5637\n",
      "Epoch 176: loss on validation set: 0.6857\n",
      "Epoch 177: loss on final training batch: 0.6222\n",
      "Epoch 177: accuracy on validation set: 0.5296\n",
      "Epoch 177: accuracy on training set: 0.5642\n",
      "Epoch 177: loss on validation set: 0.6859\n",
      "Epoch 178: loss on final training batch: 0.6257\n",
      "Epoch 178: accuracy on validation set: 0.5292\n",
      "Epoch 178: accuracy on training set: 0.5665\n",
      "Epoch 178: loss on validation set: 0.6860\n",
      "Epoch 179: loss on final training batch: 0.6150\n",
      "Epoch 179: accuracy on validation set: 0.5263\n",
      "Epoch 179: accuracy on training set: 0.5650\n",
      "Epoch 179: loss on validation set: 0.6862\n",
      "Epoch 180: loss on final training batch: 0.6126\n",
      "Epoch 180: accuracy on validation set: 0.5273\n",
      "Epoch 180: accuracy on training set: 0.5656\n",
      "Epoch 180: loss on validation set: 0.6863\n",
      "Epoch 181: loss on final training batch: 0.6148\n",
      "Epoch 181: accuracy on validation set: 0.5267\n",
      "Epoch 181: accuracy on training set: 0.5668\n",
      "Epoch 181: loss on validation set: 0.6864\n",
      "Epoch 182: loss on final training batch: 0.6178\n",
      "Epoch 182: accuracy on validation set: 0.5258\n",
      "Epoch 182: accuracy on training set: 0.5682\n",
      "Epoch 182: loss on validation set: 0.6864\n",
      "Epoch 183: loss on final training batch: 0.6012\n",
      "Epoch 183: accuracy on validation set: 0.5258\n",
      "Epoch 183: accuracy on training set: 0.5681\n",
      "Epoch 183: loss on validation set: 0.6865\n",
      "Epoch 184: loss on final training batch: 0.5934\n",
      "Epoch 184: accuracy on validation set: 0.5265\n",
      "Epoch 184: accuracy on training set: 0.5709\n",
      "Epoch 184: loss on validation set: 0.6866\n",
      "Epoch 185: loss on final training batch: 0.6035\n",
      "Epoch 185: accuracy on validation set: 0.5256\n",
      "Epoch 185: accuracy on training set: 0.5707\n",
      "Epoch 185: loss on validation set: 0.6867\n",
      "Epoch 186: loss on final training batch: 0.6016\n",
      "Epoch 186: accuracy on validation set: 0.5254\n",
      "Epoch 186: accuracy on training set: 0.5699\n",
      "Epoch 186: loss on validation set: 0.6868\n",
      "Epoch 187: loss on final training batch: 0.5918\n",
      "Epoch 187: accuracy on validation set: 0.5244\n",
      "Epoch 187: accuracy on training set: 0.5711\n",
      "Epoch 187: loss on validation set: 0.6870\n",
      "Epoch 188: loss on final training batch: 0.6007\n",
      "Epoch 188: accuracy on validation set: 0.5248\n",
      "Epoch 188: accuracy on training set: 0.5726\n",
      "Epoch 188: loss on validation set: 0.6868\n",
      "Epoch 189: loss on final training batch: 0.6102\n",
      "Epoch 189: accuracy on validation set: 0.5263\n",
      "Epoch 189: accuracy on training set: 0.5752\n",
      "Epoch 189: loss on validation set: 0.6867\n",
      "Epoch 190: loss on final training batch: 0.5820\n",
      "Epoch 190: accuracy on validation set: 0.5281\n",
      "Epoch 190: accuracy on training set: 0.5753\n",
      "Epoch 190: loss on validation set: 0.6868\n",
      "Epoch 191: loss on final training batch: 0.5735\n",
      "Epoch 191: accuracy on validation set: 0.5290\n",
      "Epoch 191: accuracy on training set: 0.5779\n",
      "Epoch 191: loss on validation set: 0.6868\n",
      "Epoch 192: loss on final training batch: 0.6085\n",
      "Epoch 192: accuracy on validation set: 0.5290\n",
      "Epoch 192: accuracy on training set: 0.5776\n",
      "Epoch 192: loss on validation set: 0.6869\n",
      "Epoch 193: loss on final training batch: 0.5808\n",
      "Epoch 193: accuracy on validation set: 0.5285\n",
      "Epoch 193: accuracy on training set: 0.5776\n",
      "Epoch 193: loss on validation set: 0.6870\n",
      "Epoch 194: loss on final training batch: 0.5877\n",
      "Epoch 194: accuracy on validation set: 0.5281\n",
      "Epoch 194: accuracy on training set: 0.5786\n",
      "Epoch 194: loss on validation set: 0.6873\n",
      "Epoch 195: loss on final training batch: 0.5762\n",
      "Epoch 195: accuracy on validation set: 0.5260\n",
      "Epoch 195: accuracy on training set: 0.5785\n",
      "Epoch 195: loss on validation set: 0.6876\n",
      "Epoch 196: loss on final training batch: 0.5862\n",
      "Epoch 196: accuracy on validation set: 0.5244\n",
      "Epoch 196: accuracy on training set: 0.5799\n",
      "Epoch 196: loss on validation set: 0.6876\n",
      "Epoch 197: loss on final training batch: 0.5876\n",
      "Epoch 197: accuracy on validation set: 0.5277\n",
      "Epoch 197: accuracy on training set: 0.5800\n",
      "Epoch 197: loss on validation set: 0.6876\n",
      "Epoch 198: loss on final training batch: 0.5778\n",
      "Epoch 198: accuracy on validation set: 0.5271\n",
      "Epoch 198: accuracy on training set: 0.5817\n",
      "Epoch 198: loss on validation set: 0.6876\n",
      "Epoch 199: loss on final training batch: 0.5750\n",
      "Epoch 199: accuracy on validation set: 0.5267\n",
      "Epoch 199: accuracy on training set: 0.5812\n",
      "Epoch 199: loss on validation set: 0.6878\n",
      "Epoch 200: loss on final training batch: 0.5586\n",
      "Epoch 200: accuracy on validation set: 0.5252\n",
      "Epoch 200: accuracy on training set: 0.5812\n",
      "Epoch 200: loss on validation set: 0.6881\n",
      "Epoch 201: loss on final training batch: 0.5890\n",
      "Epoch 201: accuracy on validation set: 0.5250\n",
      "Epoch 201: accuracy on training set: 0.5841\n",
      "Epoch 201: loss on validation set: 0.6881\n",
      "Epoch 202: loss on final training batch: 0.5708\n",
      "Epoch 202: accuracy on validation set: 0.5246\n",
      "Epoch 202: accuracy on training set: 0.5833\n",
      "Epoch 202: loss on validation set: 0.6883\n",
      "Epoch 203: loss on final training batch: 0.5625\n",
      "Epoch 203: accuracy on validation set: 0.5240\n",
      "Epoch 203: accuracy on training set: 0.5849\n",
      "Epoch 203: loss on validation set: 0.6884\n",
      "Epoch 204: loss on final training batch: 0.5491\n",
      "Epoch 204: accuracy on validation set: 0.5260\n",
      "Epoch 204: accuracy on training set: 0.5893\n",
      "Epoch 204: loss on validation set: 0.6884\n",
      "Epoch 205: loss on final training batch: 0.5427\n",
      "Epoch 205: accuracy on validation set: 0.5267\n",
      "Epoch 205: accuracy on training set: 0.5888\n",
      "Epoch 205: loss on validation set: 0.6884\n",
      "Epoch 206: loss on final training batch: 0.5396\n",
      "Epoch 206: accuracy on validation set: 0.5269\n",
      "Epoch 206: accuracy on training set: 0.5892\n",
      "Epoch 206: loss on validation set: 0.6887\n",
      "Epoch 207: loss on final training batch: 0.5437\n",
      "Epoch 207: accuracy on validation set: 0.5263\n",
      "Epoch 207: accuracy on training set: 0.5892\n",
      "Epoch 207: loss on validation set: 0.6888\n",
      "Epoch 208: loss on final training batch: 0.5346\n",
      "Epoch 208: accuracy on validation set: 0.5260\n",
      "Epoch 208: accuracy on training set: 0.5909\n",
      "Epoch 208: loss on validation set: 0.6889\n",
      "Epoch 209: loss on final training batch: 0.5610\n",
      "Epoch 209: accuracy on validation set: 0.5263\n",
      "Epoch 209: accuracy on training set: 0.5914\n",
      "Epoch 209: loss on validation set: 0.6890\n",
      "Epoch 210: loss on final training batch: 0.5400\n",
      "Epoch 210: accuracy on validation set: 0.5258\n",
      "Epoch 210: accuracy on training set: 0.5937\n",
      "Epoch 210: loss on validation set: 0.6891\n",
      "Epoch 211: loss on final training batch: 0.5167\n",
      "Epoch 211: accuracy on validation set: 0.5250\n",
      "Epoch 211: accuracy on training set: 0.5940\n",
      "Epoch 211: loss on validation set: 0.6893\n",
      "Epoch 212: loss on final training batch: 0.5445\n",
      "Epoch 212: accuracy on validation set: 0.5233\n",
      "Epoch 212: accuracy on training set: 0.5955\n",
      "Epoch 212: loss on validation set: 0.6896\n",
      "Epoch 213: loss on final training batch: 0.5316\n",
      "Epoch 213: accuracy on validation set: 0.5248\n",
      "Epoch 213: accuracy on training set: 0.6000\n",
      "Epoch 213: loss on validation set: 0.6900\n",
      "Epoch 214: loss on final training batch: 0.5278\n",
      "Epoch 214: accuracy on validation set: 0.5252\n",
      "Epoch 214: accuracy on training set: 0.5993\n",
      "Epoch 214: loss on validation set: 0.6899\n",
      "Epoch 215: loss on final training batch: 0.5351\n",
      "Epoch 215: accuracy on validation set: 0.5267\n",
      "Epoch 215: accuracy on training set: 0.5989\n",
      "Epoch 215: loss on validation set: 0.6901\n",
      "Epoch 216: loss on final training batch: 0.5134\n",
      "Epoch 216: accuracy on validation set: 0.5256\n",
      "Epoch 216: accuracy on training set: 0.6013\n",
      "Epoch 216: loss on validation set: 0.6903\n",
      "Epoch 217: loss on final training batch: 0.5251\n",
      "Epoch 217: accuracy on validation set: 0.5238\n",
      "Epoch 217: accuracy on training set: 0.6007\n",
      "Epoch 217: loss on validation set: 0.6908\n",
      "Epoch 218: loss on final training batch: 0.5198\n",
      "Epoch 218: accuracy on validation set: 0.5248\n",
      "Epoch 218: accuracy on training set: 0.6026\n",
      "Epoch 218: loss on validation set: 0.6910\n",
      "Epoch 219: loss on final training batch: 0.5003\n",
      "Epoch 219: accuracy on validation set: 0.5275\n",
      "Epoch 219: accuracy on training set: 0.6040\n",
      "Epoch 219: loss on validation set: 0.6915\n",
      "Epoch 220: loss on final training batch: 0.5070\n",
      "Epoch 220: accuracy on validation set: 0.5256\n",
      "Epoch 220: accuracy on training set: 0.6064\n",
      "Epoch 220: loss on validation set: 0.6915\n",
      "Epoch 221: loss on final training batch: 0.5270\n",
      "Epoch 221: accuracy on validation set: 0.5258\n",
      "Epoch 221: accuracy on training set: 0.6060\n",
      "Epoch 221: loss on validation set: 0.6916\n",
      "Epoch 222: loss on final training batch: 0.4900\n",
      "Epoch 222: accuracy on validation set: 0.5256\n",
      "Epoch 222: accuracy on training set: 0.6066\n",
      "Epoch 222: loss on validation set: 0.6917\n",
      "Epoch 223: loss on final training batch: 0.5042\n",
      "Epoch 223: accuracy on validation set: 0.5267\n",
      "Epoch 223: accuracy on training set: 0.6052\n",
      "Epoch 223: loss on validation set: 0.6920\n",
      "Epoch 224: loss on final training batch: 0.4993\n",
      "Epoch 224: accuracy on validation set: 0.5244\n",
      "Epoch 224: accuracy on training set: 0.6085\n",
      "Epoch 224: loss on validation set: 0.6922\n",
      "Epoch 225: loss on final training batch: 0.5055\n",
      "Epoch 225: accuracy on validation set: 0.5248\n",
      "Epoch 225: accuracy on training set: 0.6081\n",
      "Epoch 225: loss on validation set: 0.6928\n",
      "Epoch 226: loss on final training batch: 0.4946\n",
      "Epoch 226: accuracy on validation set: 0.5254\n",
      "Epoch 226: accuracy on training set: 0.6067\n",
      "Epoch 226: loss on validation set: 0.6933\n",
      "Epoch 227: loss on final training batch: 0.4893\n",
      "Epoch 227: accuracy on validation set: 0.5256\n",
      "Epoch 227: accuracy on training set: 0.6116\n",
      "Epoch 227: loss on validation set: 0.6938\n",
      "Epoch 228: loss on final training batch: 0.4819\n",
      "Epoch 228: accuracy on validation set: 0.5292\n",
      "Epoch 228: accuracy on training set: 0.6201\n",
      "Epoch 228: loss on validation set: 0.6939\n",
      "Epoch 229: loss on final training batch: 0.4701\n",
      "Epoch 229: accuracy on validation set: 0.5263\n",
      "Epoch 229: accuracy on training set: 0.6122\n",
      "Epoch 229: loss on validation set: 0.6941\n",
      "Epoch 230: loss on final training batch: 0.4842\n",
      "Epoch 230: accuracy on validation set: 0.5267\n",
      "Epoch 230: accuracy on training set: 0.6197\n",
      "Epoch 230: loss on validation set: 0.6944\n",
      "Epoch 231: loss on final training batch: 0.4837\n",
      "Epoch 231: accuracy on validation set: 0.5277\n",
      "Epoch 231: accuracy on training set: 0.6184\n",
      "Epoch 231: loss on validation set: 0.6946\n",
      "Epoch 232: loss on final training batch: 0.4450\n",
      "Epoch 232: accuracy on validation set: 0.5281\n",
      "Epoch 232: accuracy on training set: 0.6165\n",
      "Epoch 232: loss on validation set: 0.6950\n",
      "Epoch 233: loss on final training batch: 0.4537\n",
      "Epoch 233: accuracy on validation set: 0.5267\n",
      "Epoch 233: accuracy on training set: 0.6168\n",
      "Epoch 233: loss on validation set: 0.6951\n",
      "Epoch 234: loss on final training batch: 0.4504\n",
      "Epoch 234: accuracy on validation set: 0.5260\n",
      "Epoch 234: accuracy on training set: 0.6165\n",
      "Epoch 234: loss on validation set: 0.6953\n",
      "Epoch 235: loss on final training batch: 0.4641\n",
      "Epoch 235: accuracy on validation set: 0.5258\n",
      "Epoch 235: accuracy on training set: 0.6223\n",
      "Epoch 235: loss on validation set: 0.6955\n",
      "Epoch 236: loss on final training batch: 0.4723\n",
      "Epoch 236: accuracy on validation set: 0.5304\n",
      "Epoch 236: accuracy on training set: 0.6247\n",
      "Epoch 236: loss on validation set: 0.6957\n",
      "Epoch 237: loss on final training batch: 0.4461\n",
      "Epoch 237: accuracy on validation set: 0.5323\n",
      "Epoch 237: accuracy on training set: 0.6229\n",
      "Epoch 237: loss on validation set: 0.6960\n",
      "Epoch 238: loss on final training batch: 0.4664\n",
      "Epoch 238: accuracy on validation set: 0.5292\n",
      "Epoch 238: accuracy on training set: 0.6182\n",
      "Epoch 238: loss on validation set: 0.6968\n",
      "Epoch 239: loss on final training batch: 0.4408\n",
      "Epoch 239: accuracy on validation set: 0.5288\n",
      "Epoch 239: accuracy on training set: 0.6220\n",
      "Epoch 239: loss on validation set: 0.6976\n",
      "Epoch 240: loss on final training batch: 0.4550\n",
      "Epoch 240: accuracy on validation set: 0.5281\n",
      "Epoch 240: accuracy on training set: 0.6199\n",
      "Epoch 240: loss on validation set: 0.6977\n",
      "Epoch 241: loss on final training batch: 0.4601\n",
      "Epoch 241: accuracy on validation set: 0.5283\n",
      "Epoch 241: accuracy on training set: 0.6199\n",
      "Epoch 241: loss on validation set: 0.6977\n",
      "Epoch 242: loss on final training batch: 0.4612\n",
      "Epoch 242: accuracy on validation set: 0.5267\n",
      "Epoch 242: accuracy on training set: 0.6218\n",
      "Epoch 242: loss on validation set: 0.6978\n",
      "Epoch 243: loss on final training batch: 0.4399\n",
      "Epoch 243: accuracy on validation set: 0.5238\n",
      "Epoch 243: accuracy on training set: 0.6249\n",
      "Epoch 243: loss on validation set: 0.6989\n",
      "Epoch 244: loss on final training batch: 0.4386\n",
      "Epoch 244: accuracy on validation set: 0.5244\n",
      "Epoch 244: accuracy on training set: 0.6258\n",
      "Epoch 244: loss on validation set: 0.6995\n",
      "Epoch 245: loss on final training batch: 0.4240\n",
      "Epoch 245: accuracy on validation set: 0.5225\n",
      "Epoch 245: accuracy on training set: 0.6275\n",
      "Epoch 245: loss on validation set: 0.7007\n",
      "Epoch 246: loss on final training batch: 0.4053\n",
      "Epoch 246: accuracy on validation set: 0.5260\n",
      "Epoch 246: accuracy on training set: 0.6287\n",
      "Epoch 246: loss on validation set: 0.7011\n",
      "Epoch 247: loss on final training batch: 0.4590\n",
      "Epoch 247: accuracy on validation set: 0.5267\n",
      "Epoch 247: accuracy on training set: 0.6303\n",
      "Epoch 247: loss on validation set: 0.7013\n",
      "Epoch 248: loss on final training batch: 0.4459\n",
      "Epoch 248: accuracy on validation set: 0.5269\n",
      "Epoch 248: accuracy on training set: 0.6277\n",
      "Epoch 248: loss on validation set: 0.7013\n",
      "Epoch 249: loss on final training batch: 0.4515\n",
      "Epoch 249: accuracy on validation set: 0.5252\n",
      "Epoch 249: accuracy on training set: 0.6298\n",
      "Epoch 249: loss on validation set: 0.7013\n",
      "Epoch 250: loss on final training batch: 0.3992\n",
      "Epoch 250: accuracy on validation set: 0.5235\n",
      "Epoch 250: accuracy on training set: 0.6330\n",
      "Epoch 250: loss on validation set: 0.7013\n",
      "Epoch 251: loss on final training batch: 0.4096\n",
      "Epoch 251: accuracy on validation set: 0.5231\n",
      "Epoch 251: accuracy on training set: 0.6323\n",
      "Epoch 251: loss on validation set: 0.7024\n",
      "Epoch 252: loss on final training batch: 0.4455\n",
      "Epoch 252: accuracy on validation set: 0.5267\n",
      "Epoch 252: accuracy on training set: 0.6326\n",
      "Epoch 252: loss on validation set: 0.7026\n",
      "Epoch 253: loss on final training batch: 0.3993\n",
      "Epoch 253: accuracy on validation set: 0.5254\n",
      "Epoch 253: accuracy on training set: 0.6375\n",
      "Epoch 253: loss on validation set: 0.7027\n",
      "Epoch 254: loss on final training batch: 0.4198\n",
      "Epoch 254: accuracy on validation set: 0.5231\n",
      "Epoch 254: accuracy on training set: 0.6411\n",
      "Epoch 254: loss on validation set: 0.7032\n",
      "Epoch 255: loss on final training batch: 0.4283\n",
      "Epoch 255: accuracy on validation set: 0.5233\n",
      "Epoch 255: accuracy on training set: 0.6428\n",
      "Epoch 255: loss on validation set: 0.7032\n",
      "Epoch 256: loss on final training batch: 0.4135\n",
      "Epoch 256: accuracy on validation set: 0.5265\n",
      "Epoch 256: accuracy on training set: 0.6349\n",
      "Epoch 256: loss on validation set: 0.7039\n",
      "Epoch 257: loss on final training batch: 0.4226\n",
      "Epoch 257: accuracy on validation set: 0.5285\n",
      "Epoch 257: accuracy on training set: 0.6371\n",
      "Epoch 257: loss on validation set: 0.7046\n",
      "Epoch 258: loss on final training batch: 0.4480\n",
      "Epoch 258: accuracy on validation set: 0.5285\n",
      "Epoch 258: accuracy on training set: 0.6469\n",
      "Epoch 258: loss on validation set: 0.7051\n",
      "Epoch 259: loss on final training batch: 0.4111\n",
      "Epoch 259: accuracy on validation set: 0.5256\n",
      "Epoch 259: accuracy on training set: 0.6481\n",
      "Epoch 259: loss on validation set: 0.7059\n",
      "Epoch 260: loss on final training batch: 0.4042\n",
      "Epoch 260: accuracy on validation set: 0.5260\n",
      "Epoch 260: accuracy on training set: 0.6446\n",
      "Epoch 260: loss on validation set: 0.7058\n",
      "Epoch 261: loss on final training batch: 0.4039\n",
      "Epoch 261: accuracy on validation set: 0.5206\n",
      "Epoch 261: accuracy on training set: 0.6421\n",
      "Epoch 261: loss on validation set: 0.7069\n",
      "Epoch 262: loss on final training batch: 0.4061\n",
      "Epoch 262: accuracy on validation set: 0.5240\n",
      "Epoch 262: accuracy on training set: 0.6414\n",
      "Epoch 262: loss on validation set: 0.7067\n",
      "Epoch 263: loss on final training batch: 0.3983\n",
      "Epoch 263: accuracy on validation set: 0.5260\n",
      "Epoch 263: accuracy on training set: 0.6491\n",
      "Epoch 263: loss on validation set: 0.7075\n",
      "Epoch 264: loss on final training batch: 0.3837\n",
      "Epoch 264: accuracy on validation set: 0.5229\n",
      "Epoch 264: accuracy on training set: 0.6476\n",
      "Epoch 264: loss on validation set: 0.7083\n",
      "Epoch 265: loss on final training batch: 0.3711\n",
      "Epoch 265: accuracy on validation set: 0.5246\n",
      "Epoch 265: accuracy on training set: 0.6504\n",
      "Epoch 265: loss on validation set: 0.7093\n",
      "Epoch 266: loss on final training batch: 0.3600\n",
      "Epoch 266: accuracy on validation set: 0.5265\n",
      "Epoch 266: accuracy on training set: 0.6544\n",
      "Epoch 266: loss on validation set: 0.7103\n",
      "Epoch 267: loss on final training batch: 0.3934\n",
      "Epoch 267: accuracy on validation set: 0.5260\n",
      "Epoch 267: accuracy on training set: 0.6513\n",
      "Epoch 267: loss on validation set: 0.7107\n",
      "Epoch 268: loss on final training batch: 0.3903\n",
      "Epoch 268: accuracy on validation set: 0.5254\n",
      "Epoch 268: accuracy on training set: 0.6459\n",
      "Epoch 268: loss on validation set: 0.7117\n",
      "Epoch 269: loss on final training batch: 0.3973\n",
      "Epoch 269: accuracy on validation set: 0.5229\n",
      "Epoch 269: accuracy on training set: 0.6408\n",
      "Epoch 269: loss on validation set: 0.7117\n",
      "Epoch 270: loss on final training batch: 0.3880\n",
      "Epoch 270: accuracy on validation set: 0.5227\n",
      "Epoch 270: accuracy on training set: 0.6499\n",
      "Epoch 270: loss on validation set: 0.7117\n",
      "Epoch 271: loss on final training batch: 0.3712\n",
      "Epoch 271: accuracy on validation set: 0.5246\n",
      "Epoch 271: accuracy on training set: 0.6472\n",
      "Epoch 271: loss on validation set: 0.7115\n",
      "Epoch 272: loss on final training batch: 0.3932\n",
      "Epoch 272: accuracy on validation set: 0.5219\n",
      "Epoch 272: accuracy on training set: 0.6520\n",
      "Epoch 272: loss on validation set: 0.7126\n",
      "Epoch 273: loss on final training batch: 0.3938\n",
      "Epoch 273: accuracy on validation set: 0.5215\n",
      "Epoch 273: accuracy on training set: 0.6538\n",
      "Epoch 273: loss on validation set: 0.7135\n",
      "Epoch 274: loss on final training batch: 0.3878\n",
      "Epoch 274: accuracy on validation set: 0.5225\n",
      "Epoch 274: accuracy on training set: 0.6568\n",
      "Epoch 274: loss on validation set: 0.7143\n",
      "Epoch 275: loss on final training batch: 0.3964\n",
      "Epoch 275: accuracy on validation set: 0.5223\n",
      "Epoch 275: accuracy on training set: 0.6624\n",
      "Epoch 275: loss on validation set: 0.7146\n",
      "Epoch 276: loss on final training batch: 0.3776\n",
      "Epoch 276: accuracy on validation set: 0.5240\n",
      "Epoch 276: accuracy on training set: 0.6567\n",
      "Epoch 276: loss on validation set: 0.7158\n",
      "Epoch 277: loss on final training batch: 0.3660\n",
      "Epoch 277: accuracy on validation set: 0.5288\n",
      "Epoch 277: accuracy on training set: 0.6633\n",
      "Epoch 277: loss on validation set: 0.7176\n",
      "Epoch 278: loss on final training batch: 0.3475\n",
      "Epoch 278: accuracy on validation set: 0.5238\n",
      "Epoch 278: accuracy on training set: 0.6556\n",
      "Epoch 278: loss on validation set: 0.7173\n",
      "Epoch 279: loss on final training batch: 0.3773\n",
      "Epoch 279: accuracy on validation set: 0.5225\n",
      "Epoch 279: accuracy on training set: 0.6573\n",
      "Epoch 279: loss on validation set: 0.7176\n",
      "Epoch 280: loss on final training batch: 0.3640\n",
      "Epoch 280: accuracy on validation set: 0.5244\n",
      "Epoch 280: accuracy on training set: 0.6625\n",
      "Epoch 280: loss on validation set: 0.7187\n",
      "Epoch 281: loss on final training batch: 0.3475\n",
      "Epoch 281: accuracy on validation set: 0.5219\n",
      "Epoch 281: accuracy on training set: 0.6556\n",
      "Epoch 281: loss on validation set: 0.7200\n",
      "Epoch 282: loss on final training batch: 0.3757\n",
      "Epoch 282: accuracy on validation set: 0.5225\n",
      "Epoch 282: accuracy on training set: 0.6596\n",
      "Epoch 282: loss on validation set: 0.7215\n",
      "Epoch 283: loss on final training batch: 0.3493\n",
      "Epoch 283: accuracy on validation set: 0.5200\n",
      "Epoch 283: accuracy on training set: 0.6579\n",
      "Epoch 283: loss on validation set: 0.7219\n",
      "Epoch 284: loss on final training batch: 0.3806\n",
      "Epoch 284: accuracy on validation set: 0.5208\n",
      "Epoch 284: accuracy on training set: 0.6592\n",
      "Epoch 284: loss on validation set: 0.7223\n",
      "Epoch 285: loss on final training batch: 0.3597\n",
      "Epoch 285: accuracy on validation set: 0.5196\n",
      "Epoch 285: accuracy on training set: 0.6539\n",
      "Epoch 285: loss on validation set: 0.7231\n",
      "Epoch 286: loss on final training batch: 0.3466\n",
      "Epoch 286: accuracy on validation set: 0.5213\n",
      "Epoch 286: accuracy on training set: 0.6567\n",
      "Epoch 286: loss on validation set: 0.7242\n",
      "Epoch 287: loss on final training batch: 0.3737\n",
      "Epoch 287: accuracy on validation set: 0.5192\n",
      "Epoch 287: accuracy on training set: 0.6601\n",
      "Epoch 287: loss on validation set: 0.7244\n",
      "Epoch 288: loss on final training batch: 0.3473\n",
      "Epoch 288: accuracy on validation set: 0.5190\n",
      "Epoch 288: accuracy on training set: 0.6651\n",
      "Epoch 288: loss on validation set: 0.7255\n",
      "Epoch 289: loss on final training batch: 0.3751\n",
      "Epoch 289: accuracy on validation set: 0.5240\n",
      "Epoch 289: accuracy on training set: 0.6662\n",
      "Epoch 289: loss on validation set: 0.7251\n",
      "Epoch 290: loss on final training batch: 0.3258\n",
      "Epoch 290: accuracy on validation set: 0.5213\n",
      "Epoch 290: accuracy on training set: 0.6632\n",
      "Epoch 290: loss on validation set: 0.7249\n",
      "Epoch 291: loss on final training batch: 0.3515\n",
      "Epoch 291: accuracy on validation set: 0.5217\n",
      "Epoch 291: accuracy on training set: 0.6647\n",
      "Epoch 291: loss on validation set: 0.7258\n",
      "Epoch 292: loss on final training batch: 0.3358\n",
      "Epoch 292: accuracy on validation set: 0.5210\n",
      "Epoch 292: accuracy on training set: 0.6646\n",
      "Epoch 292: loss on validation set: 0.7253\n",
      "Epoch 293: loss on final training batch: 0.3425\n",
      "Epoch 293: accuracy on validation set: 0.5188\n",
      "Epoch 293: accuracy on training set: 0.6681\n",
      "Epoch 293: loss on validation set: 0.7259\n",
      "Epoch 294: loss on final training batch: 0.3501\n",
      "Epoch 294: accuracy on validation set: 0.5175\n",
      "Epoch 294: accuracy on training set: 0.6653\n",
      "Epoch 294: loss on validation set: 0.7269\n",
      "Epoch 295: loss on final training batch: 0.3332\n",
      "Epoch 295: accuracy on validation set: 0.5231\n",
      "Epoch 295: accuracy on training set: 0.6717\n",
      "Epoch 295: loss on validation set: 0.7280\n",
      "Epoch 296: loss on final training batch: 0.3471\n",
      "Epoch 296: accuracy on validation set: 0.5225\n",
      "Epoch 296: accuracy on training set: 0.6722\n",
      "Epoch 296: loss on validation set: 0.7294\n",
      "Epoch 297: loss on final training batch: 0.3535\n",
      "Epoch 297: accuracy on validation set: 0.5210\n",
      "Epoch 297: accuracy on training set: 0.6657\n",
      "Epoch 297: loss on validation set: 0.7294\n",
      "Epoch 298: loss on final training batch: 0.3296\n",
      "Epoch 298: accuracy on validation set: 0.5235\n",
      "Epoch 298: accuracy on training set: 0.6678\n",
      "Epoch 298: loss on validation set: 0.7295\n",
      "Epoch 299: loss on final training batch: 0.3511\n",
      "Epoch 299: accuracy on validation set: 0.5252\n",
      "Epoch 299: accuracy on training set: 0.6702\n",
      "Epoch 299: loss on validation set: 0.7305\n",
      "Epoch 300: loss on final training batch: 0.3200\n",
      "Epoch 300: accuracy on validation set: 0.5240\n",
      "Epoch 300: accuracy on training set: 0.6746\n",
      "Epoch 300: loss on validation set: 0.7310\n",
      "Epoch 301: loss on final training batch: 0.3359\n",
      "Epoch 301: accuracy on validation set: 0.5235\n",
      "Epoch 301: accuracy on training set: 0.6740\n",
      "Epoch 301: loss on validation set: 0.7323\n",
      "Epoch 302: loss on final training batch: 0.3312\n",
      "Epoch 302: accuracy on validation set: 0.5221\n",
      "Epoch 302: accuracy on training set: 0.6751\n",
      "Epoch 302: loss on validation set: 0.7331\n",
      "Epoch 303: loss on final training batch: 0.3230\n",
      "Epoch 303: accuracy on validation set: 0.5275\n",
      "Epoch 303: accuracy on training set: 0.6746\n",
      "Epoch 303: loss on validation set: 0.7335\n",
      "Epoch 304: loss on final training batch: 0.3195\n",
      "Epoch 304: accuracy on validation set: 0.5231\n",
      "Epoch 304: accuracy on training set: 0.6722\n",
      "Epoch 304: loss on validation set: 0.7337\n",
      "Epoch 305: loss on final training batch: 0.3582\n",
      "Epoch 305: accuracy on validation set: 0.5204\n",
      "Epoch 305: accuracy on training set: 0.6718\n",
      "Epoch 305: loss on validation set: 0.7353\n",
      "Epoch 306: loss on final training batch: 0.3347\n",
      "Epoch 306: accuracy on validation set: 0.5190\n",
      "Epoch 306: accuracy on training set: 0.6713\n",
      "Epoch 306: loss on validation set: 0.7352\n",
      "Epoch 307: loss on final training batch: 0.3271\n",
      "Epoch 307: accuracy on validation set: 0.5210\n",
      "Epoch 307: accuracy on training set: 0.6748\n",
      "Epoch 307: loss on validation set: 0.7355\n",
      "Epoch 308: loss on final training batch: 0.3463\n",
      "Epoch 308: accuracy on validation set: 0.5263\n",
      "Epoch 308: accuracy on training set: 0.6763\n",
      "Epoch 308: loss on validation set: 0.7363\n",
      "Epoch 309: loss on final training batch: 0.3177\n",
      "Epoch 309: accuracy on validation set: 0.5238\n",
      "Epoch 309: accuracy on training set: 0.6731\n",
      "Epoch 309: loss on validation set: 0.7368\n",
      "Epoch 310: loss on final training batch: 0.3546\n",
      "Epoch 310: accuracy on validation set: 0.5256\n",
      "Epoch 310: accuracy on training set: 0.6752\n",
      "Epoch 310: loss on validation set: 0.7365\n",
      "Epoch 311: loss on final training batch: 0.3098\n",
      "Epoch 311: accuracy on validation set: 0.5194\n",
      "Epoch 311: accuracy on training set: 0.6792\n",
      "Epoch 311: loss on validation set: 0.7380\n",
      "Epoch 312: loss on final training batch: 0.2972\n",
      "Epoch 312: accuracy on validation set: 0.5238\n",
      "Epoch 312: accuracy on training set: 0.6769\n",
      "Epoch 312: loss on validation set: 0.7388\n",
      "Epoch 313: loss on final training batch: 0.2990\n",
      "Epoch 313: accuracy on validation set: 0.5235\n",
      "Epoch 313: accuracy on training set: 0.6719\n",
      "Epoch 313: loss on validation set: 0.7397\n",
      "Epoch 314: loss on final training batch: 0.3184\n",
      "Epoch 314: accuracy on validation set: 0.5227\n",
      "Epoch 314: accuracy on training set: 0.6786\n",
      "Epoch 314: loss on validation set: 0.7399\n",
      "Epoch 315: loss on final training batch: 0.3192\n",
      "Epoch 315: accuracy on validation set: 0.5206\n",
      "Epoch 315: accuracy on training set: 0.6810\n",
      "Epoch 315: loss on validation set: 0.7387\n",
      "Epoch 316: loss on final training batch: 0.3277\n",
      "Epoch 316: accuracy on validation set: 0.5225\n",
      "Epoch 316: accuracy on training set: 0.6764\n",
      "Epoch 316: loss on validation set: 0.7391\n",
      "Epoch 317: loss on final training batch: 0.3321\n",
      "Epoch 317: accuracy on validation set: 0.5244\n",
      "Epoch 317: accuracy on training set: 0.6708\n",
      "Epoch 317: loss on validation set: 0.7398\n",
      "Epoch 318: loss on final training batch: 0.3169\n",
      "Epoch 318: accuracy on validation set: 0.5229\n",
      "Epoch 318: accuracy on training set: 0.6783\n",
      "Epoch 318: loss on validation set: 0.7403\n",
      "Epoch 319: loss on final training batch: 0.3197\n",
      "Epoch 319: accuracy on validation set: 0.5206\n",
      "Epoch 319: accuracy on training set: 0.6792\n",
      "Epoch 319: loss on validation set: 0.7410\n",
      "Epoch 320: loss on final training batch: 0.3169\n",
      "Epoch 320: accuracy on validation set: 0.5252\n",
      "Epoch 320: accuracy on training set: 0.6829\n",
      "Epoch 320: loss on validation set: 0.7413\n",
      "Epoch 321: loss on final training batch: 0.3231\n",
      "Epoch 321: accuracy on validation set: 0.5252\n",
      "Epoch 321: accuracy on training set: 0.6783\n",
      "Epoch 321: loss on validation set: 0.7422\n",
      "Epoch 322: loss on final training batch: 0.3112\n",
      "Epoch 322: accuracy on validation set: 0.5242\n",
      "Epoch 322: accuracy on training set: 0.6797\n",
      "Epoch 322: loss on validation set: 0.7428\n",
      "Epoch 323: loss on final training batch: 0.2916\n",
      "Epoch 323: accuracy on validation set: 0.5238\n",
      "Epoch 323: accuracy on training set: 0.6850\n",
      "Epoch 323: loss on validation set: 0.7439\n",
      "Epoch 324: loss on final training batch: 0.2945\n",
      "Epoch 324: accuracy on validation set: 0.5231\n",
      "Epoch 324: accuracy on training set: 0.6731\n",
      "Epoch 324: loss on validation set: 0.7440\n",
      "Epoch 325: loss on final training batch: 0.3369\n",
      "Epoch 325: accuracy on validation set: 0.5223\n",
      "Epoch 325: accuracy on training set: 0.6761\n",
      "Epoch 325: loss on validation set: 0.7452\n",
      "Epoch 326: loss on final training batch: 0.3169\n",
      "Epoch 326: accuracy on validation set: 0.5238\n",
      "Epoch 326: accuracy on training set: 0.6814\n",
      "Epoch 326: loss on validation set: 0.7458\n",
      "Epoch 327: loss on final training batch: 0.2890\n",
      "Epoch 327: accuracy on validation set: 0.5271\n",
      "Epoch 327: accuracy on training set: 0.6805\n",
      "Epoch 327: loss on validation set: 0.7453\n",
      "Epoch 328: loss on final training batch: 0.3225\n",
      "Epoch 328: accuracy on validation set: 0.5267\n",
      "Epoch 328: accuracy on training set: 0.6764\n",
      "Epoch 328: loss on validation set: 0.7458\n",
      "Epoch 329: loss on final training batch: 0.2930\n",
      "Epoch 329: accuracy on validation set: 0.5244\n",
      "Epoch 329: accuracy on training set: 0.6758\n",
      "Epoch 329: loss on validation set: 0.7457\n",
      "Epoch 330: loss on final training batch: 0.3122\n",
      "Epoch 330: accuracy on validation set: 0.5233\n",
      "Epoch 330: accuracy on training set: 0.6801\n",
      "Epoch 330: loss on validation set: 0.7465\n",
      "Epoch 331: loss on final training batch: 0.3096\n",
      "Epoch 331: accuracy on validation set: 0.5235\n",
      "Epoch 331: accuracy on training set: 0.6860\n",
      "Epoch 331: loss on validation set: 0.7465\n",
      "Epoch 332: loss on final training batch: 0.3136\n",
      "Epoch 332: accuracy on validation set: 0.5215\n",
      "Epoch 332: accuracy on training set: 0.6806\n",
      "Epoch 332: loss on validation set: 0.7466\n",
      "Epoch 333: loss on final training batch: 0.3135\n",
      "Epoch 333: accuracy on validation set: 0.5265\n",
      "Epoch 333: accuracy on training set: 0.6813\n",
      "Epoch 333: loss on validation set: 0.7467\n",
      "Epoch 334: loss on final training batch: 0.3013\n",
      "Epoch 334: accuracy on validation set: 0.5240\n",
      "Epoch 334: accuracy on training set: 0.6876\n",
      "Epoch 334: loss on validation set: 0.7468\n",
      "Epoch 335: loss on final training batch: 0.2928\n",
      "Epoch 335: accuracy on validation set: 0.5246\n",
      "Epoch 335: accuracy on training set: 0.6861\n",
      "Epoch 335: loss on validation set: 0.7481\n",
      "Epoch 336: loss on final training batch: 0.3080\n",
      "Epoch 336: accuracy on validation set: 0.5256\n",
      "Epoch 336: accuracy on training set: 0.6868\n",
      "Epoch 336: loss on validation set: 0.7481\n",
      "Epoch 337: loss on final training batch: 0.3202\n",
      "Epoch 337: accuracy on validation set: 0.5233\n",
      "Epoch 337: accuracy on training set: 0.6838\n",
      "Epoch 337: loss on validation set: 0.7490\n",
      "Epoch 338: loss on final training batch: 0.2797\n",
      "Epoch 338: accuracy on validation set: 0.5223\n",
      "Epoch 338: accuracy on training set: 0.6745\n",
      "Epoch 338: loss on validation set: 0.7487\n",
      "Epoch 339: loss on final training batch: 0.3150\n",
      "Epoch 339: accuracy on validation set: 0.5235\n",
      "Epoch 339: accuracy on training set: 0.6812\n",
      "Epoch 339: loss on validation set: 0.7493\n",
      "Epoch 340: loss on final training batch: 0.3104\n",
      "Epoch 340: accuracy on validation set: 0.5248\n",
      "Epoch 340: accuracy on training set: 0.6854\n",
      "Epoch 340: loss on validation set: 0.7503\n",
      "Epoch 341: loss on final training batch: 0.3041\n",
      "Epoch 341: accuracy on validation set: 0.5223\n",
      "Epoch 341: accuracy on training set: 0.6772\n",
      "Epoch 341: loss on validation set: 0.7505\n",
      "Epoch 342: loss on final training batch: 0.3199\n",
      "Epoch 342: accuracy on validation set: 0.5227\n",
      "Epoch 342: accuracy on training set: 0.6847\n",
      "Epoch 342: loss on validation set: 0.7517\n",
      "Epoch 343: loss on final training batch: 0.3153\n",
      "Epoch 343: accuracy on validation set: 0.5229\n",
      "Epoch 343: accuracy on training set: 0.6839\n",
      "Epoch 343: loss on validation set: 0.7529\n",
      "Epoch 344: loss on final training batch: 0.2969\n",
      "Epoch 344: accuracy on validation set: 0.5250\n",
      "Epoch 344: accuracy on training set: 0.6896\n",
      "Epoch 344: loss on validation set: 0.7536\n",
      "Epoch 345: loss on final training batch: 0.2885\n",
      "Epoch 345: accuracy on validation set: 0.5177\n",
      "Epoch 345: accuracy on training set: 0.6828\n",
      "Epoch 345: loss on validation set: 0.7547\n",
      "Epoch 346: loss on final training batch: 0.3129\n",
      "Epoch 346: accuracy on validation set: 0.5219\n",
      "Epoch 346: accuracy on training set: 0.6792\n",
      "Epoch 346: loss on validation set: 0.7538\n",
      "Epoch 347: loss on final training batch: 0.2749\n",
      "Epoch 347: accuracy on validation set: 0.5188\n",
      "Epoch 347: accuracy on training set: 0.6874\n",
      "Epoch 347: loss on validation set: 0.7545\n",
      "Epoch 348: loss on final training batch: 0.3071\n",
      "Epoch 348: accuracy on validation set: 0.5215\n",
      "Epoch 348: accuracy on training set: 0.6857\n",
      "Epoch 348: loss on validation set: 0.7549\n",
      "Epoch 349: loss on final training batch: 0.2803\n",
      "Epoch 349: accuracy on validation set: 0.5204\n",
      "Epoch 349: accuracy on training set: 0.6794\n",
      "Epoch 349: loss on validation set: 0.7558\n",
      "Epoch 350: loss on final training batch: 0.2792\n",
      "Epoch 350: accuracy on validation set: 0.5208\n",
      "Epoch 350: accuracy on training set: 0.6841\n",
      "Epoch 350: loss on validation set: 0.7561\n",
      "Epoch 351: loss on final training batch: 0.3025\n",
      "Epoch 351: accuracy on validation set: 0.5240\n",
      "Epoch 351: accuracy on training set: 0.6821\n",
      "Epoch 351: loss on validation set: 0.7570\n",
      "Epoch 352: loss on final training batch: 0.2776\n",
      "Epoch 352: accuracy on validation set: 0.5208\n",
      "Epoch 352: accuracy on training set: 0.6940\n",
      "Epoch 352: loss on validation set: 0.7574\n",
      "Epoch 353: loss on final training batch: 0.2941\n",
      "Epoch 353: accuracy on validation set: 0.5188\n",
      "Epoch 353: accuracy on training set: 0.6917\n",
      "Epoch 353: loss on validation set: 0.7569\n",
      "Epoch 354: loss on final training batch: 0.2971\n",
      "Epoch 354: accuracy on validation set: 0.5188\n",
      "Epoch 354: accuracy on training set: 0.6945\n",
      "Epoch 354: loss on validation set: 0.7576\n",
      "Epoch 355: loss on final training batch: 0.2953\n",
      "Epoch 355: accuracy on validation set: 0.5185\n",
      "Epoch 355: accuracy on training set: 0.6906\n",
      "Epoch 355: loss on validation set: 0.7583\n",
      "Epoch 356: loss on final training batch: 0.3027\n",
      "Epoch 356: accuracy on validation set: 0.5167\n",
      "Epoch 356: accuracy on training set: 0.6823\n",
      "Epoch 356: loss on validation set: 0.7586\n",
      "Epoch 357: loss on final training batch: 0.2798\n",
      "Epoch 357: accuracy on validation set: 0.5156\n",
      "Epoch 357: accuracy on training set: 0.6884\n",
      "Epoch 357: loss on validation set: 0.7592\n",
      "Epoch 358: loss on final training batch: 0.2956\n",
      "Epoch 358: accuracy on validation set: 0.5129\n",
      "Epoch 358: accuracy on training set: 0.6914\n",
      "Epoch 358: loss on validation set: 0.7598\n",
      "Epoch 359: loss on final training batch: 0.3104\n",
      "Epoch 359: accuracy on validation set: 0.5142\n",
      "Epoch 359: accuracy on training set: 0.6899\n",
      "Epoch 359: loss on validation set: 0.7601\n",
      "Epoch 360: loss on final training batch: 0.2919\n",
      "Epoch 360: accuracy on validation set: 0.5131\n",
      "Epoch 360: accuracy on training set: 0.6867\n",
      "Epoch 360: loss on validation set: 0.7600\n",
      "Epoch 361: loss on final training batch: 0.2761\n",
      "Epoch 361: accuracy on validation set: 0.5160\n",
      "Epoch 361: accuracy on training set: 0.6904\n",
      "Epoch 361: loss on validation set: 0.7605\n",
      "Epoch 362: loss on final training batch: 0.2715\n",
      "Epoch 362: accuracy on validation set: 0.5192\n",
      "Epoch 362: accuracy on training set: 0.6847\n",
      "Epoch 362: loss on validation set: 0.7609\n",
      "Epoch 363: loss on final training batch: 0.2850\n",
      "Epoch 363: accuracy on validation set: 0.5160\n",
      "Epoch 363: accuracy on training set: 0.6880\n",
      "Epoch 363: loss on validation set: 0.7623\n",
      "Epoch 364: loss on final training batch: 0.2732\n",
      "Epoch 364: accuracy on validation set: 0.5190\n",
      "Epoch 364: accuracy on training set: 0.6877\n",
      "Epoch 364: loss on validation set: 0.7642\n",
      "Epoch 365: loss on final training batch: 0.2684\n",
      "Epoch 365: accuracy on validation set: 0.5177\n",
      "Epoch 365: accuracy on training set: 0.6889\n",
      "Epoch 365: loss on validation set: 0.7634\n",
      "Epoch 366: loss on final training batch: 0.2774\n",
      "Epoch 366: accuracy on validation set: 0.5183\n",
      "Epoch 366: accuracy on training set: 0.6899\n",
      "Epoch 366: loss on validation set: 0.7631\n",
      "Epoch 367: loss on final training batch: 0.2835\n",
      "Epoch 367: accuracy on validation set: 0.5208\n",
      "Epoch 367: accuracy on training set: 0.6851\n",
      "Epoch 367: loss on validation set: 0.7638\n",
      "Epoch 368: loss on final training batch: 0.2775\n",
      "Epoch 368: accuracy on validation set: 0.5188\n",
      "Epoch 368: accuracy on training set: 0.6922\n",
      "Epoch 368: loss on validation set: 0.7647\n",
      "Epoch 369: loss on final training batch: 0.3152\n",
      "Epoch 369: accuracy on validation set: 0.5198\n",
      "Epoch 369: accuracy on training set: 0.6933\n",
      "Epoch 369: loss on validation set: 0.7646\n",
      "Epoch 370: loss on final training batch: 0.2623\n",
      "Epoch 370: accuracy on validation set: 0.5206\n",
      "Epoch 370: accuracy on training set: 0.6880\n",
      "Epoch 370: loss on validation set: 0.7648\n",
      "Epoch 371: loss on final training batch: 0.2784\n",
      "Epoch 371: accuracy on validation set: 0.5167\n",
      "Epoch 371: accuracy on training set: 0.6905\n",
      "Epoch 371: loss on validation set: 0.7657\n",
      "Epoch 372: loss on final training batch: 0.3232\n",
      "Epoch 372: accuracy on validation set: 0.5179\n",
      "Epoch 372: accuracy on training set: 0.6875\n",
      "Epoch 372: loss on validation set: 0.7664\n",
      "Epoch 373: loss on final training batch: 0.3029\n",
      "Epoch 373: accuracy on validation set: 0.5173\n",
      "Epoch 373: accuracy on training set: 0.6856\n",
      "Epoch 373: loss on validation set: 0.7666\n",
      "Epoch 374: loss on final training batch: 0.2894\n",
      "Epoch 374: accuracy on validation set: 0.5152\n",
      "Epoch 374: accuracy on training set: 0.6864\n",
      "Epoch 374: loss on validation set: 0.7667\n",
      "Epoch 375: loss on final training batch: 0.2682\n",
      "Epoch 375: accuracy on validation set: 0.5181\n",
      "Epoch 375: accuracy on training set: 0.6861\n",
      "Epoch 375: loss on validation set: 0.7662\n",
      "Epoch 376: loss on final training batch: 0.2954\n",
      "Epoch 376: accuracy on validation set: 0.5221\n",
      "Epoch 376: accuracy on training set: 0.6906\n",
      "Epoch 376: loss on validation set: 0.7663\n",
      "Epoch 377: loss on final training batch: 0.2715\n",
      "Epoch 377: accuracy on validation set: 0.5217\n",
      "Epoch 377: accuracy on training set: 0.6957\n",
      "Epoch 377: loss on validation set: 0.7674\n",
      "Epoch 378: loss on final training batch: 0.2711\n",
      "Epoch 378: accuracy on validation set: 0.5208\n",
      "Epoch 378: accuracy on training set: 0.6918\n",
      "Epoch 378: loss on validation set: 0.7677\n",
      "Epoch 379: loss on final training batch: 0.2824\n",
      "Epoch 379: accuracy on validation set: 0.5204\n",
      "Epoch 379: accuracy on training set: 0.6901\n",
      "Epoch 379: loss on validation set: 0.7683\n",
      "Epoch 380: loss on final training batch: 0.3070\n",
      "Epoch 380: accuracy on validation set: 0.5225\n",
      "Epoch 380: accuracy on training set: 0.6936\n",
      "Epoch 380: loss on validation set: 0.7686\n",
      "Epoch 381: loss on final training batch: 0.2587\n",
      "Epoch 381: accuracy on validation set: 0.5210\n",
      "Epoch 381: accuracy on training set: 0.6888\n",
      "Epoch 381: loss on validation set: 0.7691\n",
      "Epoch 382: loss on final training batch: 0.2953\n",
      "Epoch 382: accuracy on validation set: 0.5215\n",
      "Epoch 382: accuracy on training set: 0.6964\n",
      "Epoch 382: loss on validation set: 0.7702\n",
      "Epoch 383: loss on final training batch: 0.2840\n",
      "Epoch 383: accuracy on validation set: 0.5194\n",
      "Epoch 383: accuracy on training set: 0.6890\n",
      "Epoch 383: loss on validation set: 0.7697\n",
      "Epoch 384: loss on final training batch: 0.2815\n",
      "Epoch 384: accuracy on validation set: 0.5160\n",
      "Epoch 384: accuracy on training set: 0.6882\n",
      "Epoch 384: loss on validation set: 0.7709\n",
      "Epoch 385: loss on final training batch: 0.2780\n",
      "Epoch 385: accuracy on validation set: 0.5179\n",
      "Epoch 385: accuracy on training set: 0.6924\n",
      "Epoch 385: loss on validation set: 0.7709\n",
      "Epoch 386: loss on final training batch: 0.2861\n",
      "Epoch 386: accuracy on validation set: 0.5192\n",
      "Epoch 386: accuracy on training set: 0.6931\n",
      "Epoch 386: loss on validation set: 0.7712\n",
      "Epoch 387: loss on final training batch: 0.2965\n",
      "Epoch 387: accuracy on validation set: 0.5160\n",
      "Epoch 387: accuracy on training set: 0.6917\n",
      "Epoch 387: loss on validation set: 0.7738\n",
      "Epoch 388: loss on final training batch: 0.2806\n",
      "Epoch 388: accuracy on validation set: 0.5156\n",
      "Epoch 388: accuracy on training set: 0.6903\n",
      "Epoch 388: loss on validation set: 0.7735\n",
      "Epoch 389: loss on final training batch: 0.2731\n",
      "Epoch 389: accuracy on validation set: 0.5154\n",
      "Epoch 389: accuracy on training set: 0.6913\n",
      "Epoch 389: loss on validation set: 0.7734\n",
      "Epoch 390: loss on final training batch: 0.2982\n",
      "Epoch 390: accuracy on validation set: 0.5152\n",
      "Epoch 390: accuracy on training set: 0.6970\n",
      "Epoch 390: loss on validation set: 0.7749\n",
      "Epoch 391: loss on final training batch: 0.2873\n",
      "Epoch 391: accuracy on validation set: 0.5152\n",
      "Epoch 391: accuracy on training set: 0.6921\n",
      "Epoch 391: loss on validation set: 0.7753\n",
      "Epoch 392: loss on final training batch: 0.2709\n",
      "Epoch 392: accuracy on validation set: 0.5167\n",
      "Epoch 392: accuracy on training set: 0.6830\n",
      "Epoch 392: loss on validation set: 0.7747\n",
      "Epoch 393: loss on final training batch: 0.2660\n",
      "Epoch 393: accuracy on validation set: 0.5173\n",
      "Epoch 393: accuracy on training set: 0.6905\n",
      "Epoch 393: loss on validation set: 0.7757\n",
      "Epoch 394: loss on final training batch: 0.2840\n",
      "Epoch 394: accuracy on validation set: 0.5192\n",
      "Epoch 394: accuracy on training set: 0.6909\n",
      "Epoch 394: loss on validation set: 0.7758\n",
      "Epoch 395: loss on final training batch: 0.2884\n",
      "Epoch 395: accuracy on validation set: 0.5171\n",
      "Epoch 395: accuracy on training set: 0.6896\n",
      "Epoch 395: loss on validation set: 0.7761\n",
      "Epoch 396: loss on final training batch: 0.3110\n",
      "Epoch 396: accuracy on validation set: 0.5173\n",
      "Epoch 396: accuracy on training set: 0.6860\n",
      "Epoch 396: loss on validation set: 0.7762\n",
      "Epoch 397: loss on final training batch: 0.2753\n",
      "Epoch 397: accuracy on validation set: 0.5165\n",
      "Epoch 397: accuracy on training set: 0.6877\n",
      "Epoch 397: loss on validation set: 0.7763\n",
      "Epoch 398: loss on final training batch: 0.2758\n",
      "Epoch 398: accuracy on validation set: 0.5163\n",
      "Epoch 398: accuracy on training set: 0.6905\n",
      "Epoch 398: loss on validation set: 0.7777\n",
      "Epoch 399: loss on final training batch: 0.2826\n",
      "Epoch 399: accuracy on validation set: 0.5167\n",
      "Epoch 399: accuracy on training set: 0.6918\n",
      "Epoch 399: loss on validation set: 0.7764\n",
      "Epoch 400: loss on final training batch: 0.2466\n",
      "Epoch 400: accuracy on validation set: 0.5188\n",
      "Epoch 400: accuracy on training set: 0.6938\n",
      "Epoch 400: loss on validation set: 0.7763\n",
      "Epoch 401: loss on final training batch: 0.2896\n",
      "Epoch 401: accuracy on validation set: 0.5160\n",
      "Epoch 401: accuracy on training set: 0.6964\n",
      "Epoch 401: loss on validation set: 0.7780\n",
      "Epoch 402: loss on final training batch: 0.2663\n",
      "Epoch 402: accuracy on validation set: 0.5179\n",
      "Epoch 402: accuracy on training set: 0.6893\n",
      "Epoch 402: loss on validation set: 0.7781\n",
      "Epoch 403: loss on final training batch: 0.2703\n",
      "Epoch 403: accuracy on validation set: 0.5190\n",
      "Epoch 403: accuracy on training set: 0.6955\n",
      "Epoch 403: loss on validation set: 0.7789\n",
      "Epoch 404: loss on final training batch: 0.2576\n",
      "Epoch 404: accuracy on validation set: 0.5165\n",
      "Epoch 404: accuracy on training set: 0.6901\n",
      "Epoch 404: loss on validation set: 0.7791\n",
      "Epoch 405: loss on final training batch: 0.2373\n",
      "Epoch 405: accuracy on validation set: 0.5158\n",
      "Epoch 405: accuracy on training set: 0.6909\n",
      "Epoch 405: loss on validation set: 0.7807\n",
      "Epoch 406: loss on final training batch: 0.2811\n",
      "Epoch 406: accuracy on validation set: 0.5215\n",
      "Epoch 406: accuracy on training set: 0.6982\n",
      "Epoch 406: loss on validation set: 0.7813\n",
      "Epoch 407: loss on final training batch: 0.2984\n",
      "Epoch 407: accuracy on validation set: 0.5213\n",
      "Epoch 407: accuracy on training set: 0.6966\n",
      "Epoch 407: loss on validation set: 0.7810\n",
      "Epoch 408: loss on final training batch: 0.2619\n",
      "Epoch 408: accuracy on validation set: 0.5196\n",
      "Epoch 408: accuracy on training set: 0.6949\n",
      "Epoch 408: loss on validation set: 0.7807\n",
      "Epoch 409: loss on final training batch: 0.2602\n",
      "Epoch 409: accuracy on validation set: 0.5252\n",
      "Epoch 409: accuracy on training set: 0.6945\n",
      "Epoch 409: loss on validation set: 0.7825\n",
      "Epoch 410: loss on final training batch: 0.2616\n",
      "Epoch 410: accuracy on validation set: 0.5210\n",
      "Epoch 410: accuracy on training set: 0.6947\n",
      "Epoch 410: loss on validation set: 0.7830\n",
      "Epoch 411: loss on final training batch: 0.2988\n",
      "Epoch 411: accuracy on validation set: 0.5221\n",
      "Epoch 411: accuracy on training set: 0.6996\n",
      "Epoch 411: loss on validation set: 0.7835\n",
      "Epoch 412: loss on final training batch: 0.2572\n",
      "Epoch 412: accuracy on validation set: 0.5198\n",
      "Epoch 412: accuracy on training set: 0.6961\n",
      "Epoch 412: loss on validation set: 0.7840\n",
      "Epoch 413: loss on final training batch: 0.3000\n",
      "Epoch 413: accuracy on validation set: 0.5194\n",
      "Epoch 413: accuracy on training set: 0.7032\n",
      "Epoch 413: loss on validation set: 0.7850\n",
      "Epoch 414: loss on final training batch: 0.2538\n",
      "Epoch 414: accuracy on validation set: 0.5196\n",
      "Epoch 414: accuracy on training set: 0.6882\n",
      "Epoch 414: loss on validation set: 0.7848\n",
      "Epoch 415: loss on final training batch: 0.2580\n",
      "Epoch 415: accuracy on validation set: 0.5204\n",
      "Epoch 415: accuracy on training set: 0.7009\n",
      "Epoch 415: loss on validation set: 0.7842\n",
      "Epoch 416: loss on final training batch: 0.2437\n",
      "Epoch 416: accuracy on validation set: 0.5188\n",
      "Epoch 416: accuracy on training set: 0.6989\n",
      "Epoch 416: loss on validation set: 0.7845\n",
      "Epoch 417: loss on final training batch: 0.2347\n",
      "Epoch 417: accuracy on validation set: 0.5152\n",
      "Epoch 417: accuracy on training set: 0.7011\n",
      "Epoch 417: loss on validation set: 0.7852\n",
      "Epoch 418: loss on final training batch: 0.2655\n",
      "Epoch 418: accuracy on validation set: 0.5188\n",
      "Epoch 418: accuracy on training set: 0.7025\n",
      "Epoch 418: loss on validation set: 0.7859\n",
      "Epoch 419: loss on final training batch: 0.2713\n",
      "Epoch 419: accuracy on validation set: 0.5156\n",
      "Epoch 419: accuracy on training set: 0.7026\n",
      "Epoch 419: loss on validation set: 0.7855\n",
      "Epoch 420: loss on final training batch: 0.2792\n",
      "Epoch 420: accuracy on validation set: 0.5200\n",
      "Epoch 420: accuracy on training set: 0.7043\n",
      "Epoch 420: loss on validation set: 0.7842\n",
      "Epoch 421: loss on final training batch: 0.2364\n",
      "Epoch 421: accuracy on validation set: 0.5183\n",
      "Epoch 421: accuracy on training set: 0.7028\n",
      "Epoch 421: loss on validation set: 0.7854\n",
      "Epoch 422: loss on final training batch: 0.2448\n",
      "Epoch 422: accuracy on validation set: 0.5163\n",
      "Epoch 422: accuracy on training set: 0.7100\n",
      "Epoch 422: loss on validation set: 0.7858\n",
      "Epoch 423: loss on final training batch: 0.2634\n",
      "Epoch 423: accuracy on validation set: 0.5163\n",
      "Epoch 423: accuracy on training set: 0.6960\n",
      "Epoch 423: loss on validation set: 0.7862\n",
      "Epoch 424: loss on final training batch: 0.2796\n",
      "Epoch 424: accuracy on validation set: 0.5142\n",
      "Epoch 424: accuracy on training set: 0.6938\n",
      "Epoch 424: loss on validation set: 0.7864\n",
      "Epoch 425: loss on final training batch: 0.2452\n",
      "Epoch 425: accuracy on validation set: 0.5175\n",
      "Epoch 425: accuracy on training set: 0.7006\n",
      "Epoch 425: loss on validation set: 0.7879\n",
      "Epoch 426: loss on final training batch: 0.2493\n",
      "Epoch 426: accuracy on validation set: 0.5165\n",
      "Epoch 426: accuracy on training set: 0.7021\n",
      "Epoch 426: loss on validation set: 0.7880\n",
      "Epoch 427: loss on final training batch: 0.2574\n",
      "Epoch 427: accuracy on validation set: 0.5148\n",
      "Epoch 427: accuracy on training set: 0.7089\n",
      "Epoch 427: loss on validation set: 0.7883\n",
      "Epoch 428: loss on final training batch: 0.2450\n",
      "Epoch 428: accuracy on validation set: 0.5138\n",
      "Epoch 428: accuracy on training set: 0.7022\n",
      "Epoch 428: loss on validation set: 0.7875\n",
      "Epoch 429: loss on final training batch: 0.2301\n",
      "Epoch 429: accuracy on validation set: 0.5167\n",
      "Epoch 429: accuracy on training set: 0.7058\n",
      "Epoch 429: loss on validation set: 0.7870\n",
      "Epoch 430: loss on final training batch: 0.2462\n",
      "Epoch 430: accuracy on validation set: 0.5148\n",
      "Epoch 430: accuracy on training set: 0.7015\n",
      "Epoch 430: loss on validation set: 0.7876\n",
      "Epoch 431: loss on final training batch: 0.2596\n",
      "Epoch 431: accuracy on validation set: 0.5177\n",
      "Epoch 431: accuracy on training set: 0.7043\n",
      "Epoch 431: loss on validation set: 0.7887\n",
      "Epoch 432: loss on final training batch: 0.2495\n",
      "Epoch 432: accuracy on validation set: 0.5160\n",
      "Epoch 432: accuracy on training set: 0.7021\n",
      "Epoch 432: loss on validation set: 0.7899\n",
      "Epoch 433: loss on final training batch: 0.2687\n",
      "Epoch 433: accuracy on validation set: 0.5131\n",
      "Epoch 433: accuracy on training set: 0.6953\n",
      "Epoch 433: loss on validation set: 0.7898\n",
      "Epoch 434: loss on final training batch: 0.2575\n",
      "Epoch 434: accuracy on validation set: 0.5167\n",
      "Epoch 434: accuracy on training set: 0.7014\n",
      "Epoch 434: loss on validation set: 0.7902\n",
      "Epoch 435: loss on final training batch: 0.2445\n",
      "Epoch 435: accuracy on validation set: 0.5179\n",
      "Epoch 435: accuracy on training set: 0.7011\n",
      "Epoch 435: loss on validation set: 0.7901\n",
      "Epoch 436: loss on final training batch: 0.2583\n",
      "Epoch 436: accuracy on validation set: 0.5173\n",
      "Epoch 436: accuracy on training set: 0.7018\n",
      "Epoch 436: loss on validation set: 0.7916\n",
      "Epoch 437: loss on final training batch: 0.2890\n",
      "Epoch 437: accuracy on validation set: 0.5169\n",
      "Epoch 437: accuracy on training set: 0.7051\n",
      "Epoch 437: loss on validation set: 0.7940\n",
      "Epoch 438: loss on final training batch: 0.2539\n",
      "Epoch 438: accuracy on validation set: 0.5200\n",
      "Epoch 438: accuracy on training set: 0.7109\n",
      "Epoch 438: loss on validation set: 0.7953\n",
      "Epoch 439: loss on final training batch: 0.2690\n",
      "Epoch 439: accuracy on validation set: 0.5158\n",
      "Epoch 439: accuracy on training set: 0.6995\n",
      "Epoch 439: loss on validation set: 0.7957\n",
      "Epoch 440: loss on final training batch: 0.2602\n",
      "Epoch 440: accuracy on validation set: 0.5177\n",
      "Epoch 440: accuracy on training set: 0.7112\n",
      "Epoch 440: loss on validation set: 0.7973\n",
      "Epoch 441: loss on final training batch: 0.2386\n",
      "Epoch 441: accuracy on validation set: 0.5177\n",
      "Epoch 441: accuracy on training set: 0.7028\n",
      "Epoch 441: loss on validation set: 0.7969\n",
      "Epoch 442: loss on final training batch: 0.2457\n",
      "Epoch 442: accuracy on validation set: 0.5165\n",
      "Epoch 442: accuracy on training set: 0.7045\n",
      "Epoch 442: loss on validation set: 0.7966\n",
      "Epoch 443: loss on final training batch: 0.2479\n",
      "Epoch 443: accuracy on validation set: 0.5188\n",
      "Epoch 443: accuracy on training set: 0.7029\n",
      "Epoch 443: loss on validation set: 0.7962\n",
      "Epoch 444: loss on final training batch: 0.2723\n",
      "Epoch 444: accuracy on validation set: 0.5173\n",
      "Epoch 444: accuracy on training set: 0.7049\n",
      "Epoch 444: loss on validation set: 0.7967\n",
      "Epoch 445: loss on final training batch: 0.2840\n",
      "Epoch 445: accuracy on validation set: 0.5158\n",
      "Epoch 445: accuracy on training set: 0.7086\n",
      "Epoch 445: loss on validation set: 0.7987\n",
      "Epoch 446: loss on final training batch: 0.2585\n",
      "Epoch 446: accuracy on validation set: 0.5142\n",
      "Epoch 446: accuracy on training set: 0.7062\n",
      "Epoch 446: loss on validation set: 0.7987\n",
      "Epoch 447: loss on final training batch: 0.2644\n",
      "Epoch 447: accuracy on validation set: 0.5144\n",
      "Epoch 447: accuracy on training set: 0.7081\n",
      "Epoch 447: loss on validation set: 0.7979\n",
      "Epoch 448: loss on final training batch: 0.2554\n",
      "Epoch 448: accuracy on validation set: 0.5140\n",
      "Epoch 448: accuracy on training set: 0.7024\n",
      "Epoch 448: loss on validation set: 0.7974\n",
      "Epoch 449: loss on final training batch: 0.2568\n",
      "Epoch 449: accuracy on validation set: 0.5115\n",
      "Epoch 449: accuracy on training set: 0.7074\n",
      "Epoch 449: loss on validation set: 0.7985\n",
      "Epoch 450: loss on final training batch: 0.2187\n",
      "Epoch 450: accuracy on validation set: 0.5146\n",
      "Epoch 450: accuracy on training set: 0.7002\n",
      "Epoch 450: loss on validation set: 0.7999\n",
      "Epoch 451: loss on final training batch: 0.2459\n",
      "Epoch 451: accuracy on validation set: 0.5135\n",
      "Epoch 451: accuracy on training set: 0.6961\n",
      "Epoch 451: loss on validation set: 0.8000\n",
      "Epoch 452: loss on final training batch: 0.2455\n",
      "Epoch 452: accuracy on validation set: 0.5158\n",
      "Epoch 452: accuracy on training set: 0.6944\n",
      "Epoch 452: loss on validation set: 0.7990\n",
      "Epoch 453: loss on final training batch: 0.2383\n",
      "Epoch 453: accuracy on validation set: 0.5133\n",
      "Epoch 453: accuracy on training set: 0.6971\n",
      "Epoch 453: loss on validation set: 0.7977\n",
      "Epoch 454: loss on final training batch: 0.2176\n",
      "Epoch 454: accuracy on validation set: 0.5140\n",
      "Epoch 454: accuracy on training set: 0.7055\n",
      "Epoch 454: loss on validation set: 0.7982\n",
      "Epoch 455: loss on final training batch: 0.2531\n",
      "Epoch 455: accuracy on validation set: 0.5146\n",
      "Epoch 455: accuracy on training set: 0.7061\n",
      "Epoch 455: loss on validation set: 0.7998\n",
      "Epoch 456: loss on final training batch: 0.2427\n",
      "Epoch 456: accuracy on validation set: 0.5154\n",
      "Epoch 456: accuracy on training set: 0.7033\n",
      "Epoch 456: loss on validation set: 0.7982\n",
      "Epoch 457: loss on final training batch: 0.2580\n",
      "Epoch 457: accuracy on validation set: 0.5165\n",
      "Epoch 457: accuracy on training set: 0.7047\n",
      "Epoch 457: loss on validation set: 0.7981\n",
      "Epoch 458: loss on final training batch: 0.2460\n",
      "Epoch 458: accuracy on validation set: 0.5152\n",
      "Epoch 458: accuracy on training set: 0.7093\n",
      "Epoch 458: loss on validation set: 0.7986\n",
      "Epoch 459: loss on final training batch: 0.2586\n",
      "Epoch 459: accuracy on validation set: 0.5148\n",
      "Epoch 459: accuracy on training set: 0.6982\n",
      "Epoch 459: loss on validation set: 0.7985\n",
      "Epoch 460: loss on final training batch: 0.2790\n",
      "Epoch 460: accuracy on validation set: 0.5142\n",
      "Epoch 460: accuracy on training set: 0.7029\n",
      "Epoch 460: loss on validation set: 0.7989\n",
      "Epoch 461: loss on final training batch: 0.2370\n",
      "Epoch 461: accuracy on validation set: 0.5125\n",
      "Epoch 461: accuracy on training set: 0.7103\n",
      "Epoch 461: loss on validation set: 0.8006\n",
      "Epoch 462: loss on final training batch: 0.2384\n",
      "Epoch 462: accuracy on validation set: 0.5138\n",
      "Epoch 462: accuracy on training set: 0.7083\n",
      "Epoch 462: loss on validation set: 0.8008\n",
      "Epoch 463: loss on final training batch: 0.2127\n",
      "Epoch 463: accuracy on validation set: 0.5123\n",
      "Epoch 463: accuracy on training set: 0.7073\n",
      "Epoch 463: loss on validation set: 0.8037\n",
      "Epoch 464: loss on final training batch: 0.2215\n",
      "Epoch 464: accuracy on validation set: 0.5092\n",
      "Epoch 464: accuracy on training set: 0.7080\n",
      "Epoch 464: loss on validation set: 0.8039\n",
      "Epoch 465: loss on final training batch: 0.2685\n",
      "Epoch 465: accuracy on validation set: 0.5096\n",
      "Epoch 465: accuracy on training set: 0.7060\n",
      "Epoch 465: loss on validation set: 0.8038\n",
      "Epoch 466: loss on final training batch: 0.2262\n",
      "Epoch 466: accuracy on validation set: 0.5098\n",
      "Epoch 466: accuracy on training set: 0.7069\n",
      "Epoch 466: loss on validation set: 0.8048\n",
      "Epoch 467: loss on final training batch: 0.2751\n",
      "Epoch 467: accuracy on validation set: 0.5090\n",
      "Epoch 467: accuracy on training set: 0.7095\n",
      "Epoch 467: loss on validation set: 0.8059\n",
      "Epoch 468: loss on final training batch: 0.2161\n",
      "Epoch 468: accuracy on validation set: 0.5106\n",
      "Epoch 468: accuracy on training set: 0.7070\n",
      "Epoch 468: loss on validation set: 0.8056\n",
      "Epoch 469: loss on final training batch: 0.2378\n",
      "Epoch 469: accuracy on validation set: 0.5092\n",
      "Epoch 469: accuracy on training set: 0.7061\n",
      "Epoch 469: loss on validation set: 0.8057\n",
      "Epoch 470: loss on final training batch: 0.2125\n",
      "Epoch 470: accuracy on validation set: 0.5102\n",
      "Epoch 470: accuracy on training set: 0.7027\n",
      "Epoch 470: loss on validation set: 0.8057\n",
      "Epoch 471: loss on final training batch: 0.2581\n",
      "Epoch 471: accuracy on validation set: 0.5104\n",
      "Epoch 471: accuracy on training set: 0.7018\n",
      "Epoch 471: loss on validation set: 0.8058\n",
      "Epoch 472: loss on final training batch: 0.2112\n",
      "Epoch 472: accuracy on validation set: 0.5113\n",
      "Epoch 472: accuracy on training set: 0.7061\n",
      "Epoch 472: loss on validation set: 0.8065\n",
      "Epoch 473: loss on final training batch: 0.2312\n",
      "Epoch 473: accuracy on validation set: 0.5085\n",
      "Epoch 473: accuracy on training set: 0.7017\n",
      "Epoch 473: loss on validation set: 0.8075\n",
      "Epoch 474: loss on final training batch: 0.2290\n",
      "Epoch 474: accuracy on validation set: 0.5108\n",
      "Epoch 474: accuracy on training set: 0.6979\n",
      "Epoch 474: loss on validation set: 0.8073\n",
      "Epoch 475: loss on final training batch: 0.2462\n",
      "Epoch 475: accuracy on validation set: 0.5113\n",
      "Epoch 475: accuracy on training set: 0.7116\n",
      "Epoch 475: loss on validation set: 0.8099\n",
      "Epoch 476: loss on final training batch: 0.2497\n",
      "Epoch 476: accuracy on validation set: 0.5085\n",
      "Epoch 476: accuracy on training set: 0.7100\n",
      "Epoch 476: loss on validation set: 0.8104\n",
      "Epoch 477: loss on final training batch: 0.2300\n",
      "Epoch 477: accuracy on validation set: 0.5098\n",
      "Epoch 477: accuracy on training set: 0.7036\n",
      "Epoch 477: loss on validation set: 0.8100\n",
      "Epoch 478: loss on final training batch: 0.2267\n",
      "Epoch 478: accuracy on validation set: 0.5129\n",
      "Epoch 478: accuracy on training set: 0.7050\n",
      "Epoch 478: loss on validation set: 0.8104\n",
      "Epoch 479: loss on final training batch: 0.2305\n",
      "Epoch 479: accuracy on validation set: 0.5113\n",
      "Epoch 479: accuracy on training set: 0.7072\n",
      "Epoch 479: loss on validation set: 0.8112\n",
      "Epoch 480: loss on final training batch: 0.2414\n",
      "Epoch 480: accuracy on validation set: 0.5142\n",
      "Epoch 480: accuracy on training set: 0.7064\n",
      "Epoch 480: loss on validation set: 0.8111\n",
      "Epoch 481: loss on final training batch: 0.2482\n",
      "Epoch 481: accuracy on validation set: 0.5142\n",
      "Epoch 481: accuracy on training set: 0.7063\n",
      "Epoch 481: loss on validation set: 0.8119\n",
      "Epoch 482: loss on final training batch: 0.2362\n",
      "Epoch 482: accuracy on validation set: 0.5100\n",
      "Epoch 482: accuracy on training set: 0.7087\n",
      "Epoch 482: loss on validation set: 0.8127\n",
      "Epoch 483: loss on final training batch: 0.2608\n",
      "Epoch 483: accuracy on validation set: 0.5106\n",
      "Epoch 483: accuracy on training set: 0.7059\n",
      "Epoch 483: loss on validation set: 0.8138\n",
      "Epoch 484: loss on final training batch: 0.2282\n",
      "Epoch 484: accuracy on validation set: 0.5104\n",
      "Epoch 484: accuracy on training set: 0.7165\n",
      "Epoch 484: loss on validation set: 0.8137\n",
      "Epoch 485: loss on final training batch: 0.2426\n",
      "Epoch 485: accuracy on validation set: 0.5108\n",
      "Epoch 485: accuracy on training set: 0.7075\n",
      "Epoch 485: loss on validation set: 0.8138\n",
      "Epoch 486: loss on final training batch: 0.2146\n",
      "Epoch 486: accuracy on validation set: 0.5110\n",
      "Epoch 486: accuracy on training set: 0.7118\n",
      "Epoch 486: loss on validation set: 0.8153\n",
      "Epoch 487: loss on final training batch: 0.2401\n",
      "Epoch 487: accuracy on validation set: 0.5123\n",
      "Epoch 487: accuracy on training set: 0.7082\n",
      "Epoch 487: loss on validation set: 0.8146\n",
      "Epoch 488: loss on final training batch: 0.2428\n",
      "Epoch 488: accuracy on validation set: 0.5119\n",
      "Epoch 488: accuracy on training set: 0.7047\n",
      "Epoch 488: loss on validation set: 0.8151\n",
      "Epoch 489: loss on final training batch: 0.2315\n",
      "Epoch 489: accuracy on validation set: 0.5117\n",
      "Epoch 489: accuracy on training set: 0.7104\n",
      "Epoch 489: loss on validation set: 0.8137\n",
      "Epoch 490: loss on final training batch: 0.2466\n",
      "Epoch 490: accuracy on validation set: 0.5133\n",
      "Epoch 490: accuracy on training set: 0.7117\n",
      "Epoch 490: loss on validation set: 0.8134\n",
      "Epoch 491: loss on final training batch: 0.2193\n",
      "Epoch 491: accuracy on validation set: 0.5125\n",
      "Epoch 491: accuracy on training set: 0.7099\n",
      "Epoch 491: loss on validation set: 0.8129\n",
      "Epoch 492: loss on final training batch: 0.2588\n",
      "Epoch 492: accuracy on validation set: 0.5113\n",
      "Epoch 492: accuracy on training set: 0.7103\n",
      "Epoch 492: loss on validation set: 0.8149\n",
      "Epoch 493: loss on final training batch: 0.2472\n",
      "Epoch 493: accuracy on validation set: 0.5113\n",
      "Epoch 493: accuracy on training set: 0.7122\n",
      "Epoch 493: loss on validation set: 0.8137\n",
      "Epoch 494: loss on final training batch: 0.2245\n",
      "Epoch 494: accuracy on validation set: 0.5113\n",
      "Epoch 494: accuracy on training set: 0.7132\n",
      "Epoch 494: loss on validation set: 0.8151\n",
      "Epoch 495: loss on final training batch: 0.2452\n",
      "Epoch 495: accuracy on validation set: 0.5138\n",
      "Epoch 495: accuracy on training set: 0.7118\n",
      "Epoch 495: loss on validation set: 0.8169\n",
      "Epoch 496: loss on final training batch: 0.2340\n",
      "Epoch 496: accuracy on validation set: 0.5129\n",
      "Epoch 496: accuracy on training set: 0.7165\n",
      "Epoch 496: loss on validation set: 0.8174\n",
      "Epoch 497: loss on final training batch: 0.2245\n",
      "Epoch 497: accuracy on validation set: 0.5096\n",
      "Epoch 497: accuracy on training set: 0.7108\n",
      "Epoch 497: loss on validation set: 0.8187\n",
      "Epoch 498: loss on final training batch: 0.2467\n",
      "Epoch 498: accuracy on validation set: 0.5113\n",
      "Epoch 498: accuracy on training set: 0.7165\n",
      "Epoch 498: loss on validation set: 0.8191\n",
      "Epoch 499: loss on final training batch: 0.2049\n",
      "Epoch 499: accuracy on validation set: 0.5165\n",
      "Epoch 499: accuracy on training set: 0.7113\n",
      "Epoch 499: loss on validation set: 0.8171\n",
      "Epoch 500: loss on final training batch: 0.2189\n",
      "Epoch 500: accuracy on validation set: 0.5140\n",
      "Epoch 500: accuracy on training set: 0.7099\n",
      "Epoch 500: loss on validation set: 0.8184\n"
     ]
    }
   ],
   "source": [
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(276,128),\n",
    "    #torch.nn.BatchNorm1d(128),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    \n",
    "    torch.nn.Linear(128,64),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    # torch.nn.Dropout(p=0.2),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,16),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16,1)\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "device = torch.device(dev)\n",
    "\n",
    "model2.to(device)\n",
    "\n",
    "\n",
    "#loss function and optimizer\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.Adam(model2.parameters(),lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "#epoch number\n",
    "num_epoch = 500\n",
    "next_epoch = 1\n",
    "batch_size = 1000\n",
    "\n",
    "valid_loss = 100000000000\n",
    "loss_flag = False\n",
    "\n",
    "#training loop\n",
    "for epoch in range(next_epoch, next_epoch+num_epoch):\n",
    "    model2.train()\n",
    "    \n",
    "    \n",
    "    # Make an entire pass (an 'epoch') over the training data in batch_size chunks\n",
    "    for i in range(0, len(X_train), batch_size):        \n",
    "        X = X_train[i:i+batch_size].to(device)     # Slice out a mini-batch of features\n",
    "        y = y_train[i:i+batch_size].to(device)     # Slice out a mini-batch of targets\n",
    "        \n",
    "        \n",
    "\n",
    "        y_pred = model2(X)                   # Make predictions (final-layer activations)\n",
    "        \n",
    "        l = loss(y_pred, y)                 # Compute loss with respect to predictions\n",
    "        \n",
    "        model2.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\n",
    "        l.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\n",
    "        optim.step()                    # Use the gradients to take a step with SGD.\n",
    "        \n",
    "    print(\"Epoch %2d: loss on final training batch: %.4f\" % (epoch, l.item()))\n",
    "    # pred = torch.sign(model(X_val))\n",
    "    # acc = torch.mean((pred == y_val).float())\n",
    "    \n",
    "    model2.eval()\n",
    "\n",
    "    #create dire query\n",
    "    \n",
    "    #validation set calculations\n",
    "    dire_X =  torch.index_select(X_val, 1, torch.LongTensor([*range(138,276)]))\n",
    "    dire_X = torch.cat((dire_X,torch.index_select(X_val, 1, torch.LongTensor([*range(0,138)]))),1).to(device)\n",
    "\n",
    "    dire_pred = (model2(dire_X) >= 0).float()\n",
    "    \n",
    "    \n",
    "    rad_pred = (model2(X_val.to(device)) >= 0).float()\n",
    "    \n",
    "\n",
    "    overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "    \n",
    "    val_acc = torch.mean((overall_prob.to('cpu') == y_val.to('cpu')).float())\n",
    "\n",
    "    #training set calculations\n",
    "    dire_X_train =  torch.index_select(X_train, 1, torch.LongTensor([*range(138,276)]))\n",
    "    dire_X_train = torch.cat((dire_X_train,torch.index_select(X_train, 1, torch.LongTensor([*range(0,138)]))),1).to(device)\n",
    "\n",
    "    dire_pred = (model2(dire_X_train) >= 0).float()\n",
    "    \n",
    "    rad_pred = (model2(X_train.to(device)) >= 0).float()\n",
    "    \n",
    "\n",
    "    overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "    \n",
    "    train_acc = torch.mean((overall_prob.to('cpu') == y_train.to('cpu')).float())\n",
    "    \n",
    "    print(\"Epoch %2d: accuracy on validation set: %.4f\" % (epoch, val_acc))\n",
    "    print(\"Epoch %2d: accuracy on training set: %.4f\" % (epoch, train_acc))\n",
    "    \n",
    "    print(\"Epoch %2d: loss on validation set: %.4f\" % (epoch, loss(model2(X_val.to(device)), y_val.to(device))))\n",
    "    new_val_loss = loss(model2(X_val.to(device)), y_val.to(device))\n",
    "\n",
    "    if new_val_loss > valid_loss:\n",
    "       pass\n",
    "\n",
    "    valid_loss = new_val_loss\n",
    "    \n",
    "   \n",
    "\n",
    "next_epoch = epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  50.64062499999999 %\n",
      "held-out accuracy (testing):  50.9875 %\n",
      "log training accuracy:  58.203125 %\n",
      "log held-out accuracy (testing):  57.037499999999994 %\n",
      "log overall accuracy:  53.82499999999999 %\n",
      "held-out accuracy (2-fold):   54.1%\n",
      "held-out accuracy (3-fold):   55.1%\n",
      "held-out accuracy (4-fold):   55.3%\n",
      "held-out accuracy (5-fold):   55.8%\n",
      "held-out accuracy (6-fold):   55.8%\n",
      "held-out accuracy (7-fold):   55.5%\n",
      "held-out accuracy (8-fold):   55.6%\n",
      "held-out accuracy (9-fold):   55.6%\n",
      "held-out accuracy (10-fold):   55.6%\n",
      "held-out accuracy (11-fold):   55.8%\n",
      "held-out accuracy (12-fold):   55.7%\n",
      "held-out accuracy (13-fold):   55.6%\n",
      "held-out accuracy (14-fold):   55.8%\n",
      "held-out accuracy (15-fold):   55.5%\n",
      "held-out accuracy (16-fold):   55.6%\n",
      "held-out accuracy (17-fold):   55.5%\n",
      "held-out accuracy (18-fold):   55.5%\n",
      "held-out accuracy (19-fold):   55.4%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_val_trn,y_data, test_size = 0.2, shuffle= True)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = 4,activation='relu',solver='adam',batch_size=16,learning_rate_init=0.05,max_iter=3000,random_state=0)# Your code here\n",
    "log = LogisticRegression(max_iter=300)\n",
    "mlp.fit(X_train,y_train.ravel())\n",
    "log.fit(X_train,y_train.ravel())\n",
    "\n",
    "print('training accuracy: ',mlp.score(X_train,y_train)*100,'%')\n",
    "print('held-out accuracy (testing): ',mlp.score(X_test,y_test)*100,'%')\n",
    "\n",
    "dire_X =  torch.index_select(X_test, 1, torch.LongTensor([*range(138,276)]))\n",
    "dire_X = torch.cat((dire_X, torch.index_select(X_test, 1, torch.LongTensor([*range(0,138)]))),1)\n",
    "\n",
    "\n",
    "\n",
    "dire_prob = log.score(dire_X,y_test)\n",
    "rad_prob = log.score(X_test,y_test)\n",
    "\n",
    "overall_prob = ((rad_prob + (1 - dire_prob))/2)\n",
    "\n",
    "print('log training accuracy: ',log.score(X_train,y_train)*100,'%')\n",
    "print('log held-out accuracy (testing): ',log.score(X_test,y_test)*100,'%')\n",
    "\n",
    "print('log overall accuracy: ',overall_prob*100,'%')\n",
    "\n",
    "for k in range(2,10):\n",
    "    scores = sklearn.model_selection.cross_val_score(log,X_test,y_test.ravel(),cv=k) \n",
    "    print(\"held-out accuracy (%d-fold):   %.1f%%\" % (k, scores.mean()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.520, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#test accuracy on test set \n",
    "dire_X =  torch.index_select(X_test, 1, torch.LongTensor([*range(138,276)]))\n",
    "dire_X = torch.cat((dire_X,torch.index_select(X_test, 1, torch.LongTensor([*range(0,138)]))),1).to(device)\n",
    "dire_pred = (model2(dire_X) >= 0).float()\n",
    "rad_pred = (model2(X_test.to(device)) >= 0).float()\n",
    "\n",
    "\n",
    "overall_prob = (((rad_pred + (1 - dire_pred))/2) > 0.5).float()\n",
    "acc = torch.mean((overall_prob.to(device) == y_test.to(device)).float())\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DotaBuddies-THvEIQnQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6eacacc539b8e0bbdcf113f1d8da63857cb8c36d69ef3a0551f6e912e2e12e74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
